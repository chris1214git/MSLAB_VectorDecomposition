{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmZ1uZecv7qW"
   },
   "source": [
    "# Tutorial: Combined Topic Modeling\n",
    "\n",
    "(last updated 10-05-2021)\n",
    "\n",
    "In this tutorial, we are going to use our **Combined Topic Model** to get the topics out of a collections of articles.\n",
    "\n",
    "## Topic Models \n",
    "\n",
    "Topic models allow you to discover latent topics in your documents in a completely unsupervised way. Just use your documents and get topics out.\n",
    "\n",
    "## Contextualized Topic Models\n",
    "\n",
    "![](https://raw.githubusercontent.com/MilaNLProc/contextualized-topic-models/master/img/logo.png)\n",
    "\n",
    "What are Contextualized Topic Models? **CTMs** are a family of topic models that combine the expressive power of BERT embeddings with the unsupervised capabilities of topic models to get topics out of documents. \n",
    "\n",
    "## Python Package\n",
    "\n",
    "You can find our package [here](https://github.com/MilaNLProc/contextualized-topic-models).\n",
    "\n",
    "![https://github.com/MilaNLProc/contextualized-topic-models/actions](https://github.com/MilaNLProc/contextualized-topic-models/workflows/Python%20package/badge.svg) ![https://pypi.python.org/pypi/contextualized_topic_models](https://img.shields.io/pypi/v/contextualized_topic_models.svg) ![https://pepy.tech/badge/contextualized-topic-models](https://pepy.tech/badge/contextualized-topic-models)\n",
    "\n",
    "# **Before you start...**\n",
    "\n",
    "If you have additional questions about these topics, follow the links:\n",
    "\n",
    "- you need to work with languages different than English: [click here!](https://contextualized-topic-models.readthedocs.io/en/latest/language.html#language-specific)\n",
    "- you can't get good results with topic models: [click here!](https://contextualized-topic-models.readthedocs.io/en/latest/faq.html#i-am-getting-very-poor-results-what-can-i-do)\n",
    "- you want to load your own embeddings: [click here!](https://contextualized-topic-models.readthedocs.io/en/latest/faq.html#can-i-load-my-own-embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ln23-soXeQk3"
   },
   "source": [
    "# Enabling the GPU\n",
    "\n",
    "First, you'll need to enable GPUs for the notebook:\n",
    "\n",
    "- Navigate to Edit→Notebook Settings\n",
    "- select GPU from the Hardware Accelerator drop-down\n",
    "\n",
    "[Reference](https://colab.research.google.com/notebooks/gpu.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsNRo8I8Yem2"
   },
   "source": [
    "# Installing Contextualized Topic Models\n",
    "\n",
    "First, we install the contextualized topic model library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AzfVSJfZE2f"
   },
   "source": [
    "## Restart the Notebook\n",
    "\n",
    "For the changes to take effect, we now need to restart the notebook.\n",
    "\n",
    "From the Menu:\n",
    "\n",
    "Runtime → Restart Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgxRbgSZ9MsB"
   },
   "source": [
    "# Data\n",
    "\n",
    "We are going to need some data. You should upload a file with one document per line. We assume you haven't run any preprocessing script.\n",
    "\n",
    "However, if you want to first test the model without uploading your data, you can simply use the test file I'm putting here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0c_ftxjxY_H"
   },
   "source": [
    "# Importing what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SZmTpQUov8y8"
   },
   "outputs": [],
   "source": [
    "from contextualized_topic_models.models.ctm import CombinedTM\n",
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "from contextualized_topic_models.utils.preprocessing import WhiteSpacePreprocessing\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQkeIrdWLU2m"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-AM070Ez6lW"
   },
   "source": [
    "Why do we use the **preprocessed text** here? We need text without punctuation to build the bag of word. Also, we might want only to have the most frequent words inside the BoW. Too many words might not help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 20news\n",
    "import sys \n",
    "sys.path.append(\"../\")\n",
    "from utils.data_loader import load_document\n",
    "dataset = \"20news\"\n",
    "raw_documents = load_document(dataset)[\"documents\"]\n",
    "\n",
    "sp = WhiteSpacePreprocessing(raw_documents, stopwords_language='english')\n",
    "preprocessed_documents, unpreprocessed_corpus, vocab = sp.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IDxo_ERVonRE",
    "outputId": "cd831aac-0eb9-4a3c-f8ef-efd6432d3308"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['berkeley edu subject behind article organization university california berkeley lines nntp posting host berkeley edu net writes run higher last year much good think season far lead top even better atlanta braves know early season fans short still',\n",
       " 'rutgers edu subject thanks apple free article apr organization rutgers univ new lines well got yesterday took two weeks order dealer rutgers computer store apple made order one without one wanted know would must shipping anyway happy decided open first time scsi cable hard drive connected must come shipping big deal would tried come hard drive also high heard apple might try get also lack power mentioned saw also keyboard control functions much oh screen seems way power seen mentioned others must anyway fast great machine great price physics rutgers edu']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_documents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stAb2Q4eBB3W"
   },
   "source": [
    "We don't discard the non-preprocessed texts, because we are going to use them as input for obtaining the contextualized document representations. \n",
    "\n",
    "Let's pass our files with preprocess and unpreprocessed data to our `TopicModelDataPreparation` object. This object takes care of creating the bag of words for you and of obtaining the contextualized BERT representations of documents. This operation allows us to create our training dataset.\n",
    "\n",
    "Note: Here we use the contextualized model \"paraphrase-distilroberta-base-v1\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "9a104fba32e4458293bc59c0cf0ae516",
      "c97cca45dff241019ccbe54ae8849eda",
      "f2b51fe892df4ea79015e1c1ec2750e5",
      "c943462d183f4c199487a73a1d5384da",
      "a4b39f79f8dd4616a1a656443ca7d8d6",
      "c0b81ba3298642dc9182d14605187faa",
      "a5e9a52c048a48829f33369988962434",
      "dc8cf011110941f5990c040098262105",
      "a88c1f8bdd154a8ebf0da92744eec2da",
      "b9859fdef4f94727bf5f441101e46336",
      "aa26f2b6b38841d59d5e7d5c5e39fcbe",
      "fcbe1de1a4e94ed7ae4bcad633975a60",
      "3c9495c104364d34a882500a7322be0d",
      "d5a3986e6a43443a85f7e86167152321",
      "b771eb6308de4a4d9a83427cb3ada7a0",
      "2a8463d07d654ffd86a3c77c5df414fa",
      "dfc7dc989af1449387b4ba9a41ca6c9e",
      "b314c321e2c0438399df422fc7c59426",
      "d125f7e7522e46eb8d039ba325f6dc47",
      "54c61fae70154d5d9161e6abbf228a07",
      "cabdec3fec4540cb88d6e35e23aa29de",
      "0f13296f1d2a4748bafd32bab2143b1e"
     ]
    },
    "id": "KhLt6VA3wvCB",
    "outputId": "b05ad1a5-e833-468c-f33e-5cc0f83fd691",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba697cd168b246a484f33fcf989fd088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tp = TopicModelDataPreparation(\"paraphrase-distilroberta-base-v1\")\n",
    "\n",
    "training_dataset = tp.fit(text_for_contextual=unpreprocessed_corpus, text_for_bow=preprocessed_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-aevSxSBbYZG"
   },
   "source": [
    "Let's check the first ten words of the vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "evwSKgdyyhWq",
    "outputId": "0469319e-7962-47a1-a7ec-d71c7b3c6028"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ab',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'ac',\n",
       " 'accept',\n",
       " 'accepted',\n",
       " 'access',\n",
       " 'according']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp.vocab[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4x-esnySk7uO"
   },
   "source": [
    "## Training our Combined TM\n",
    "\n",
    "Finally, we can fit our new topic model. We will ask the model to find 50 topics in our collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextualized_topic_models.evaluation.measures import CoherenceNPMI, InvertedRBO\n",
    "\n",
    "texts = [text.split() for text in preprocessed_documents]\n",
    "# npmi = CoherenceNPMI(texts=texts, topics=ctm.get_topic_lists(10))\n",
    "# npmi.score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.eval import retrieval_normalized_dcg_all, retrieval_precision_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_embeddings_tensor(word2embedding, tp):\n",
    "    word_embeddings = torch.zeros(len(tp.vocab), len(word2embedding['a']))\n",
    "    for k in tp.id2token:\n",
    "        if tp.id2token[k] not in word2embedding:\n",
    "            print('not found word embedding', tp.id2token[k])\n",
    "            continue\n",
    "        word_embeddings[k] = torch.tensor(word2embedding[tp.id2token[k]])\n",
    "\n",
    "    return word_embeddings\n",
    "\n",
    "word_embeddings = calculate_word_embeddings_tensor(word2embedding, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_precision_all(preds, target, word_embeddings, k = [10], th = 0.5, display_word_result=False):\n",
    "    \"\"\"Computes `TopK precision`_ (for information retrieval).\n",
    "    Note:\n",
    "        select topk pred\n",
    "        consider all positive target as ground truth\n",
    "        one ground truth only count once\n",
    "\n",
    "    Args:\n",
    "    (1) preds: tensor with 2d shape\n",
    "    (2) target: tensor with 2d shape\n",
    "    (3) word_embeddings: word_embeddings matrix tensor with 2d shape(v, d)\n",
    "    (4) k: a list of integer\n",
    "    (5) th: threshold of word embeddings cosine similarity\n",
    "    (5) display_word_result: whether to display ground truth, prediction & hit words\n",
    "\n",
    "    Return:\n",
    "    (1) precision_scores: dict\n",
    "        key -> k, value -> average precision score\n",
    "\n",
    "    \"\"\"\n",
    "    assert preds.shape == target.shape and max(k) <= preds.shape[-1]\n",
    "    assert preds.shape[1] == word_embeddings.shape[0]\n",
    "    \n",
    "    if not isinstance(k, list):\n",
    "        raise ValueError(\"`k` has to be a list of positive integer\")\n",
    "        \n",
    "    precision_scores = {}\n",
    "    target_onehot = target > 0\n",
    "    vocab = np.array(tp.vocab)\n",
    "    word_result = defaultdict(list)\n",
    "    \n",
    "    for topk in k:\n",
    "        relevants = []\n",
    "        for i in range(preds.shape[0]):\n",
    "            preds_word_emb = word_embeddings[preds[i].topk(topk)[1]]\n",
    "            target_word_emb = word_embeddings[target_onehot[i]]\n",
    "            \n",
    "            similarity_matrix = torch.zeros(preds_word_emb.shape[0], target_word_emb.shape[0])\n",
    "            for j in range(preds_word_emb.shape[0]):\n",
    "                similarity_matrix[j] = F.cosine_similarity(preds_word_emb[j].view(1, -1), target_word_emb)\n",
    "                \n",
    "            max_similarity_score, max_similarity_idx = torch.max(similarity_matrix, dim=1)\n",
    "#             print('a', len(max_similarity_idx))\n",
    "            max_similarity_idx = max_similarity_idx[max_similarity_score >= th]\n",
    "#             print('b', len(max_similarity_idx))\n",
    "            relevant = len(torch.unique(max_similarity_idx)) / topk\n",
    "#             print('c', relevant)\n",
    "#             print('d', target_word_emb.shape[1])\n",
    "#             print('target', torch.arange(target_onehot.shape[1])[target_onehot[i] > 0])\n",
    "#             print('max score', max_similarity_score)\n",
    "#             print('max_similarity_idx', max_similarity_idx)\n",
    "#             for k in max_similarity_idx:\n",
    "#                 print(vocab[k], end=' ')\n",
    "#             print()\n",
    "            relevants.append(relevant)\n",
    "            if topk == 10 and display_word_result:\n",
    "                word_result['ground truth'].append(vocab[target_onehot[i].cpu().numpy()])\n",
    "                word_result['prediction'].append(vocab[preds[i].topk(topk)[1].cpu().numpy()])\n",
    "                word_result['hit'].append(vocab[target_onehot[i].cpu().numpy()][max_similarity_idx.cpu().numpy()])\n",
    "                \n",
    "        precision_scores[topk] = np.mean(relevants)\n",
    "    \n",
    "    return precision_scores, word_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import wordcloud\n",
    "from scipy.special import softmax\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from contextualized_topic_models.utils.early_stopping.early_stopping import EarlyStopping\n",
    "from contextualized_topic_models.networks.decoding_network import DecoderNetwork\n",
    "\n",
    "\n",
    "class CTM:\n",
    "    \"\"\"Class to train the contextualized topic model. This is the more general class that we are keeping to\n",
    "    avoid braking code, users should use the two subclasses ZeroShotTM and CombinedTm to do topic modeling.\n",
    "    :param bow_size: int, dimension of input\n",
    "    :param contextual_size: int, dimension of input that comes from BERT embeddings\n",
    "    :param inference_type: string, you can choose between the contextual model and the combined model\n",
    "    :param n_components: int, number of topic components, (default 10)\n",
    "    :param model_type: string, 'prodLDA' or 'LDA' (default 'prodLDA')\n",
    "    :param hidden_sizes: tuple, length = n_layers, (default (100, 100))\n",
    "    :param activation: string, 'softplus', 'relu', (default 'softplus')\n",
    "    :param dropout: float, dropout to use (default 0.2)\n",
    "    :param learn_priors: bool, make priors a learnable parameter (default True)\n",
    "    :param batch_size: int, size of batch to use for training (default 64)\n",
    "    :param lr: float, learning rate to use for training (default 2e-3)\n",
    "    :param momentum: float, momentum to use for training (default 0.99)\n",
    "    :param solver: string, optimizer 'adam' or 'sgd' (default 'adam')\n",
    "    :param num_epochs: int, number of epochs to train for, (default 100)\n",
    "    :param reduce_on_plateau: bool, reduce learning rate by 10x on plateau of 10 epochs (default False)\n",
    "    :param num_data_loader_workers: int, number of data loader workers (default cpu_count). set it to 0 if you are using Windows\n",
    "    :param label_size: int, number of total labels (default: 0)\n",
    "    :param loss_weights: dict, it contains the name of the weight parameter (key) and the weight (value) for each loss.\n",
    "    It supports only the weight parameter beta for now. If None, then the weights are set to 1 (default: None).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bow_size, contextual_size, inference_type=\"combined\", n_components=10, model_type='prodLDA',\n",
    "                 hidden_sizes=(100, 100), activation='softplus', dropout=0.2, learn_priors=True, batch_size=64,\n",
    "                 lr=2e-3, momentum=0.99, solver='adam', num_epochs=100, reduce_on_plateau=False,\n",
    "                 num_data_loader_workers=mp.cpu_count(), label_size=0, loss_weights=None):\n",
    "\n",
    "        self.device = (\n",
    "                torch.device(\"cuda\")\n",
    "                if torch.cuda.is_available()\n",
    "                else torch.device(\"cpu\")\n",
    "            )\n",
    "\n",
    "        if self.__class__.__name__ == \"CTM\":\n",
    "            raise Exception(\"You cannot call this class. Use ZeroShotTM or CombinedTM\")\n",
    "\n",
    "        assert isinstance(bow_size, int) and bow_size > 0, \\\n",
    "            \"input_size must by type int > 0.\"\n",
    "        assert isinstance(n_components, int) and bow_size > 0, \\\n",
    "            \"n_components must by type int > 0.\"\n",
    "        assert model_type in ['LDA', 'prodLDA'], \\\n",
    "            \"model must be 'LDA' or 'prodLDA'.\"\n",
    "        assert isinstance(hidden_sizes, tuple), \\\n",
    "            \"hidden_sizes must be type tuple.\"\n",
    "        assert activation in ['softplus', 'relu'], \\\n",
    "            \"activation must be 'softplus' or 'relu'.\"\n",
    "        assert dropout >= 0, \"dropout must be >= 0.\"\n",
    "        assert isinstance(learn_priors, bool), \"learn_priors must be boolean.\"\n",
    "        assert isinstance(batch_size, int) and batch_size > 0, \\\n",
    "            \"batch_size must be int > 0.\"\n",
    "        assert lr > 0, \"lr must be > 0.\"\n",
    "        assert isinstance(momentum, float) and 0 < momentum <= 1, \\\n",
    "            \"momentum must be 0 < float <= 1.\"\n",
    "        assert solver in ['adam', 'sgd'], \"solver must be 'adam' or 'sgd'.\"\n",
    "        assert isinstance(reduce_on_plateau, bool), \\\n",
    "            \"reduce_on_plateau must be type bool.\"\n",
    "        assert isinstance(num_data_loader_workers, int) and num_data_loader_workers >= 0, \\\n",
    "            \"num_data_loader_workers must by type int >= 0. set 0 if you are using windows\"\n",
    "\n",
    "        self.bow_size = bow_size\n",
    "        self.n_components = n_components\n",
    "        self.model_type = model_type\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.learn_priors = learn_priors\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.contextual_size = contextual_size\n",
    "        self.momentum = momentum\n",
    "        self.solver = solver\n",
    "        self.num_epochs = num_epochs\n",
    "        self.reduce_on_plateau = reduce_on_plateau\n",
    "        self.num_data_loader_workers = num_data_loader_workers\n",
    "        self.training_doc_topic_distributions = None\n",
    "\n",
    "        if loss_weights:\n",
    "            self.weights = loss_weights\n",
    "        else:\n",
    "            self.weights = {\"beta\": 1}\n",
    "\n",
    "        self.model = DecoderNetwork(\n",
    "            bow_size, self.contextual_size, inference_type, n_components, model_type, hidden_sizes, activation,\n",
    "            dropout, learn_priors, label_size=label_size)\n",
    "\n",
    "        self.early_stopping = None\n",
    "\n",
    "        # init optimizer\n",
    "        if self.solver == 'adam':\n",
    "            self.optimizer = optim.Adam(\n",
    "                self.model.parameters(), lr=lr, betas=(self.momentum, 0.99))\n",
    "        elif self.solver == 'sgd':\n",
    "            self.optimizer = optim.SGD(\n",
    "                self.model.parameters(), lr=lr, momentum=self.momentum)\n",
    "\n",
    "        # init lr scheduler\n",
    "        if self.reduce_on_plateau:\n",
    "            self.scheduler = ReduceLROnPlateau(self.optimizer, patience=10)\n",
    "\n",
    "        # performance attributes\n",
    "        self.best_loss_train = float('inf')\n",
    "\n",
    "        # training attributes\n",
    "        self.model_dir = None\n",
    "        self.train_data = None\n",
    "        self.nn_epoch = None\n",
    "\n",
    "        # validation attributes\n",
    "        self.validation_data = None\n",
    "\n",
    "        # learned topics\n",
    "        self.best_components = None\n",
    "\n",
    "        # Use cuda if available\n",
    "        if torch.cuda.is_available():\n",
    "            self.USE_CUDA = True\n",
    "        else:\n",
    "            self.USE_CUDA = False\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def _loss(self, inputs, word_dists, prior_mean, prior_variance,\n",
    "              posterior_mean, posterior_variance, posterior_log_variance):\n",
    "\n",
    "        # KL term\n",
    "        # var division term\n",
    "        var_division = torch.sum(posterior_variance / prior_variance, dim=1)\n",
    "        # diff means term\n",
    "        diff_means = prior_mean - posterior_mean\n",
    "        diff_term = torch.sum(\n",
    "            (diff_means * diff_means) / prior_variance, dim=1)\n",
    "        # logvar det division term\n",
    "        logvar_det_division = \\\n",
    "            prior_variance.log().sum() - posterior_log_variance.sum(dim=1)\n",
    "        # combine terms\n",
    "        KL = 0.5 * (\n",
    "            var_division + diff_term - self.n_components + logvar_det_division)\n",
    "        \n",
    "        # print('unique',torch.unique(inputs[0]))\n",
    "        # inputs[inputs > 1] = 1\n",
    "        # Reconstruction term\n",
    "\n",
    "#         inputs = torch.nn.functional.normalize(inputs, p=1)\n",
    "        \n",
    "#         word_dists = torch.sigmoid(word_dists)\n",
    "#         word_dists = torch.nn.functional.normalize(word_dists, p=1)\n",
    "        \n",
    "#         word_dists = word_dists + 1e-10\n",
    "#         word_dists = torch.log(word_dists)\n",
    "    \n",
    "#         RL = torch.sum(-inputs * word_dists, dim=1)\n",
    "        \n",
    "        RL = -torch.sum(inputs * torch.log(word_dists + 1e-10), dim=1)\n",
    "\n",
    "        #loss = self.weights[\"beta\"]*KL + RL\n",
    "\n",
    "        return KL, RL\n",
    "\n",
    "    def _train_epoch(self, loader, validate_decompose=False):\n",
    "        \"\"\"Train epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        samples_processed = 0\n",
    "\n",
    "        results = defaultdict(list)\n",
    "        word_result = defaultdict(list)\n",
    "        \n",
    "        for batch_samples in loader:\n",
    "            # batch_size x vocab_size\n",
    "            X_bow = batch_samples['X_bow']\n",
    "            X_bow = X_bow.reshape(X_bow.shape[0], -1)\n",
    "            X_contextual = batch_samples['X_contextual']\n",
    "\n",
    "            if \"labels\" in batch_samples.keys():\n",
    "                labels = batch_samples[\"labels\"]\n",
    "                labels = labels.reshape(labels.shape[0], -1)\n",
    "                labels = labels.to(self.device)\n",
    "            else:\n",
    "                labels = None\n",
    "\n",
    "            if self.USE_CUDA:\n",
    "                X_bow = X_bow.cuda()\n",
    "                X_contextual = X_contextual.cuda()\n",
    "\n",
    "            # forward pass\n",
    "            self.model.zero_grad()\n",
    "            prior_mean, prior_variance, posterior_mean, posterior_variance,\\\n",
    "            posterior_log_variance, word_dists, estimated_labels = self.model(X_bow, X_contextual, labels)\n",
    "\n",
    "            # backward pass\n",
    "            kl_loss, rl_loss = self._loss(\n",
    "                X_bow, word_dists, prior_mean, prior_variance,\n",
    "                posterior_mean, posterior_variance, posterior_log_variance)\n",
    "\n",
    "            loss = self.weights[\"beta\"]*kl_loss + rl_loss\n",
    "            loss = loss.sum()\n",
    "\n",
    "            if labels is not None:\n",
    "                target_labels = torch.argmax(labels, 1)\n",
    "\n",
    "                label_loss = torch.nn.CrossEntropyLoss()(estimated_labels, target_labels)\n",
    "                loss += label_loss\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # compute train loss\n",
    "            samples_processed += X_bow.size()[0]\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            if validate_decompose:\n",
    "                # Precision\n",
    "                precision_scores = retrieval_precision_all(word_dists, X_bow, k=[10, 30, 50])\n",
    "                for k, v in precision_scores.items():\n",
    "                    results['precision@{}'.format(k)].append(v)\n",
    "                # NDCG\n",
    "                ndcg_scores = retrieval_normalized_dcg_all(word_dists, X_bow, k=[10, 30, 50])\n",
    "                for k, v in ndcg_scores.items():\n",
    "                    results['ndcg@{}'.format(k)].append(v)\n",
    "                # Word embeddings Precision\n",
    "                precision_scores, word_result = semantic_precision_all(word_dists, X_bow, word_embeddings,\\\n",
    "                                                                       k = [10, 30, 50], th = 0.6, display_word_result=display_word_result)\n",
    "                for k, v in precision_scores.items():\n",
    "                    results['semantic_precision@{}'.format(k)].append(v)\n",
    "                \n",
    "            \n",
    "        train_loss /= samples_processed\n",
    "\n",
    "        return samples_processed, train_loss, results, word_result\n",
    "\n",
    "    def fit(self, train_dataset, validation_dataset=None, save_dir=None, verbose=False, patience=5, delta=0,\n",
    "            n_samples=20):\n",
    "        \"\"\"\n",
    "        Train the CTM model.\n",
    "        :param train_dataset: PyTorch Dataset class for training data.\n",
    "        :param validation_dataset: PyTorch Dataset class for validation data. If not None, the training stops if validation loss doesn't improve after a given patience\n",
    "        :param save_dir: directory to save checkpoint models to.\n",
    "        :param verbose: verbose\n",
    "        :param patience: How long to wait after last time validation loss improved. Default: 5\n",
    "        :param delta: Minimum change in the monitored quantity to qualify as an improvement. Default: 0\n",
    "        :param n_samples: int, number of samples of the document topic distribution (default: 20)\n",
    "        \"\"\"\n",
    "        # Print settings to output file\n",
    "        if verbose:\n",
    "            print(\"Settings: \\n\\\n",
    "                   N Components: {}\\n\\\n",
    "                   Topic Prior Mean: {}\\n\\\n",
    "                   Topic Prior Variance: {}\\n\\\n",
    "                   Model Type: {}\\n\\\n",
    "                   Hidden Sizes: {}\\n\\\n",
    "                   Activation: {}\\n\\\n",
    "                   Dropout: {}\\n\\\n",
    "                   Learn Priors: {}\\n\\\n",
    "                   Learning Rate: {}\\n\\\n",
    "                   Momentum: {}\\n\\\n",
    "                   Reduce On Plateau: {}\\n\\\n",
    "                   Save Dir: {}\".format(\n",
    "                self.n_components, 0.0,\n",
    "                1. - (1. / self.n_components), self.model_type,\n",
    "                self.hidden_sizes, self.activation, self.dropout, self.learn_priors,\n",
    "                self.lr, self.momentum, self.reduce_on_plateau, save_dir))\n",
    "\n",
    "        self.model_dir = save_dir\n",
    "        self.train_data = train_dataset\n",
    "        self.validation_data = validation_dataset\n",
    "        if self.validation_data is not None:\n",
    "            self.early_stopping = EarlyStopping(patience=patience, verbose=verbose, path=save_dir, delta=delta)\n",
    "        train_loader = DataLoader(\n",
    "            self.train_data, batch_size=self.batch_size, shuffle=True,\n",
    "            num_workers=self.num_data_loader_workers)\n",
    "\n",
    "        # init training variables\n",
    "        train_loss = 0\n",
    "        samples_processed = 0\n",
    "\n",
    "        # train loop\n",
    "        pbar = tqdm(range(self.num_epochs), position=0, leave=True)\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.nn_epoch = epoch\n",
    "            # train epoch\n",
    "            s = datetime.datetime.now()\n",
    "            sp, train_loss, results, word_result = self._train_epoch(train_loader, epoch%10==0)\n",
    "            samples_processed += sp\n",
    "            e = datetime.datetime.now()\n",
    "            pbar.update(1)\n",
    "\n",
    "            if self.validation_data is not None:\n",
    "                validation_loader = DataLoader(self.validation_data, batch_size=self.batch_size, shuffle=True,\n",
    "                                               num_workers=self.num_data_loader_workers)\n",
    "                # train epoch\n",
    "                s = datetime.datetime.now()\n",
    "                val_samples_processed, val_loss = self._validation(validation_loader)\n",
    "                e = datetime.datetime.now()\n",
    "\n",
    "                # report\n",
    "                if verbose:\n",
    "                    print(\"Epoch: [{}/{}]\\tSamples: [{}/{}]\\tValidation Loss: {}\\tTime: {}\".format(\n",
    "                        epoch + 1, self.num_epochs, val_samples_processed,\n",
    "                        len(self.validation_data) * self.num_epochs, val_loss, e - s))\n",
    "\n",
    "                pbar.set_description(\"Epoch: [{}/{}]\\t Seen Samples: [{}/{}]\\tTrain Loss: {}\\tValid Loss: {}\\tTime: {}\".format(\n",
    "                    epoch + 1, self.num_epochs, samples_processed,\n",
    "                    len(self.train_data) * self.num_epochs, train_loss, val_loss, e - s))\n",
    "\n",
    "                self.early_stopping(val_loss, self)\n",
    "                if self.early_stopping.early_stop:\n",
    "                    print(\"Early stopping\")\n",
    "\n",
    "                    break\n",
    "            else:\n",
    "                # save last epoch\n",
    "                self.best_components = self.model.beta\n",
    "                if save_dir is not None:\n",
    "                    self.save(save_dir)\n",
    "            pbar.set_description(\"Epoch: [{}/{}]\\t Seen Samples: [{}/{}]\\tTrain Loss: {}\\tTime: {}\".format(\n",
    "                epoch + 1, self.num_epochs, samples_processed,\n",
    "                len(self.train_data) * self.num_epochs, train_loss, e - s))\n",
    "            if epoch % 10 == 0:\n",
    "                npmi = CoherenceNPMI(texts=texts, topics=ctm.get_topic_lists(10))\n",
    "                print('epoch', epoch)\n",
    "                print('npmi', npmi.score())\n",
    "                irbo = InvertedRBO(topics=ctm.get_topic_lists(10))\n",
    "                print('irbo', irbo.score())\n",
    "                \n",
    "                for k, v in results.items():\n",
    "                    print(k, np.mean(v))\n",
    "                for k, v in word_result.items():\n",
    "                    print(k, v[:3])\n",
    "                \n",
    "\n",
    "\n",
    "        pbar.close()\n",
    "        self.training_doc_topic_distributions = self.get_doc_topic_distribution(train_dataset, n_samples)\n",
    "\n",
    "    def _validation(self, loader):\n",
    "        \"\"\"Validation epoch.\"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        samples_processed = 0\n",
    "        \n",
    "        results = defaultdict(list)\n",
    "        for batch_samples in loader:\n",
    "            # batch_size x vocab_size\n",
    "            X_bow = batch_samples['X_bow']\n",
    "            X_bow = X_bow.reshape(X_bow.shape[0], -1)\n",
    "            X_contextual = batch_samples['X_contextual']\n",
    "\n",
    "            if \"labels\" in batch_samples.keys():\n",
    "                labels = batch_samples[\"labels\"]\n",
    "                labels = labels.to(self.device)\n",
    "                labels = labels.reshape(labels.shape[0], -1)\n",
    "            else:\n",
    "                labels = None\n",
    "\n",
    "            if self.USE_CUDA:\n",
    "                X_bow = X_bow.cuda()\n",
    "                X_contextual = X_contextual.cuda()\n",
    "\n",
    "            # forward pass\n",
    "            self.model.zero_grad()\n",
    "            prior_mean, prior_variance, posterior_mean, posterior_variance, posterior_log_variance, word_dists, \\\n",
    "            estimated_labels =\\\n",
    "                self.model(X_bow, X_contextual, labels)\n",
    "\n",
    "            kl_loss, rl_loss = self._loss(X_bow, word_dists, prior_mean, prior_variance,\n",
    "                              posterior_mean, posterior_variance, posterior_log_variance)\n",
    "\n",
    "            loss = self.weights[\"beta\"]*kl_loss + rl_loss\n",
    "            loss = loss.sum()\n",
    "                \n",
    "            if labels is not None:\n",
    "                target_labels = torch.argmax(labels, 1)\n",
    "                label_loss = torch.nn.CrossEntropyLoss()(estimated_labels, target_labels)\n",
    "                loss += label_loss\n",
    "\n",
    "            # compute train loss\n",
    "            samples_processed += X_bow.size()[0]\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        val_loss /= samples_processed\n",
    "        \n",
    "        return samples_processed, val_loss\n",
    "\n",
    "    def get_thetas(self, dataset, n_samples=20):\n",
    "        \"\"\"\n",
    "        Get the document-topic distribution for a dataset of topics. Includes multiple sampling to reduce variation via\n",
    "        the parameter n_sample.\n",
    "        :param dataset: a PyTorch Dataset containing the documents\n",
    "        :param n_samples: the number of sample to collect to estimate the final distribution (the more the better).\n",
    "        \"\"\"\n",
    "        return self.get_doc_topic_distribution(dataset, n_samples=n_samples)\n",
    "\n",
    "    def get_doc_topic_distribution(self, dataset, n_samples=20):\n",
    "        \"\"\"\n",
    "        Get the document-topic distribution for a dataset of topics. Includes multiple sampling to reduce variation via\n",
    "        the parameter n_sample.\n",
    "        :param dataset: a PyTorch Dataset containing the documents\n",
    "        :param n_samples: the number of sample to collect to estimate the final distribution (the more the better).\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        loader = DataLoader(\n",
    "            dataset, batch_size=self.batch_size, shuffle=False,\n",
    "            num_workers=self.num_data_loader_workers)\n",
    "        pbar = tqdm(range(n_samples), position=0, leave=True)\n",
    "        final_thetas = []\n",
    "        for sample_index in range(n_samples):\n",
    "            with torch.no_grad():\n",
    "                collect_theta = []\n",
    "\n",
    "                for batch_samples in loader:\n",
    "                    # batch_size x vocab_size\n",
    "                    X_bow = batch_samples['X_bow']\n",
    "                    X_bow = X_bow.reshape(X_bow.shape[0], -1)\n",
    "                    X_contextual = batch_samples['X_contextual']\n",
    "\n",
    "                    if \"labels\" in batch_samples.keys():\n",
    "                        labels = batch_samples[\"labels\"]\n",
    "                        labels = labels.to(self.device)\n",
    "                        labels = labels.reshape(labels.shape[0], -1)\n",
    "                    else:\n",
    "                        labels = None\n",
    "\n",
    "                    if self.USE_CUDA:\n",
    "                        X_bow = X_bow.cuda()\n",
    "                        X_contextual = X_contextual.cuda()\n",
    "\n",
    "                    # forward pass\n",
    "                    self.model.zero_grad()\n",
    "                    collect_theta.extend(self.model.get_theta(X_bow, X_contextual, labels).cpu().numpy().tolist())\n",
    "\n",
    "                pbar.update(1)\n",
    "                pbar.set_description(\"Sampling: [{}/{}]\".format(sample_index + 1, n_samples))\n",
    "\n",
    "                final_thetas.append(np.array(collect_theta))\n",
    "        pbar.close()\n",
    "        return np.sum(final_thetas, axis=0) / n_samples\n",
    "\n",
    "    def get_most_likely_topic(self, doc_topic_distribution):\n",
    "        \"\"\" get the most likely topic for each document\n",
    "        :param doc_topic_distribution: ndarray representing the topic distribution of each document\n",
    "        \"\"\"\n",
    "        return np.argmax(doc_topic_distribution, axis=0)\n",
    "\n",
    "    def get_topics(self, k=10):\n",
    "        \"\"\"\n",
    "        Retrieve topic words.\n",
    "        :param k: int, number of words to return per topic, default 10.\n",
    "        \"\"\"\n",
    "        assert k <= self.bow_size, \"k must be <= input size.\"\n",
    "        component_dists = self.best_components\n",
    "        topics = defaultdict(list)\n",
    "        for i in range(self.n_components):\n",
    "            _, idxs = torch.topk(component_dists[i], k)\n",
    "            component_words = [self.train_data.idx2token[idx]\n",
    "                               for idx in idxs.cpu().numpy()]\n",
    "            topics[i] = component_words\n",
    "        return topics\n",
    "\n",
    "    def get_topic_lists(self, k=10):\n",
    "        \"\"\"\n",
    "        Retrieve the lists of topic words.\n",
    "        :param k: (int) number of words to return per topic, default 10.\n",
    "        \"\"\"\n",
    "        assert k <= self.bow_size, \"k must be <= input size.\"\n",
    "        # TODO: collapse this method with the one that just returns the topics\n",
    "        component_dists = self.best_components\n",
    "        topics = []\n",
    "        for i in range(self.n_components):\n",
    "            _, idxs = torch.topk(component_dists[i], k)\n",
    "            component_words = [self.train_data.idx2token[idx]\n",
    "                               for idx in idxs.cpu().numpy()]\n",
    "            topics.append(component_words)\n",
    "        return topics\n",
    "\n",
    "    def _format_file(self):\n",
    "        model_dir = \"contextualized_topic_model_nc_{}_tpm_{}_tpv_{}_hs_{}_ac_{}_do_{}_lr_{}_mo_{}_rp_{}\". \\\n",
    "            format(self.n_components, 0.0, 1 - (1. / self.n_components),\n",
    "                   self.model_type, self.hidden_sizes, self.activation,\n",
    "                   self.dropout, self.lr, self.momentum,\n",
    "                   self.reduce_on_plateau)\n",
    "        return model_dir\n",
    "\n",
    "    def save(self, models_dir=None):\n",
    "        \"\"\"\n",
    "        Save model. (Experimental Feature, not tested)\n",
    "        :param models_dir: path to directory for saving NN models.\n",
    "        \"\"\"\n",
    "        warnings.simplefilter('always', Warning)\n",
    "        warnings.warn(\"This is an experimental feature that we has not been fully tested. Refer to the following issue:\"\n",
    "                      \"https://github.com/MilaNLProc/contextualized-topic-models/issues/38\",\n",
    "                      Warning)\n",
    "\n",
    "        if (self.model is not None) and (models_dir is not None):\n",
    "\n",
    "            model_dir = self._format_file()\n",
    "            if not os.path.isdir(os.path.join(models_dir, model_dir)):\n",
    "                os.makedirs(os.path.join(models_dir, model_dir))\n",
    "\n",
    "            filename = \"epoch_{}\".format(self.nn_epoch) + '.pth'\n",
    "            fileloc = os.path.join(models_dir, model_dir, filename)\n",
    "            with open(fileloc, 'wb') as file:\n",
    "                torch.save({'state_dict': self.model.state_dict(),\n",
    "                            'dcue_dict': self.__dict__}, file)\n",
    "\n",
    "    def load(self, model_dir, epoch):\n",
    "        \"\"\"\n",
    "        Load a previously trained model. (Experimental Feature, not tested)\n",
    "        :param model_dir: directory where models are saved.\n",
    "        :param epoch: epoch of model to load.\n",
    "        \"\"\"\n",
    "\n",
    "        warnings.simplefilter('always', Warning)\n",
    "        warnings.warn(\"This is an experimental feature that we has not been fully tested. Refer to the following issue:\"\n",
    "                      \"https://github.com/MilaNLProc/contextualized-topic-models/issues/38\",\n",
    "                      Warning)\n",
    "\n",
    "        epoch_file = \"epoch_\" + str(epoch) + \".pth\"\n",
    "        model_file = os.path.join(model_dir, epoch_file)\n",
    "        with open(model_file, 'rb') as model_dict:\n",
    "            checkpoint = torch.load(model_dict, map_location=torch.device(self.device))\n",
    "\n",
    "        for (k, v) in checkpoint['dcue_dict'].items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "        self.model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    def get_topic_word_matrix(self):\n",
    "        \"\"\"\n",
    "        Return the topic-word matrix (dimensions: number of topics x length of the vocabulary).\n",
    "        If model_type is LDA, the matrix is normalized; otherwise the matrix is unnormalized.\n",
    "        \"\"\"\n",
    "        return self.model.topic_word_matrix.cpu().detach().numpy()\n",
    "\n",
    "    def get_topic_word_distribution(self):\n",
    "        \"\"\"\n",
    "        Return the topic-word distribution (dimensions: number of topics x length of the vocabulary).\n",
    "        \"\"\"\n",
    "        mat = self.get_topic_word_matrix()\n",
    "        return softmax(mat, axis=1)\n",
    "\n",
    "    def get_word_distribution_by_topic_id(self, topic):\n",
    "        \"\"\"\n",
    "        Return the word probability distribution of a topic sorted by probability.\n",
    "        :param topic: id of the topic (int)\n",
    "        :returns list of tuples (word, probability) sorted by the probability in descending order\n",
    "        \"\"\"\n",
    "        if topic >= self.n_components:\n",
    "            raise Exception('Topic id must be lower than the number of topics')\n",
    "        else:\n",
    "            wd = self.get_topic_word_distribution()\n",
    "            t = [(word, wd[topic][idx]) for idx, word in self.train_data.idx2token.items()]\n",
    "            t = sorted(t, key=lambda x: -x[1])\n",
    "        return t\n",
    "\n",
    "    def get_wordcloud(self, topic_id, n_words=5, background_color=\"black\", width=1000, height=400):\n",
    "        \"\"\"\n",
    "        Plotting the wordcloud. It is an adapted version of the code found here:\n",
    "        http://amueller.github.io/word_cloud/auto_examples/simple.html#sphx-glr-auto-examples-simple-py and\n",
    "        here https://github.com/ddangelov/Top2Vec/blob/master/top2vec/Top2Vec.py\n",
    "        :param topic_id: id of the topic\n",
    "        :param n_words: number of words to show in word cloud\n",
    "        :param background_color: color of the background\n",
    "        :param width: width of the produced image\n",
    "        :param height: height of the produced image\n",
    "        \"\"\"\n",
    "        word_score_list = self.get_word_distribution_by_topic_id(topic_id)[:n_words]\n",
    "        word_score_dict = {tup[0]: tup[1] for tup in word_score_list}\n",
    "        plt.figure(figsize=(10, 4), dpi=200)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(wordcloud.WordCloud(width=width, height=height, background_color=background_color\n",
    "                                       ).generate_from_frequencies(word_score_dict))\n",
    "        plt.title(\"Displaying Topic \" + str(topic_id), loc='center', fontsize=24)\n",
    "        plt.show()\n",
    "\n",
    "    def get_predicted_topics(self, dataset, n_samples):\n",
    "        \"\"\"\n",
    "        Return the a list containing the predicted topic for each document (length: number of documents).\n",
    "        :param dataset: CTMDataset to infer topics\n",
    "        :param n_samples: number of sampling of theta\n",
    "        :return: the predicted topics\n",
    "        \"\"\"\n",
    "        predicted_topics = []\n",
    "        thetas = self.get_doc_topic_distribution(dataset, n_samples)\n",
    "\n",
    "        for idd in range(len(dataset)):\n",
    "            predicted_topic = np.argmax(thetas[idd] / np.sum(thetas[idd]))\n",
    "            predicted_topics.append(predicted_topic)\n",
    "        return predicted_topics\n",
    "\n",
    "    def get_ldavis_data_format(self, vocab, dataset, n_samples):\n",
    "        \"\"\"\n",
    "        Returns the data that can be used in input to pyldavis to plot\n",
    "        the topics\n",
    "        \"\"\"\n",
    "        term_frequency = np.ravel(dataset.X_bow.sum(axis=0))\n",
    "        doc_lengths = np.ravel(dataset.X_bow.sum(axis=1))\n",
    "        term_topic = self.get_topic_word_distribution()\n",
    "        doc_topic_distribution = self.get_doc_topic_distribution(dataset, n_samples=n_samples)\n",
    "\n",
    "        data = {'topic_term_dists': term_topic,\n",
    "                'doc_topic_dists': doc_topic_distribution,\n",
    "                'doc_lengths': doc_lengths,\n",
    "                'vocab': vocab,\n",
    "                'term_frequency': term_frequency}\n",
    "\n",
    "        return data\n",
    "\n",
    "    def get_top_documents_per_topic_id(self, unpreprocessed_corpus, document_topic_distributions, topic_id, k=5):\n",
    "        probability_list = document_topic_distributions.T[topic_id]\n",
    "        ind = probability_list.argsort()[-k:][::-1]\n",
    "        res = []\n",
    "        for i in ind:\n",
    "            res.append((unpreprocessed_corpus[i], document_topic_distributions[i][topic_id]))\n",
    "        return res\n",
    "\n",
    "class ZeroShotTM(CTM):\n",
    "    \"\"\"ZeroShotTM, as described in https://arxiv.org/pdf/2004.07737v1.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        inference_type = \"zeroshot\"\n",
    "        super().__init__(**kwargs, inference_type=inference_type)\n",
    "\n",
    "\n",
    "class CombinedTM(CTM):\n",
    "    \"\"\"CombinedTM, as described in https://arxiv.org/pdf/2004.03974.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        inference_type = \"combined\"\n",
    "        super().__init__(**kwargs, inference_type=inference_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from contextualized_topic_models.models.ctm import CombinedTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9I3ThmBf0BcK",
    "outputId": "5ab3e9ab-7c01-4a3b-caf7-f8f343df106c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79bbbe59d4b436a8daae0e9f51cfe8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "npmi -0.155784855305963\n",
      "irbo 0.9610792355549096\n",
      "precision@10 0.06543714799365755\n",
      "precision@30 0.059853463992476465\n",
      "precision@50 0.05613495556606075\n",
      "ndcg@10 0.02934434602384345\n",
      "ndcg@30 0.03803187364365085\n",
      "ndcg@50 0.04384291853945134\n",
      "ndcg@all 0.3552574466850798\n",
      "semantic_precision@10 0.29218361581920904\n",
      "semantic_precision@30 0.2466539548022599\n",
      "semantic_precision@50 0.21776370056497174\n",
      "epoch 10\n",
      "npmi 0.06334201155454802\n",
      "irbo 0.9467422727497843\n",
      "precision@10 0.23640042780819587\n",
      "precision@30 0.18645281099666983\n",
      "precision@50 0.16444688747494907\n",
      "ndcg@10 0.14680487859552188\n",
      "ndcg@30 0.156518728298656\n",
      "ndcg@50 0.16437667648671037\n",
      "ndcg@all 0.4489974665439735\n",
      "semantic_precision@10 0.4302535310734464\n",
      "semantic_precision@30 0.3342900188323917\n",
      "semantic_precision@50 0.2821427966101695\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-e6faf086d7e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdisplay_word_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mctm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCombinedTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontextual_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mctm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# run the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-173-ba425b76e27a>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_dataset, validation_dataset, save_dir, verbose, patience, delta, n_samples)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0;31m# train epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m             \u001b[0msp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m             \u001b[0msamples_processed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-173-ba425b76e27a>\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, loader, validate_decompose)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0;31m# Word embeddings Precision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                 precision_scores, word_result = semantic_precision_all(word_dists, X_bow, word_embeddings,\\\n\u001b[0;32m--> 242\u001b[0;31m                                                                        k = [10, 30, 50], th = 0.6, display_word_result=display_word_result)\n\u001b[0m\u001b[1;32m    243\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprecision_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'semantic_precision@{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-172-782290564daf>\u001b[0m in \u001b[0;36msemantic_precision_all\u001b[0;34m(preds, target, word_embeddings, k, th, display_word_result)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0msimilarity_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_word_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_word_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_word_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0msimilarity_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_word_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_word_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mmax_similarity_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_similarity_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "display_word_result = True\n",
    "ctm = CombinedTM(bow_size=len(tp.vocab), contextual_size=768, n_components=50, num_epochs=1000)\n",
    "ctm.fit(training_dataset) # run the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SEBG6wj9Zdu"
   },
   "source": [
    "# Topics\n",
    "\n",
    "After training, now it is the time to look at our topics: we can use the \n",
    "\n",
    "```\n",
    "get_topic_lists\n",
    "```\n",
    "\n",
    "function to get the topics. It also accepts a parameter that allows you to select how many words you want to see for each topic.\n",
    "\n",
    "If you look at the topics, you will see that they all make sense and are representative of a collection of documents that comes from Wikipedia (general knowledge). Notice that the topics are in English, because we trained the model on English documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lxcKgjbx3V2o",
    "outputId": "a905d83d-55f2-4500-c508-bfba61d596e8"
   },
   "outputs": [],
   "source": [
    "ctm.get_topic_lists(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIxPpLs4sn-Q"
   },
   "source": [
    "# Let's Draw!\n",
    "\n",
    "We can use PyLDAvis to plot our topic in a nice and friendly manner :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mnN3vcyQstZE",
    "outputId": "16d9c88d-c79a-45e8-a386-d0fe7f59530b"
   },
   "outputs": [],
   "source": [
    " lda_vis_data = ctm.get_ldavis_data_format(tp.vocab, training_dataset, n_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFhhsliMtDXJ"
   },
   "outputs": [],
   "source": [
    "import pyLDAvis as vis\n",
    "\n",
    "lda_vis_data = ctm.get_ldavis_data_format(tp.vocab, training_dataset, n_samples=10)\n",
    "\n",
    "ctm_pd = vis.prepare(**lda_vis_data)\n",
    "vis.display(ctm_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeNJfHvzjD2R"
   },
   "source": [
    "# Topic Predictions\n",
    "\n",
    "Ok now we can take a document and see which topic has been assigned to it. Results will obviously change with respect to the documents you are using. For example, let's predict the topic of the first preprocessed document that is talking about a peninsula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zl5O1HExjI0e",
    "outputId": "3de02710-fa97-4f9d-9a7e-51b64d0cc8a5"
   },
   "outputs": [],
   "source": [
    "# topics_predictions = ctm.get_thetas(training_dataset, n_samples=5) # get all the topic predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "fFWsIxDYj0pC",
    "outputId": "fe9ec120-1777-42cc-f1fa-0899dd2dee16"
   },
   "outputs": [],
   "source": [
    "# preprocessed_documents[0] # see the text of our preprocessed document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJf1bP5PjqOQ"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# topic_number = np.argmax(topics_predictions[0]) # get the topic id of the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yMA9vUsgjwOi",
    "outputId": "de056c09-cd2b-41d6-df29-a9e7507d4f97"
   },
   "outputs": [],
   "source": [
    "# ctm.get_topic_lists(5)[topic_number] #and the topic should be about natural location related things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pB9i2zyKxQ0X"
   },
   "source": [
    "# Save Our Model for Later Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextualized_topic_models.evaluation.measures import CoherenceNPMI\n",
    "\n",
    "texts = [text.split() for text in preprocessed_documents]\n",
    "npmi = CoherenceNPMI(texts=texts, topics=ctm.get_topic_lists(10))\n",
    "npmi.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63550456c9f04faaadafda881fe7e5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:400001\n"
     ]
    }
   ],
   "source": [
    "def load_word2emb(embedding_file):\n",
    "    import re\n",
    "    from tqdm.auto import tqdm\n",
    "    import numpy as np\n",
    "    \n",
    "    word2embedding = dict()\n",
    "    word_dim = int(re.findall(r\".(\\d+)d\", embedding_file)[0])\n",
    "\n",
    "    with open(embedding_file, \"r\") as f:\n",
    "        for line in tqdm(f):\n",
    "            line = line.strip().split()\n",
    "            word = line[0]\n",
    "            embedding = list(map(float, line[1:]))\n",
    "            word2embedding[word] = np.array(embedding)\n",
    "\n",
    "    print(\"Number of words:%d\" % len(word2embedding))\n",
    "\n",
    "    return word2embedding\n",
    "\n",
    "word2embedding = load_word2emb(\"../data/glove.6B.200d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rice tensor([1.], dtype=torch.float64)\n",
      "wheat tensor([0.6079], dtype=torch.float64)\n",
      "condoleezza tensor([0.6111], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "th = 0.6\n",
    "\n",
    "q = 'rice'\n",
    "input1 = torch.tensor(word2embedding[q]).view(1, -1)\n",
    "    \n",
    "for w in word2embedding:\n",
    "    input2 = torch.tensor(word2embedding[w]).view(1, -1)\n",
    "    if F.cosine_similarity(input1, input2) > th:\n",
    "        print(w, F.cosine_similarity(input1, input2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0490, -0.0294,  0.0496,  0.1302,  0.1189, -0.0571, -0.0439,  0.0834,\n",
      "        -0.0011, -0.0478,  0.1038,  0.0549, -0.0481,  0.1146,  0.0335, -0.0876,\n",
      "         0.0079,  0.1197, -0.0302,  0.0147,  0.1249,  0.0464, -0.0449, -0.1389,\n",
      "        -0.0493, -0.0891,  0.0403,  0.0639, -0.1237, -0.1102, -0.1996, -0.0574,\n",
      "         0.0540,  0.0114, -0.0968,  0.0890, -0.0347, -0.0747,  0.1417, -0.0577,\n",
      "         0.1442,  0.0711, -0.0251,  0.0179, -0.0338, -0.0703, -0.0697, -0.2003,\n",
      "         0.0496, -0.0622,  0.1487, -0.0495,  0.0713, -0.0183,  0.1387,  0.0480,\n",
      "        -0.0493,  0.2066, -0.0486,  0.1466, -0.0937,  0.0426,  0.0635, -0.0405,\n",
      "         0.0266,  0.1986,  0.0009, -0.0791, -0.0325, -0.0129,  0.1075, -0.1198,\n",
      "        -0.0525, -0.1240,  0.0659, -0.1176,  0.0847,  0.0642,  0.0455,  0.0261,\n",
      "         0.1069, -0.1104, -0.0664,  0.0010,  0.0160,  0.1476, -0.1021,  0.0084,\n",
      "        -0.0035, -0.0140,  0.0874,  0.0983,  0.1836, -0.0957, -0.0258,  0.0518,\n",
      "        -0.0304, -0.1184,  0.1292, -0.0977, -0.1306, -0.0776, -0.0684,  0.0197,\n",
      "         0.0786,  0.0725,  0.0713, -0.0770,  0.1191, -0.0993, -0.0895, -0.0138,\n",
      "         0.0569, -0.0037, -0.1585, -0.1322, -0.1072,  0.0440,  0.0636,  0.1414,\n",
      "        -0.0445,  0.0203, -0.2279, -0.0947, -0.1295,  0.0427, -0.0537, -0.0190,\n",
      "        -0.0851, -0.0098,  0.0878,  0.0515,  0.0051,  0.1292,  0.1057, -0.0654,\n",
      "         0.1579, -0.0479,  0.1184,  0.0724, -0.0258,  0.0326, -0.1675,  0.0221,\n",
      "        -0.0080,  0.0570,  0.0566, -0.0724,  0.1239,  0.0566,  0.0266, -0.1348,\n",
      "        -0.0336,  0.1054,  0.0229, -0.0395, -0.0845,  0.0298, -0.0545, -0.1195,\n",
      "         0.0286, -0.1473,  0.0128,  0.0310,  0.0523, -0.0334, -0.0926, -0.0187,\n",
      "         0.0121, -0.0072,  0.1272,  0.0626, -0.1923, -0.0330,  0.0819,  0.0538,\n",
      "         0.0109, -0.2082,  0.0121, -0.0257, -0.0979,  0.0341,  0.0141,  0.0394,\n",
      "         0.0165,  0.0211,  0.0642, -0.0039, -0.0334, -0.1081,  0.0422, -0.0154,\n",
      "         0.0071, -0.2121, -0.0520,  0.0321,  0.1419, -0.0985, -0.0193,  0.2027])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "input1 = torch.randn(1, 128)\n",
    "input2 = torch.randn(200, 128)\n",
    "output = F.cosine_similarity(input1, input2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Combined TM on Wikipedia Data (Preproc+Saving+Viz) (stable v2.2.0)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0f13296f1d2a4748bafd32bab2143b1e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a8463d07d654ffd86a3c77c5df414fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f13296f1d2a4748bafd32bab2143b1e",
      "placeholder": "​",
      "style": "IPY_MODEL_cabdec3fec4540cb88d6e35e23aa29de",
      "value": " 100/100 [00:38&lt;00:00,  2.85it/s]"
     }
    },
    "3c9495c104364d34a882500a7322be0d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54c61fae70154d5d9161e6abbf228a07": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a104fba32e4458293bc59c0cf0ae516": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f2b51fe892df4ea79015e1c1ec2750e5",
       "IPY_MODEL_c943462d183f4c199487a73a1d5384da",
       "IPY_MODEL_a4b39f79f8dd4616a1a656443ca7d8d6"
      ],
      "layout": "IPY_MODEL_c97cca45dff241019ccbe54ae8849eda"
     }
    },
    "a4b39f79f8dd4616a1a656443ca7d8d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa26f2b6b38841d59d5e7d5c5e39fcbe",
      "placeholder": "​",
      "style": "IPY_MODEL_b9859fdef4f94727bf5f441101e46336",
      "value": " 306M/306M [01:09&lt;00:00, 9.62MB/s]"
     }
    },
    "a5e9a52c048a48829f33369988962434": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a88c1f8bdd154a8ebf0da92744eec2da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa26f2b6b38841d59d5e7d5c5e39fcbe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b314c321e2c0438399df422fc7c59426": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b771eb6308de4a4d9a83427cb3ada7a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54c61fae70154d5d9161e6abbf228a07",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d125f7e7522e46eb8d039ba325f6dc47",
      "value": 100
     }
    },
    "b9859fdef4f94727bf5f441101e46336": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c0b81ba3298642dc9182d14605187faa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c943462d183f4c199487a73a1d5384da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a88c1f8bdd154a8ebf0da92744eec2da",
      "max": 305584576,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dc8cf011110941f5990c040098262105",
      "value": 305584576
     }
    },
    "c97cca45dff241019ccbe54ae8849eda": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cabdec3fec4540cb88d6e35e23aa29de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d125f7e7522e46eb8d039ba325f6dc47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d5a3986e6a43443a85f7e86167152321": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b314c321e2c0438399df422fc7c59426",
      "placeholder": "​",
      "style": "IPY_MODEL_dfc7dc989af1449387b4ba9a41ca6c9e",
      "value": "Batches: 100%"
     }
    },
    "dc8cf011110941f5990c040098262105": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dfc7dc989af1449387b4ba9a41ca6c9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f2b51fe892df4ea79015e1c1ec2750e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5e9a52c048a48829f33369988962434",
      "placeholder": "​",
      "style": "IPY_MODEL_c0b81ba3298642dc9182d14605187faa",
      "value": "100%"
     }
    },
    "fcbe1de1a4e94ed7ae4bcad633975a60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d5a3986e6a43443a85f7e86167152321",
       "IPY_MODEL_b771eb6308de4a4d9a83427cb3ada7a0",
       "IPY_MODEL_2a8463d07d654ffd86a3c77c5df414fa"
      ],
      "layout": "IPY_MODEL_3c9495c104364d34a882500a7322be0d"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
