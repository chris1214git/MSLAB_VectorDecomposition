{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24ecc693",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "815ade9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/chrisliu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Used to get the data\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "sys.path.append('../')\n",
    "from utils.eval import retrieval_normalized_dcg_all, retrieval_precision_all\n",
    "from utils.loss import ListNet, ListNet2, ListNet_origin, MultiLabelMarginLossCustom, MultiLabelMarginLossCustomV, MSE\n",
    "from utils.data_processing import get_process_data\n",
    "\n",
    "seed = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f477c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2embedding from ../data/glove.6B.100d.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9383c757d84041a7a49edd612d7c93de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:400000\n",
      "Generating document tfidf representation...\n",
      "Document TFIDF dim:(18846, 21365)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d1ed6777c04a6b92712e1d4f72df00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Start buiding vocabulary...:   0%|          | 0/18846 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc num 18846\n",
      "eliminate freq words\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5ab44f126d0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#                      documentembedding_normalize=documentembedding_normalize,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#                      embedding_dim=embedding_dim,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                      \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#                      load_embedding=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                        )\n",
      "\u001b[0;32m/tmp2/chrisliu/MSLAB_VectorDecomposition/utils/data_processing.py\u001b[0m in \u001b[0;36mget_process_data\u001b[0;34m(dataset, agg, embedding_type, word2embedding_path, word2embedding_normalize, min_df, max_df, max_seq_length)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;31m# Prepare document representations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0mdocument_word_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_document_tfidf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0mdocument_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weighted_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword2embedding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0mindex_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp2/chrisliu/MSLAB_VectorDecomposition/utils/data_processing.py\u001b[0m in \u001b[0;36mget_weighted_embedding\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;31m# Geberate weughted embedding.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         print(\"Vocabulary size:{}, Word embedding dim:{}\".format(\n",
      "\u001b[0;32m/tmp2/chrisliu/MSLAB_VectorDecomposition/utils/data_processing.py\u001b[0m in \u001b[0;36mbuild_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mIDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "embedding_type = ''\n",
    "dataset = '20news'\n",
    "documentembedding_normalize = False\n",
    "\n",
    "embedding_dim = 128\n",
    "data = get_process_data(dataset='20news', agg='IDF', embedding_type=embedding_type, \n",
    "                     word2embedding_path='../data/glove.6B.100d.txt', word2embedding_normalize=False,\n",
    "#                      documentembedding_normalize=documentembedding_normalize,\n",
    "#                      embedding_dim=embedding_dim, \n",
    "                     max_seq_length=128,\n",
    "#                      load_embedding=True\n",
    "                       )\n",
    "\n",
    "document_TFIDF = np.array(data[\"document_word_weight\"])\n",
    "document_vectors = np.array(data[\"document_embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a5adfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config[\"topk\"] = [10, 30, 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04a4563",
   "metadata": {},
   "source": [
    "## MLP Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da651ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDecoderDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 doc_vectors,\n",
    "                 weight_ans):\n",
    "        \n",
    "        assert len(doc_vectors) == len(weight_ans)\n",
    "\n",
    "        self.doc_vectors = torch.FloatTensor(doc_vectors)\n",
    "        self.weight_ans = torch.FloatTensor(weight_ans)        \n",
    "        self.weight_ans_s = torch.argsort(self.weight_ans, dim=1, descending=True)\n",
    "        self.topk = torch.sum(self.weight_ans > 0, dim=1)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.doc_vectors[idx], self.weight_ans[idx], self.weight_ans_s[idx], self.topk[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47717a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(batch_size=100, train_size_ratio=0.8, topk=50, TFIDF_normalize=False):\n",
    "    train_size = int(len(document_vectors) * train_size_ratio)\n",
    "    \n",
    "    print('train size', train_size)\n",
    "    print('valid size', len(document_vectors) - train_size)\n",
    "\n",
    "    if TFIDF_normalize:\n",
    "        # normalize TFIDF summation of each document to 1 \n",
    "        norm = document_TFIDF.sum(axis=1).reshape(-1, 1)\n",
    "        document_TFIDF_ = (document_TFIDF / norm)\n",
    "        # normalize TFIDF L2 norm of each document to 1\n",
    "        # norm = np.linalg.norm(document_TFIDF, axis=1).reshape(-1, 1)\n",
    "        # document_TFIDF_ = (document_TFIDF / norm)\n",
    "    else:\n",
    "        document_TFIDF_ = document_TFIDF\n",
    "    \n",
    "    # shuffle\n",
    "    randomize = np.arange(len(document_vectors))\n",
    "    np.random.shuffle(randomize)\n",
    "    document_vectors_ = document_vectors[randomize]\n",
    "    document_TFIDF_ = document_TFIDF_[randomize]\n",
    "    \n",
    "    # dataloader\n",
    "    train_dataset = MLPDecoderDataset(document_vectors_[:train_size], document_TFIDF_[:train_size])\n",
    "    train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "    valid_dataset = MLPDecoderDataset(document_vectors_[train_size:], document_TFIDF_[train_size:])\n",
    "    valid_loader  = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "    return train_loader, valid_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e0e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDecoder(nn.Module):\n",
    "    def __init__(self, doc_emb_dim, num_words, h_dim=300):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(doc_emb_dim, h_dim) \n",
    "        self.fc4 = nn.Linear(h_dim, num_words)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b51e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_MLPDecoder(model, data_loader):\n",
    "    results = defaultdict(list)\n",
    "    model.eval()\n",
    "        \n",
    "    # predict all data\n",
    "    for data in data_loader:\n",
    "        doc_embs, target, _, _ = data\n",
    "        \n",
    "        doc_embs = doc_embs.to(device)\n",
    "        target = target.to(device)\n",
    "                \n",
    "        pred = model(doc_embs)\n",
    "    \n",
    "        # Precision\n",
    "        precision_scores = retrieval_precision_all(pred, target, k=config[\"topk\"])\n",
    "        for k, v in precision_scores.items():\n",
    "            results['precision@{}'.format(k)].append(v)\n",
    "        \n",
    "        # NDCG\n",
    "        ndcg_scores = retrieval_normalized_dcg_all(pred, target, k=config[\"topk\"])\n",
    "        for k, v in ndcg_scores.items():\n",
    "            results['ndcg@{}'.format(k)].append(v)\n",
    "        \n",
    "    for k in results:\n",
    "        results[k] = np.mean(results[k])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e333e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader, valid_loader = prepare_dataloader(batch_size=100,\\\n",
    "#                                                 train_size_ratio=0.8, topk=30,\n",
    "#                                                 TFIDF_normalize=True)\n",
    "# for data in train_loader:\n",
    "#     doc_embs, target, target_rank, target_topk = data\n",
    "#     print(doc_embs.shape)\n",
    "#     print(target.shape)\n",
    "#     print(target_rank.shape)\n",
    "#     print(target_topk.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5472d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decoder(config):\n",
    "    model = MLPDecoder(\n",
    "        doc_emb_dim=document_vectors.shape[1], num_words=document_TFIDF.shape[1], h_dim=config[\"h_dim\"]).to(device)\n",
    "    model.train()\n",
    "\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"],\n",
    "                          weight_decay=config[\"weight_decay\"])\n",
    "    # prepare loss\n",
    "    if config[\"criterion\"] == \"MultiLabelMarginLoss\":\n",
    "        criterion = nn.MultiLabelMarginLoss(reduction='mean')\n",
    "    elif config[\"criterion\"].startswith(\"MultiLabelMarginLossCustomV\"):\n",
    "        def criterion(a, b, c): return MultiLabelMarginLossCustomV(\n",
    "            a, b, c, float(config[\"criterion\"].split(':')[-1]))\n",
    "    elif config[\"criterion\"].startswith(\"MultiLabelMarginLossCustom\"):\n",
    "        def criterion(a, b, c): return MultiLabelMarginLossCustom(\n",
    "            a, b, c, float(config[\"criterion\"].split(':')[-1]))\n",
    "    elif config[\"criterion\"] == \"ListNet\":\n",
    "        criterion = ListNet_origin\n",
    "    elif config[\"criterion\"] == \"ListNet2\":\n",
    "        criterion = ListNet2\n",
    "    elif config[\"criterion\"] == \"MSE\":\n",
    "        criterion = MSE\n",
    "    else:\n",
    "        print(\"loss not found\")\n",
    "        return\n",
    "\n",
    "    results = []\n",
    "    n_epoch = config[\"n_epoch\"]\n",
    "    valid_epoch = config[\"valid_epoch\"]\n",
    "    verbose = config[\"verbose\"]\n",
    "\n",
    "    for epoch in tqdm(range(n_epoch)):\n",
    "        train_loss_his = []\n",
    "        valid_loss_his = []\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for data in train_loader:\n",
    "            doc_embs, target, target_rank, target_topk = data\n",
    "            doc_embs = doc_embs.to(device)\n",
    "            target = target.to(device)\n",
    "            target_rank = target_rank.to(device)\n",
    "            target_topk = target_topk.to(device)\n",
    "\n",
    "            # loss\n",
    "            pred = model(doc_embs)\n",
    "            if config[\"criterion\"] == \"MultiLabelMarginLoss\":\n",
    "                target_rank[:, config[\"topk\"]] = -1\n",
    "                loss = criterion(pred, target_rank)\n",
    "            elif config[\"criterion\"].startswith(\"MultiLabelMarginLossCustomV\"):\n",
    "                loss = criterion(pred, target_rank, target_topk)\n",
    "            elif config[\"criterion\"].startswith(\"MultiLabelMarginLossCustom\"):\n",
    "                loss = criterion(pred, target_rank, config[\"topk\"])\n",
    "            elif config[\"criterion\"] == \"ListNet\":\n",
    "                loss = criterion(pred, target)\n",
    "            elif config[\"criterion\"] == \"ListNet2\":\n",
    "                loss = criterion(pred, target)\n",
    "            elif config[\"criterion\"] == \"MSE\":\n",
    "                loss = criterion(pred, target)\n",
    "\n",
    "            train_loss_his.append(loss.item())\n",
    "\n",
    "            # Model backwarding\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        model.eval()\n",
    "        for data in valid_loader:\n",
    "            doc_embs, target, target_rank, target_topk = data\n",
    "            doc_embs = doc_embs.to(device)\n",
    "            target = target.to(device)\n",
    "            target_rank = target_rank.to(device)\n",
    "            target_topk = target_topk.to(device)\n",
    "\n",
    "            # loss\n",
    "            pred = model(doc_embs)\n",
    "            if config[\"criterion\"] == \"MultiLabelMarginLoss\":\n",
    "                target_rank[:, config[\"topk\"]] = -1\n",
    "                loss = criterion(pred, target_rank)\n",
    "            elif config[\"criterion\"].startswith(\"MultiLabelMarginLossCustomV\"):\n",
    "                loss = criterion(pred, target_rank, target_topk)\n",
    "            elif config[\"criterion\"].startswith(\"MultiLabelMarginLossCustom\"):\n",
    "                loss = criterion(pred, target_rank, config[\"topk\"])\n",
    "            elif config[\"criterion\"] == \"ListNet\":\n",
    "                loss = criterion(pred, target)\n",
    "            elif config[\"criterion\"] == \"ListNet2\":\n",
    "                loss = criterion(pred, target)\n",
    "            elif config[\"criterion\"] == \"MSE\":\n",
    "                loss = criterion(pred, target)\n",
    "\n",
    "            valid_loss_his.append(loss.item())\n",
    "\n",
    "        print(\"Epoch\", epoch, np.mean(train_loss_his), np.mean(valid_loss_his))\n",
    "\n",
    "        # show decoder result\n",
    "        if epoch % valid_epoch == 0:\n",
    "            res = {}\n",
    "            res['epoch'] = epoch\n",
    "\n",
    "            train_res_ndcg = evaluate_MLPDecoder(model, train_loader)\n",
    "            valid_res_ndcg = evaluate_MLPDecoder(model, valid_loader)\n",
    "\n",
    "            res.update(valid_res_ndcg)\n",
    "            results.append(res)\n",
    "\n",
    "            if verbose:\n",
    "                print()\n",
    "                print('train', train_res_ndcg)\n",
    "                print('valid', valid_res_ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50b0eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataloader\n",
    "train_loader, valid_loader = prepare_dataloader(batch_size=100,\\\n",
    "                                                train_size_ratio=0.8,\n",
    "                                                TFIDF_normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346a54c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_config = {\n",
    "    \"lr\": 0.05,\n",
    "    \"momentum\": 0.0,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \n",
    "    \"n_epoch\": 200,\n",
    "    \"verbose\": True,\n",
    "    \"valid_epoch\": 10,\n",
    "    \n",
    "    \"topk\": 50,\n",
    "    \n",
    "    \"h_dim\": 3000,\n",
    "    \"criterion\": \"MultiLabelMarginLoss\", # \"ListNet\",\n",
    "    \"TFIDF_normalize\": False\n",
    "}\n",
    "\n",
    "train_decoder(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a0e689",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_config[\"lr\"] = 0.05\n",
    "train_config[\"criterion\"] = \"ListNet\"\n",
    "train_decoder(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c65f387",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config[\"lr\"] = 2\n",
    "train_config[\"criterion\"] = \"ListNet2\"\n",
    "train_decoder(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e982de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config[\"lr\"] = 0.05\n",
    "train_config[\"criterion\"] = \"MultiLabelMarginLossCustomV:1\"\n",
    "train_decoder(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323e24d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config[\"lr\"] = 0.1\n",
    "train_config[\"criterion\"] = \"MultiLabelMarginLossCustom:1\"\n",
    "train_decoder(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d931a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config[\"lr\"] = 1e-4\n",
    "train_config[\"criterion\"] = \"MSE\"\n",
    "train_decoder(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d734a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
