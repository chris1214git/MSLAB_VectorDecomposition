{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16096b03",
   "metadata": {},
   "source": [
    "# MLP baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815ade9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('../')\n",
    "from utils.eval import retrieval_normalized_dcg_all, retrieval_precision_all, retrieval_precision_all_v2, semantic_precision_all, semantic_precision_all_v2, precision_recall_f1_all\n",
    "from utils.loss import *\n",
    "from utils.data_loader import load_document\n",
    "from utils.toolbox import preprocess_document, get_preprocess_document, get_preprocess_document_embs,\\\n",
    "                          get_preprocess_document_labels, get_preprocess_document_labels_v2, get_word_embs,\\\n",
    "                          get_free_gpu, merge_targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61da837f",
   "metadata": {},
   "source": [
    "## Load Data, Label\n",
    "label -> bow, tf-idf, keybert, classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06caec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset ='20news'\n",
    "# cross domain\n",
    "dataset2 = None # None\n",
    "model_name = 'mpnet'\n",
    "label_type = 'tf-idf'\n",
    "# 用binary(f1) evaluation或rank evaluation\n",
    "eval_f1 = False\n",
    "criterion = 'ListNet_sigmoid_L1'#'ListNet_sigmoid_L1'\n",
    "# 選preprocess config\n",
    "preprocess_config_dir = 'parameters_baseline2'\n",
    "n_gram = 1\n",
    "\n",
    "lr = 1e-3\n",
    "n_epoch = 300\n",
    "valid_epoch = 10\n",
    "h_dim = 3000\n",
    "target_normalization = False\n",
    "\n",
    "# 訓練幾次\n",
    "n_time = 1\n",
    "seed = 133\n",
    "if dataset2:\n",
    "    experiment_dir = f'cross_{dataset}_{dataset2}_{model_name}_{label_type}_{criterion}'\n",
    "else:\n",
    "    experiment_dir = f'{dataset}_{model_name}_{label_type}_{criterion}'\n",
    "    \n",
    "save_dir = 'default1'\n",
    "\n",
    "config = {}\n",
    "config['experiment_dir'] = experiment_dir\n",
    "config['preprocess_config_dir'] = preprocess_config_dir\n",
    "config['save_dir'] = save_dir\n",
    "config['dataset'] = dataset\n",
    "config['dataset2'] = dataset2\n",
    "config['model_name'] = model_name\n",
    "config['label_type'] = label_type\n",
    "config['eval_f1'] = eval_f1\n",
    "config['n_gram'] = n_gram\n",
    "config['criterion'] = criterion\n",
    "config['n_time'] = n_time\n",
    "config['seed'] = seed\n",
    "\n",
    "config['lr'] = lr\n",
    "config['n_epoch'] = n_epoch\n",
    "config['valid_epoch'] = valid_epoch\n",
    "config['h_dim'] = h_dim\n",
    "config['target_normalization'] = target_normalization\n",
    "        \n",
    "save_dir = os.path.join('experiment', experiment_dir, config['save_dir'])\n",
    "os.makedirs(save_dir, exist_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ceca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(config, dataset):\n",
    "    preprocess_config_dir = config['preprocess_config_dir']\n",
    "    with open(os.path.join(f'../chris/{preprocess_config_dir}', f'preprocess_config_{dataset}.json'), 'r') as f:\n",
    "        preprocess_config = json.load(f)\n",
    "        \n",
    "    # load preprocess dataset\n",
    "    unpreprocessed_docs, preprocessed_docs = get_preprocess_document(**preprocess_config)\n",
    "    print('doc num', len(preprocessed_docs))\n",
    "\n",
    "    # get document embeddings\n",
    "    doc_embs, doc_model, device = get_preprocess_document_embs(preprocessed_docs, model_name)\n",
    "    print('doc_embs', doc_embs.shape)\n",
    "    \n",
    "    # load labels\n",
    "    labels, vocabularys = get_preprocess_document_labels_v2(preprocessed_docs, preprocess_config, preprocess_config_dir, config['n_gram'])    \n",
    "    # check nonzero numbers\n",
    "    for k in labels:\n",
    "        print(k, np.sum(labels[k]!=0), labels[k].shape)\n",
    "    print(len(vocabularys))\n",
    "    # select label type\n",
    "    targets = labels[config['label_type']].toarray()\n",
    "    vocabularys = vocabularys\n",
    "    \n",
    "    return unpreprocessed_docs ,preprocessed_docs, doc_embs, targets, vocabularys, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c09635",
   "metadata": {},
   "outputs": [],
   "source": [
    "unpreprocessed_docs, preprocessed_docs, doc_embs, targets, vocabularys, device = load_training_data(config, config['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c7cb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['dataset2'] is not None:\n",
    "    unpreprocessed_docs2, preprocessed_docs2, doc_embs2, targets2, vocabularys2, device = load_training_data(config, config['dataset2'])\n",
    "    targets, targets2, vocabularys = merge_targets(targets, targets2, vocabularys, vocabularys2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb108fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embs = get_word_embs(vocabularys)\n",
    "print('word_embs', word_embs.shape)\n",
    "word_embs_tensor = torch.FloatTensor(word_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f142b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "(targets[0] ** 2).sum()\n",
    "# targets[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04a4563",
   "metadata": {},
   "source": [
    "## MLP Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da651ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNDecoderDataset(Dataset):\n",
    "    def __init__(self, doc_embs, targets):\n",
    "        \n",
    "        assert len(doc_embs) == len(targets)\n",
    "\n",
    "        self.doc_embs = torch.FloatTensor(doc_embs)\n",
    "        self.targets = torch.FloatTensor(targets)        \n",
    "        self.targets_rank = torch.argsort(self.targets, dim=1, descending=True)\n",
    "        self.topk = torch.sum(self.targets > 0, dim=1)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.doc_embs[idx], self.targets[idx], self.targets_rank[idx], self.topk[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47717a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(doc_embs, targets, batch_size=100, train_valid_test_ratio=[0.7, 0.1, 0.2],\\\n",
    "                       target_normalize=False, seed=123):\n",
    "    train_size = int(len(doc_embs) * train_valid_test_ratio[0])\n",
    "    valid_size = int(len(doc_embs) * (train_valid_test_ratio[0] + train_valid_test_ratio[1])) - train_size\n",
    "    test_size = len(doc_embs) - train_size - valid_size\n",
    "    \n",
    "    print('Preparing dataloader')\n",
    "    print('train size', train_size)\n",
    "    print('valid size', valid_size)\n",
    "    print('test size', test_size)\n",
    "\n",
    "    if target_normalize:\n",
    "        # normalize target summation of each document to 1 \n",
    "        norm = targets.sum(axis=1).reshape(-1, 1)\n",
    "        targets = (targets / norm)\n",
    "        # normalize target L2 norm of each document to 1\n",
    "        # norm = np.linalg.norm(targets, axis=1).reshape(-1, 1)\n",
    "        # targets = (targets / norm)\n",
    "\n",
    "    # shuffle\n",
    "    randomize = np.arange(len(doc_embs))\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(randomize)\n",
    "    doc_embs = doc_embs[randomize]\n",
    "    targets = targets[randomize]\n",
    "    \n",
    "    # dataloader\n",
    "    train_dataset = DNNDecoderDataset(doc_embs[:train_size], targets[:train_size])\n",
    "    train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "    valid_dataset = DNNDecoderDataset(doc_embs[train_size:train_size+valid_size], targets[train_size:train_size+valid_size])\n",
    "    valid_loader  = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "    test_dataset = DNNDecoderDataset(doc_embs[train_size+valid_size:], targets[train_size+valid_size:])\n",
    "    test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def50778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataloader\n",
    "train_loader, valid_loader, test_loader = prepare_dataloader(doc_embs, targets, batch_size=64,\\\n",
    "                                                             train_valid_test_ratio=[0.6, 0.1, 0.2],\\\n",
    "                                                             target_normalize=config['target_normalization'],\\\n",
    "                                                             seed=seed)\n",
    "if config['dataset2'] is not None:\n",
    "    _, _, test_loader = prepare_dataloader(doc_embs2, targets2, batch_size=64,\\\n",
    "                                           train_valid_test_ratio=[0.6, 0.1, 0.2],\\\n",
    "                                           target_normalize=config['target_normalization'],\\\n",
    "                                           seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e0e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DNNDecoder(nn.Module):\n",
    "#     def __init__(self, doc_emb_dim, num_words, h_dim=300):\n",
    "#         super().__init__()\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(doc_emb_dim, h_dim),\n",
    "# #             nn.Dropout(p=0.2),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(h_dim, h_dim),\n",
    "#             nn.Dropout(p=0.3),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(h_dim, num_words),\n",
    "#             # nn.Dropout(p=0.5),\n",
    "#             # nn.Sigmoid(),\n",
    "#         )\n",
    "#     def forward(self, x):\n",
    "#         return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28658ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNDecoder(nn.Module):\n",
    "\n",
    "    ### casimir\n",
    "    # (1) Add parameter vocab_size\n",
    "    def __init__(self, doc_emb_dim, num_words=0, h_dim=300):\n",
    "        super(DNNDecoder, self).__init__()\n",
    "        vocab_size = num_words\n",
    "        bert_size = doc_emb_dim\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.network = nn.Sequential(\n",
    "#             nn.Dropout(p=0.1),\n",
    "            nn.Linear(bert_size, bert_size*4),\n",
    "            nn.BatchNorm1d(bert_size*4),\n",
    "            nn.Sigmoid(),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Mish(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(bert_size*4, vocab_size),\n",
    "            nn.BatchNorm1d(vocab_size),\n",
    "#             nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_bert):\n",
    "        recon_dist = self.network(x_bert)\n",
    "\n",
    "        return recon_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b51e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_DNNDecoder(model, data_loader, config, pred_semantic=False):\n",
    "    results = defaultdict(list)\n",
    "    model.eval()\n",
    "    \n",
    "    # predict all data\n",
    "    for data in data_loader:\n",
    "        doc_embs, target, _, _ = data\n",
    "        \n",
    "        doc_embs = doc_embs.to(device)\n",
    "        target = target.to(device)\n",
    "                \n",
    "        pred = model(doc_embs)\n",
    "        if config['eval_f1']:\n",
    "            # Precision / Recall / F1\n",
    "            p, r, f = precision_recall_f1_all(pred, target)\n",
    "            results['precision'].append(p)\n",
    "            results['recall'].append(r)\n",
    "            results['f1_score'].append(f)\n",
    "        else:\n",
    "            # Precision\n",
    "            precision_scores = retrieval_precision_all(pred, target, k=config[\"valid_topk\"])\n",
    "            for k, v in precision_scores.items():\n",
    "                results['precision@{}'.format(k)].append(v)\n",
    "\n",
    "            # Precision\n",
    "            precision_scores = retrieval_precision_all_v2(pred, target, k=config[\"valid_topk\"])\n",
    "            for k, v in precision_scores.items():\n",
    "                results['precisionv2@{}'.format(k)].append(v)\n",
    "\n",
    "            # NDCG\n",
    "            ndcg_scores = retrieval_normalized_dcg_all(pred, target, k=config[\"valid_topk\"])\n",
    "            for k, v in ndcg_scores.items():\n",
    "                results['ndcg@{}'.format(k)].append(v)\n",
    "            \n",
    "            # Semantic Precision\n",
    "            if pred_semantic:\n",
    "                semantic_precision_scores, word_result = semantic_precision_all(pred, target, word_embs_tensor, vocabularys,\\\n",
    "                                                                                k=config[\"valid_topk\"], th=0.5, display_word_result=False)\n",
    "                for k, v in semantic_precision_scores.items():\n",
    "                    results['semantic_precision@{}'.format(k)].append(v)\n",
    "                    \n",
    "                semantic_precision_scores, word_result = semantic_precision_all_v2(pred, target, word_embs_tensor, vocabularys,\\\n",
    "                                                                                k=config[\"valid_topk\"], th=0.5, display_word_result=False)\n",
    "                for k, v in semantic_precision_scores.items():\n",
    "                    results['semantic_precision_v2@{}'.format(k)].append(v)\n",
    "\n",
    "    for k in results:\n",
    "        results[k] = np.mean(results[k])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf3640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(train_train_config, criterion, pred, target, target_rank, target_topk):\n",
    "    if train_config[\"criterion\"] == \"MultiLabelMarginLoss\":\n",
    "        assert target_rank.shape[0] == len(target_topk)\n",
    "        for i in range(len(target_topk)):\n",
    "            target_rank[i, target_topk[i]] = -1\n",
    "        loss = criterion(pred, target_rank)\n",
    "    elif train_config[\"criterion\"].startswith(\"MultiLabelMarginLossCustomV\"):\n",
    "        loss = criterion(pred, target_rank, target_topk)\n",
    "    elif train_config[\"criterion\"].startswith(\"MultiLabelMarginLossCustom\"):\n",
    "        loss = criterion(pred, target_rank, train_config[\"loss_topk\"])\n",
    "    else:\n",
    "        loss = criterion(pred, target)\n",
    "        \n",
    "    return loss\n",
    "    \n",
    "def train_decoder(doc_embs, targets, train_config):\n",
    "    model = DNNDecoder(doc_emb_dim=doc_embs.shape[1], num_words=targets.shape[1],\\\n",
    "                       h_dim=train_config[\"h_dim\"]).to(device)\n",
    "    model.train()\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=train_config[\"lr\"], weight_decay=train_config[\"weight_decay\"])\n",
    "    # prepare loss\n",
    "    if train_config[\"criterion\"] == \"MultiLabelMarginLoss\":\n",
    "        criterion = nn.MultiLabelMarginLoss(reduction='mean')\n",
    "    elif train_config[\"criterion\"] == \"BCE\":\n",
    "        criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "    elif train_config[\"criterion\"].startswith(\"MultiLabelMarginLossCustomV\"):\n",
    "        def criterion(a, b, c): return MultiLabelMarginLossCustomV(\n",
    "            a, b, c, float(train_config[\"criterion\"].split(':')[-1]))\n",
    "    elif train_config[\"criterion\"].startswith(\"MultiLabelMarginLossCustom\"):\n",
    "        def criterion(a, b, c): return MultiLabelMarginLossCustom(\n",
    "            a, b, c, float(train_config[\"criterion\"].split(':')[-1]))\n",
    "    else:\n",
    "        criterion = eval(train_config[\"criterion\"])\n",
    "\n",
    "    results = []\n",
    "    n_epoch = train_config[\"n_epoch\"]\n",
    "    valid_epoch = train_config[\"valid_epoch\"]\n",
    "    valid_verbose = train_config[\"valid_verbose\"]\n",
    "\n",
    "    for epoch in tqdm(range(n_epoch)):\n",
    "        train_loss_his = []\n",
    "        valid_loss_his = []\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for data in train_loader:\n",
    "            doc_embs, target, target_rank, target_topk = data\n",
    "            doc_embs = doc_embs.to(device)\n",
    "            target = target.to(device)\n",
    "            target_rank = target_rank.to(device)\n",
    "            target_topk = target_topk.to(device)\n",
    "            # loss\n",
    "            pred = model(doc_embs)\n",
    "            loss = calculate_loss(train_config, criterion, pred, target, target_rank, target_topk)\n",
    "            train_loss_his.append(loss.item())\n",
    "\n",
    "            # Model backwarding\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        model.eval()\n",
    "        for data in valid_loader:\n",
    "            doc_embs, target, target_rank, target_topk = data\n",
    "            doc_embs = doc_embs.to(device)\n",
    "            target = target.to(device)\n",
    "            target_rank = target_rank.to(device)\n",
    "            target_topk = target_topk.to(device)\n",
    "\n",
    "            # loss\n",
    "            pred = model(doc_embs)\n",
    "            loss = calculate_loss(train_config, criterion, pred, target, target_rank, target_topk)\n",
    "            valid_loss_his.append(loss.item())\n",
    "\n",
    "        print(\"Epoch\", epoch, np.mean(train_loss_his), np.mean(valid_loss_his))\n",
    "\n",
    "        # show decoder result\n",
    "        if (valid_epoch > 0 and epoch % valid_epoch == 0) or epoch == n_epoch-1:\n",
    "            res = {}\n",
    "            res['epoch'] = epoch\n",
    "\n",
    "            train_res_ndcg = evaluate_DNNDecoder(model, train_loader, train_config, False)\n",
    "            valid_res_ndcg = evaluate_DNNDecoder(model, valid_loader, train_config, False)\n",
    "            test_res_ndcg = evaluate_DNNDecoder(model, test_loader, train_config, False)\n",
    "            \n",
    "            res['train'] = train_res_ndcg\n",
    "            res['valid'] = valid_res_ndcg\n",
    "            res['test'] = test_res_ndcg \n",
    "            results.append(res)\n",
    "\n",
    "            if valid_verbose:\n",
    "                print()\n",
    "                print('train', train_res_ndcg)\n",
    "                print('valid', valid_res_ndcg)\n",
    "                print('test', test_res_ndcg)\n",
    "                \n",
    "    del model\n",
    "    \n",
    "    return results\n",
    "\n",
    "def train_experiment(n_time):\n",
    "    # train n_time in different seed\n",
    "    results = []\n",
    "    for _ in range(n_time):\n",
    "        result = train_decoder(doc_embs, targets, train_config)\n",
    "        results.append(result)\n",
    "\n",
    "    with open(os.path.join(save_dir, 'result.json'), 'w') as f:\n",
    "        json.dump(results, f)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bc3b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = {\n",
    "    \"n_time\": config['n_time'],\n",
    "    \"lr\": config['lr'],\n",
    "    \"lr\": 1e-4,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"loss_topk\": 15,\n",
    "    \n",
    "    \"n_epoch\": config['n_epoch'],\n",
    "    \"valid_epoch\": config['valid_epoch'],\n",
    "    \"valid_verbose\": True,\n",
    "    \"valid_topk\": [5, 10, 15],\n",
    "    \n",
    "    \"h_dim\": config['h_dim'],\n",
    "    \"label_type\": config['label_type'],\n",
    "    \"eval_f1\": config['eval_f1'],\n",
    "    \"criterion\": 'MSE',#config['criterion'] 'ListNet_sigmoid_L1'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaee2a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_experiment(config['n_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102dbb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save config, training config\n",
    "with open(os.path.join(save_dir, 'config.json'), 'w') as f:\n",
    "    json.dump(config, f)\n",
    "with open(os.path.join(save_dir, 'train_config.json'), 'w') as f:\n",
    "    json.dump(train_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cdfbbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
