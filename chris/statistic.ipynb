{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dcf1df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.data_loader import load_document\n",
    "from utils.toolbox import preprocess_document, get_preprocess_document, get_preprocess_document_embs,\\\n",
    "                          get_preprocess_document_labels, get_preprocess_document_labels_v2, get_word_embs,\\\n",
    "                          get_free_gpu, merge_targets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d29c4520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(config, dataset):\n",
    "    preprocess_config_dir = config['preprocess_config_dir']\n",
    "    with open(os.path.join(f'../chris/{preprocess_config_dir}', f'preprocess_config_{dataset}.json'), 'r') as f:\n",
    "        preprocess_config = json.load(f)\n",
    "        \n",
    "    # load preprocess dataset\n",
    "    unpreprocessed_docs, preprocessed_docs = get_preprocess_document(**preprocess_config)\n",
    "    print('doc num', len(preprocessed_docs))\n",
    "    \n",
    "    # load labels\n",
    "    labels, vocabularys = get_preprocess_document_labels_v2(preprocessed_docs, preprocess_config, preprocess_config_dir, config['n_gram'])    \n",
    "    # check nonzero numbers\n",
    "    for k in labels:\n",
    "        print(k, np.sum(labels[k]!=0), labels[k].shape)\n",
    "    print(len(vocabularys))\n",
    "    # select label type\n",
    "    targets = labels[config['label_type']].toarray()\n",
    "    vocabularys = vocabularys\n",
    "    \n",
    "    return unpreprocessed_docs ,preprocessed_docs, targets, vocabularys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cc153fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset ='20news'\n",
    "label_type = 'tf-idf'\n",
    "preprocess_config_dir = 'parameters_baseline3'\n",
    "n_gram = 1\n",
    "\n",
    "config = {}\n",
    "config['preprocess_config_dir'] = preprocess_config_dir\n",
    "config['dataset'] = dataset\n",
    "config['label_type'] = label_type\n",
    "config['n_gram'] = n_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91612b1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config['dataset'] ='20news'\n",
    "unpreprocessed_docs ,preprocessed_docs, targets, vocabularys = load_training_data(config, config['dataset'])\n",
    "np.mean([len(preprocessed_docs[i].split()) for i in range(len(preprocessed_docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cba3e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['dataset'] ='agnews'\n",
    "unpreprocessed_docs ,preprocessed_docs, targets, vocabularys = load_training_data(config, config['dataset'])\n",
    "np.mean([len(preprocessed_docs[i].split()) for i in range(len(preprocessed_docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acff19ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting preprocess documents: wiki\n",
      "min_df: 200 max_df: 1.0 vocabulary_size: None min_doc_word: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (/home/chrisliu/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc num 664744\n",
      "Getting preprocess documents labels\n",
      "Finding precompute_keyword by preprocess_config {'dataset': 'wiki', 'min_df': 200, 'max_df': 1.0, 'vocabulary_size': None, 'min_doc_word': 15}\n",
      "Getting gensim tf-idf labels\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 86.7 GiB for an array with shape (664744, 17497) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6d9433ee51a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'wiki'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0munpreprocessed_docs\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mpreprocessed_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabularys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-97f6cc25d764>\u001b[0m in \u001b[0;36mload_training_data\u001b[0;34m(config, dataset)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# load labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabularys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_preprocess_document_labels_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_config_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_gram'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# check nonzero numbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MSLAB_VectorDecomposition/utils/toolbox.py\u001b[0m in \u001b[0;36mget_preprocess_document_labels_v2\u001b[0;34m(preprocessed_docs, preprocess_config, preprocess_config_dir, ngram)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;31m# covert sparse matrix to numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0msklearn_tf_idf_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m     \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed_docs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0mgensim_dct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtual_env/py37/lib/python3.7/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output array must be C or F contiguous'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtual_env/py37/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1202\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 86.7 GiB for an array with shape (664744, 17497) and data type float64"
     ]
    }
   ],
   "source": [
    "config['dataset'] ='wiki'\n",
    "unpreprocessed_docs ,preprocessed_docs, targets, vocabularys = load_training_data(config, config['dataset'])\n",
    "np.mean([len(preprocessed_docs[i].split()) for i in range(len(preprocessed_docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525a1253",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config['dataset'] ='IMDB'\n",
    "unpreprocessed_docs ,preprocessed_docs, targets, vocabularys = load_training_data(config, config['dataset'])\n",
    "np.mean([len(preprocessed_docs[i].split()) for i in range(len(preprocessed_docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e30c24f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
