{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "860e31ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from load_pretrain_label import load_preprocess_document_labels\n",
    "from model.ide_gan_decoder import IDEDataset, IDEGanDecoder\n",
    "from utils.toolbox import same_seeds, show_settings, record_settings, get_preprocess_document, get_preprocess_document_embs, get_word_embs, merge_targets\n",
    "from utils.eval import retrieval_normalized_dcg_all, retrieval_precision_all, semantic_precision_all, retrieval_precision_all_v2, semantic_precision_all_v2\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.set_num_threads(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b641a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import corpus2dense\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def get_preprocess_document_labels(preprocessed_docs, preprocess_config='../chris/parameters/preprocess_config.json'):\n",
    "    '''\n",
    "    Returns labels for document decoder\n",
    "\n",
    "            Parameters:\n",
    "                    preprocessed_docs (list): \n",
    "            Returns:\n",
    "                    labels (dict): bow, tf-idf\n",
    "                    vocabulary (dict): bow, tf-idf\n",
    "    '''\n",
    "    print('Getting preprocess documents labels')\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    # covert sparse matrix to numpy array\n",
    "    sklearn_tf_idf_vector = vectorizer.fit_transform(preprocessed_docs).toarray()\n",
    "    docs = [doc.split() for doc in preprocessed_docs]\n",
    "    gensim_dct = Dictionary(docs)\n",
    "    gensim_corpus = [gensim_dct.doc2bow(doc) for doc in docs]\n",
    "    model = TfidfModel(gensim_corpus, normalize=False)\n",
    "    gensim_vector = model[gensim_corpus]\n",
    "    gensim_tf_idf_vector = corpus2dense(gensim_vector, num_terms=len(gensim_dct.keys()), num_docs=gensim_dct.num_docs)\n",
    "    gensim_tf_idf_vector = np.array(gensim_tf_idf_vector).T.tolist()\n",
    "    bow_vector = sklearn_tf_idf_vector.copy()\n",
    "    bow_vector[bow_vector > 0] = 1\n",
    "    bow_vector[bow_vector < 0] = 0\n",
    "    vocabulary = vectorizer.get_feature_names()\n",
    "\n",
    "    labels = {}\n",
    "    labels['tf-idf'] = sklearn_tf_idf_vector\n",
    "    labels['tf-idf-gensim'] = np.array(gensim_tf_idf_vector)\n",
    "    labels['bow'] = bow_vector\n",
    "    \n",
    "    vocabularys = {}\n",
    "    vocabularys['tf-idf'] = vocabulary\n",
    "    vocabularys['tf-idf-gensim'] = list(zip(*gensim_dct.items()))[1]\n",
    "    vocabularys['bow'] = vocabulary\n",
    "\n",
    "    return labels, vocabularys, gensim_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3643b71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'experiment': 'with_classifier',\n",
    "    'model': 'IDE_GAN',\n",
    "    'architecture': 'concatenate',\n",
    "    'activation': 'sigmoid',\n",
    "    'dataset': '20news',\n",
    "    'vocab_size':0,\n",
    "    'encoder': 'mpnet',\n",
    "    'target': 'tf-idf-gensim',\n",
    "    'seed': 123,\n",
    "    'epochs': 3000,\n",
    "    'lr': 1e-4,\n",
    "    'optim': 'AdamW',\n",
    "    'scheduler': True,\n",
    "    'warmup': 'linear',\n",
    "    'warmup_proportion': 0.1, \n",
    "    'loss': 'listnet',\n",
    "    'batch_size': 32,\n",
    "    'weight_decay': 0,\n",
    "    'ratio': 0.1,\n",
    "    'topk': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'save': False,\n",
    "    'threshold': 0.7,\n",
    "}\n",
    "same_seeds(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e85abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(config, balance=False):    \n",
    "    # Data preprocessing\n",
    "    unpreprocessed_corpus ,preprocessed_corpus = get_preprocess_document(**config)\n",
    "    texts = [text.split() for text in preprocessed_corpus]\n",
    "    print('[INFO] Load corpus done.')\n",
    "\n",
    "    # Generating document embedding\n",
    "    while True:\n",
    "        try:\n",
    "            doc_embs, doc_model, device = get_preprocess_document_embs(preprocessed_corpus, config['encoder'])\n",
    "            break\n",
    "        except:\n",
    "            print('[Error] CUDA Memory Insufficient, retry after 15 secondes.')\n",
    "            time.sleep(15)\n",
    "    print('[INFO] Generate embedding done.')\n",
    "    \n",
    "    # Generate Decode target & Vocabulary\n",
    "    if config['target'] == 'keybert' or config['target'] == 'yake':\n",
    "        labels, vocabularys, gensim_dct = load_preprocess_document_labels(config)\n",
    "        label = labels[config['target']].toarray()\n",
    "    else:\n",
    "        labels, vocabularys, gensim_dct= get_preprocess_document_labels(preprocessed_corpus)\n",
    "        label = labels[config['target']]\n",
    "        vocabularys = vocabularys[config['target']]\n",
    "    print('[INFO] Load label done.')\n",
    "    \n",
    "    # generate idx to token\n",
    "    id2token = {k: v for k, v in zip(range(0, len(vocabularys)), vocabularys)}\n",
    "    print('[INFO] Generate id2token done.')\n",
    "    \n",
    "    idx = np.arange(len(unpreprocessed_corpus))\n",
    "    np.random.shuffle(idx)\n",
    "    train_length = int(len(unpreprocessed_corpus) * 0.8)\n",
    "    train_idx = idx[:train_length]\n",
    "    valid_idx = idx[train_length:]\n",
    "\n",
    "    train_unpreprocessed_corpus = list(np.array(unpreprocessed_corpus)[train_idx])\n",
    "    valid_unpreprocessed_corpus = list(np.array(unpreprocessed_corpus)[valid_idx])\n",
    "    train_embs = np.array(doc_embs)[train_idx]\n",
    "    valid_embs = np.array(doc_embs)[valid_idx]\n",
    "    train_label = np.array(label)[train_idx]\n",
    "    valid_label = np.array(label)[valid_idx]\n",
    "    \n",
    "    # Generate labeled mask\n",
    "    label_masks = np.zeros((train_embs.shape[0], 1), dtype=bool)\n",
    "    num_labeled_data = int(train_embs.shape[0] * config['ratio'])\n",
    "    while True:\n",
    "        if num_labeled_data > 0:\n",
    "            idx = random.randrange(0, train_embs.shape[0])\n",
    "            if label_masks[idx] == 0:\n",
    "                label_masks[idx] = 1\n",
    "                num_labeled_data -= 1\n",
    "        else:\n",
    "            break\n",
    "    print('[INFO] mask labels done.')\n",
    "    print(num_labeled_data)\n",
    "\n",
    "    # Balance data if required\n",
    "    original_num_data = train_embs.shape[0]\n",
    "    if config['ratio'] != 1 and balance:\n",
    "        for idx in range(original_num_data): \n",
    "            if label_masks[idx]:\n",
    "                balance = int(1/config['ratio'])\n",
    "                balance = int(math.log(balance,2))\n",
    "                if balance < 1:\n",
    "                    balance = 1\n",
    "                for b in range(0, int(balance)):\n",
    "                    train_unpreprocessed_corpus.append(train_preprocessed_corpus[idx])\n",
    "                    train_embs = np.concatenate((train_embs, train_embs[idx].reshape(1, train_embs.shape[1])), axis=0)\n",
    "                    train_label = np.concatenate((train_label, train_label[idx].reshape(1, train_label.shape[1])), axis=0)\n",
    "                    label_masks = np.concatenate((label_masks, label_masks[idx].reshape(1, label_masks.shape[1])), axis=0)\n",
    "    \n",
    "    training_set = IDEDataset(train_unpreprocessed_corpus, train_embs, train_label, label_masks)\n",
    "    validation_set = IDEDataset(valid_unpreprocessed_corpus, valid_embs, valid_label, np.ones((valid_embs.shape[0], 1), dtype=bool))\n",
    "    \n",
    "    return training_set, validation_set, vocabularys, id2token, gensim_dct, device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b4ffbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter\n",
    "if config['dataset'] == '20news':\n",
    "    config['min_df'], config['max_df'], config['min_doc_word'] = 62, 1.0, 15\n",
    "elif config['dataset'] == 'agnews':\n",
    "    config['min_df'], config['max_df'], config['min_doc_word'] = 425, 1.0, 15\n",
    "elif config['dataset'] == 'IMDB':\n",
    "    config['min_df'], config['max_df'], config['min_doc_word'] = 166, 1.0, 15\n",
    "elif config['dataset'] == 'wiki':\n",
    "    config['min_df'], config['max_df'], config['min_doc_word'] = 2872, 1.0, 15\n",
    "elif config['dataset'] == 'tweet':\n",
    "    config['min_df'], config['max_df'], config['min_doc_word'] = 5, 1.0, 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e9a2d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting preprocess documents: 20news\n",
      "min_df: 62 max_df: 1.0 vocabulary_size: None min_doc_word: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/casimir0304/miniconda3/envs/ide/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Load corpus done.\n",
      "Getting preprocess documents embeddings\n",
      "Using cuda 4 for training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a790053ebf644a798cae76cd36087693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generate embedding done.\n",
      "Getting preprocess documents labels\n",
      "[INFO] Load label done.\n",
      "[INFO] Generate id2token done.\n",
      "[INFO] mask labels done.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "training_set, validation_set, vocabularys, id2token, gensim_dct, device = generate_dataset(config, balance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa4c070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Singular_MythNet(y_pred, y_true, eps=1e-10):\n",
    "\t# ListNet switch softmax to L1 norm\n",
    "    # (1) y_pred: the decoded vector. \n",
    "    #     ex: tfidf score of each word in certain document.\n",
    "    # (2) y_true: the vector before encoded. \n",
    "    #     ex: same as above.\n",
    "    # (3) eps: a small number to avoid error when computing log operation. \n",
    "    #     ex: log0 will cause error while log(0+eps) will not.\n",
    "\n",
    "    y_pred = torch.sigmoid(y_pred) \n",
    "    y_pred = torch.nn.functional.normalize(y_pred, dim=1, p=1)\n",
    "    # y_true = torch.nn.functional.softmax(y_true, dim=1) \n",
    "    y_true = torch.nn.functional.normalize(y_true, dim=1, p=1)\n",
    "    pred = y_pred + eps\n",
    "    pred_log = torch.log(pred)\n",
    "\n",
    "    return torch.sum(-y_true * pred_log, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5866087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule_with_warmup, BertTokenizer, BertForMaskedLM, RobertaTokenizer, RobertaForMaskedLM, AlbertTokenizer, AlbertForMaskedLM\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(\"./\")\n",
    "from utils.eval import retrieval_normalized_dcg_all, retrieval_precision_all, semantic_precision_all, retrieval_precision_all_v2, semantic_precision_all_v2\n",
    "from utils.toolbox import get_free_gpu, record_settings\n",
    "\n",
    "class IDEDataset(Dataset):\n",
    "    def __init__(self, corpus, emb, target, mask):\n",
    "        \n",
    "        assert len(emb) == len(target)\n",
    "        self.corpus = corpus\n",
    "        self.emb = torch.FloatTensor(emb)\n",
    "        self.target = torch.FloatTensor(target)\n",
    "        self.mask = torch.BoolTensor(mask)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.corpus[idx], self.emb[idx], self.target[idx], self.mask[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emb)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim=100, output_dim=768, hidden_dim=512, dropout=0.2):\n",
    "        super(Generator, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True), \n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        return self.layers(noise)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim=768, output_dim=100, num_labels=2, dropout=0.2):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim*4),\n",
    "            nn.BatchNorm1d(input_dim*4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(input_dim*4, output_dim),\n",
    "            nn.BatchNorm1d(output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, embs):\n",
    "        recons = self.decoder(embs)\n",
    "        return recons\n",
    "    \n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim=768, output_dim=2, dropout=0.2):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, embs):\n",
    "        return self.classifier(embs)\n",
    "\n",
    "class BertFamily(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = BertForMaskedLM.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "    def forward(self, documents):\n",
    "        return self.get_docvec(documents)\n",
    "\n",
    "    def get_docvec(self, documents):\n",
    "        inputs = self.tokenizer(documents, return_tensors='pt', padding=True,\n",
    "                                truncation=True, max_length=128).to(self.device)\n",
    "        embedding = self.model.bert(**inputs).last_hidden_state[:, 0, :]\n",
    "        return embedding    \n",
    "    \n",
    "class IDEGanDecoder:\n",
    "    def __init__(self, config, train_set, valid_set, vocab = None, id2token=None, gensim_dct=None ,device=None, contextual_dim=768, noise_dim=100, word_embeddings=None, dropout=0.2, momentum=0.99, num_data_loader_workers=mp.cpu_count(), loss_weights=None, eps=1e-8):\n",
    "        self.config = config\n",
    "        self.train_set = train_set\n",
    "        self.valid_set = valid_set\n",
    "        self.vocab = vocab\n",
    "        self.id2token = id2token\n",
    "        self.gensim_dct = gensim_dct\n",
    "        self.device = device\n",
    "        self.contextual_dim = contextual_dim\n",
    "        self.noise_dim = noise_dim\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.dropout = dropout\n",
    "        self.momentum = momentum\n",
    "        self.num_data_loader_workers = num_data_loader_workers\n",
    "        self.loss_weights = loss_weights\n",
    "        self.eps = eps\n",
    "        self.cls_loss = torch.nn.CrossEntropyLoss()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "        # self.generator = Generator(input_dim=noise_dim, output_dim=contextual_dim, dropout=dropout)\n",
    "        self.discriminator = Discriminator(input_dim=contextual_dim, output_dim=len(vocab), num_labels=2, dropout=dropout)\n",
    "        self.classifier = Classifier(input_dim=contextual_dim, output_dim=2)\n",
    "        self.bert = BertFamily(device)\n",
    "        \n",
    "        if config['optim'] == 'AdamW':\n",
    "            # self.gen_optimizer = AdamW(self.generator.parameters(), lr=config['lr'], eps=eps)\n",
    "            self.dis_optimizer = AdamW(self.discriminator.parameters(), lr=config['lr'], eps=eps)\n",
    "            self.cls_optimizer = AdamW(self.classifier.parameters(), lr=config['lr'], eps=eps)\n",
    "            self.bert_optimizer = AdamW(self.bert.parameters(), lr=config['lr'], eps=eps)\n",
    "        else:\n",
    "            # self.gen_optimizer = Adam(self.generator.parameters(), lr=config['lr'], betas=(self.momentum, 0.99), weight_decay=config['weight_decay'])\n",
    "            self.dis_optimizer = Adam(self.discriminator.parameters(), lr=config['lr'], betas=(self.momentum, 0.99), weight_decay=config['weight_decay'])\n",
    "            self.cls_optimizer = Adam(self.classifier.parameters(), lr=config['lr'], betas=(self.momentum, 0.99), weight_decay=config['weight_decay'])\n",
    "            self.bert_optimizer = Adam(self.bert.parameters(), lr=config['lr'], betas=(self.momentum, 0.99), weight_decay=config['weight_decay'])\n",
    "            \n",
    "        if config['scheduler']:\n",
    "            num_training_steps = int(len(train_set) / config['batch_size'] * config['epochs'])\n",
    "            num_warmup_steps = int(num_training_steps * config['warmup_proportion'])\n",
    "            # self.gen_optimizer = AdamW(self.generator.parameters(), lr=config['lr'], eps=eps)\n",
    "            self.dis_optimizer = AdamW(self.discriminator.parameters(), lr=config['lr'], eps=eps)\n",
    "            self.cls_optimizer = AdamW(self.classifier.parameters(), lr=config['lr'], eps=eps)\n",
    "            self.bert_optimizer = AdamW(self.bert.parameters(), lr=config['lr'], eps=eps)\n",
    "            if config['warmup'] == 'linear':\n",
    "                # self.gen_scheduler = get_linear_schedule_with_warmup(self.gen_optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "                self.dis_scheduler = get_linear_schedule_with_warmup(self.dis_optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "                self.cls_scheduler = get_linear_schedule_with_warmup(self.cls_optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "                self.bert_scheduler = get_linear_schedule_with_warmup(self.bert_optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "            \n",
    "            else:\n",
    "                # self.gen_scheduler = get_constant_schedule_with_warmup(self.gen_optimizer, num_warmup_steps=num_warmup_steps)\n",
    "                self.dis_scheduler = get_constant_schedule_with_warmup(self.dis_optimizer, num_warmup_steps=num_warmup_steps)\n",
    "                self.cls_scheduler = get_constant_schedule_with_warmup(self.cls_optimizer, num_warmup_steps=num_warmup_steps)\n",
    "                self.bert_scheduler = get_constant_schedule_with_warmup(self.bert_optimizer, num_warmup_steps=num_warmup_steps)\n",
    "                \n",
    "    def training(self, epoch, loader):\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch + 1, config['epochs']))\n",
    "        print('Training...')\n",
    "\n",
    "        gen_train_loss, dis_train_loss, cls_train_loss = 0, 0, 0\n",
    "\n",
    "        # self.generator.train()\n",
    "        self.discriminator.train()\n",
    "        self.classifier.train()\n",
    "        self.bert.train()\n",
    "\n",
    "        for batch, (corpus, embs, labels, masks) in enumerate(loader):\n",
    "            real_embs, labels, masks = embs.to(self.device), labels.to(self.device), masks.to(self.device)\n",
    "            cur_batch_size = embs.shape[0]\n",
    "\n",
    "            # noise = torch.zeros(cur_batch_size, 100, device=self.device).uniform_(0, 1)\n",
    "            # fake_embs = self.generator(noise)\n",
    "            \n",
    "            # fake label from BERT\n",
    "            noise = torch.empty([cur_batch_size, 100], dtype=torch.long).random_(len(self.vocab))\n",
    "            noise_docs = []\n",
    "            for i in range(cur_batch_size):\n",
    "                noise_doc = []\n",
    "                for j in range(100):\n",
    "                    noise_doc.append(id2token[int(noise[i][j])])\n",
    "                noise_docs.append(noise_doc)\n",
    "            gensim_corpus = [self.gensim_dct.doc2bow(doc) for doc in noise_docs]\n",
    "            model = TfidfModel(gensim_corpus, normalize=False)\n",
    "            gensim_vector = model[gensim_corpus]\n",
    "            gensim_tf_idf_vector = corpus2dense(gensim_vector, num_terms=len(gensim_dct.keys()), num_docs=cur_batch_size)\n",
    "            gensim_tf_idf_vector = np.array(gensim_tf_idf_vector).T.tolist()\n",
    "            fake_labels = torch.FloatTensor(gensim_tf_idf_vector).to(self.device)\n",
    "            #\n",
    "            noise_corpus = [\" \".join(doc) for doc in noise_docs]\n",
    "            fake_embs = self.bert(noise_corpus).to(device)\n",
    "\n",
    "            mixed_embs = torch.cat((real_embs, fake_embs), dim=0)\n",
    "            \n",
    "            logits = self.classifier(mixed_embs)\n",
    "            logits_list = torch.split(logits, cur_batch_size)\n",
    "            D_real_logits = logits_list[0]\n",
    "            D_fake_logits = logits_list[1]\n",
    "            \n",
    "            probs_list = torch.split(self.softmax(logits), cur_batch_size)\n",
    "            D_real_probs = probs_list[0]\n",
    "            D_fake_probs = probs_list[1]\n",
    "            \n",
    "            recons = self.discriminator(mixed_embs)\n",
    "            recons_list = torch.split(recons, cur_batch_size)\n",
    "            D_real_recons = recons_list[0]\n",
    "            D_fake_recons = recons_list[1]\n",
    "            \n",
    "            # Classifier's LOSS\n",
    "            c_loss_r = self.cls_loss(D_real_logits, torch.ones(cur_batch_size, dtype=torch.long).to(self.device))\n",
    "            c_loss_f = self.cls_loss(D_fake_logits, torch.zeros(cur_batch_size, dtype=torch.long).to(self.device))\n",
    "            cls_loss = c_loss_r + c_loss_f\n",
    "            \n",
    "            # Generator's LOSS\n",
    "            # g_loss_d = torch.reciprocal(c_loss_f + self.eps)#-1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + self.eps))\n",
    "            # g_recon_reg = -1 * torch.log(self.relu(torch.nn.functional.cosine_similarity(torch.mean(real_embs, dim=0), torch.mean(fake_embs, dim=0), dim=0)) + self.eps)\n",
    "            # gen_loss = g_loss_d + g_recon_reg\n",
    "            \n",
    "            # BERT's LOSS\n",
    "            g_loss_d = torch.reciprocal(c_loss_f + self.eps)#-1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + self.eps))\n",
    "            g_recon_reg = -1 * torch.log(self.relu(torch.nn.functional.cosine_similarity(torch.mean(real_embs, dim=0), torch.mean(fake_embs, dim=0), dim=0)) + self.eps)\n",
    "            g_recon_weight = self.relu(torch.nn.functional.cosine_similarity(torch.mean(real_embs, dim=0), torch.mean(fake_embs, dim=0), dim=0))\n",
    "            gen_loss = g_loss_d + g_recon_reg\n",
    "    \n",
    "            # Disciminator's LOSS\n",
    "            recon_loss = torch.masked_select(Singular_MythNet(D_real_recons, labels), torch.flatten(masks))\n",
    "            fake_recon_loss = Singular_MythNet(D_fake_recons, fake_labels) * g_recon_weight\n",
    "            labeled_count = recon_loss.type(torch.float32).numel()\n",
    "            if labeled_count == 0:\n",
    "                D_L_Supervised = torch.mean(fake_recon_loss)\n",
    "            else:\n",
    "                D_L_Supervised = torch.mean(recon_loss) + torch.mean(fake_recon_loss)          \n",
    "            #D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + self.eps))\n",
    "            #D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + self.eps))\n",
    "            dis_loss = D_L_Supervised + cls_loss#+ D_L_unsupervised1U + D_L_unsupervised2U\n",
    "\n",
    "            # self.gen_optimizer.zero_grad()\n",
    "            self.bert_optimizer.zero_grad()\n",
    "            self.dis_optimizer.zero_grad()\n",
    "            self.cls_optimizer.zero_grad()\n",
    "\n",
    "            gen_loss.backward(retain_graph=True)\n",
    "            cls_loss.backward(retain_graph=True)\n",
    "            dis_loss.backward() \n",
    "            \n",
    "            # self.gen_optimizer.step()\n",
    "            self.bert_optimizer.step()\n",
    "            self.dis_optimizer.step()\n",
    "            self.cls_optimizer.step()\n",
    "\n",
    "            if config['scheduler']:\n",
    "                # self.gen_scheduler.step()\n",
    "                self.bert_scheduler.step()\n",
    "                self.dis_scheduler.step()\n",
    "                self.cls_scheduler.step()\n",
    "\n",
    "            gen_train_loss += gen_loss.item()\n",
    "            dis_train_loss += dis_loss.item()\n",
    "            cls_train_loss += cls_loss.item()\n",
    "            #break\n",
    "        avg_gen_train_loss = gen_train_loss / len(loader)\n",
    "        avg_dis_train_loss = dis_train_loss / len(loader)   \n",
    "        avg_cls_train_loss = cls_train_loss / len(loader) \n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss generetor: {0:.3f}\".format(avg_gen_train_loss))\n",
    "        print(\"  Average training loss discriminator: {0:.3f}\".format(avg_dis_train_loss))\n",
    "        print(\"  Average training loss classifier: {0:.3f}\".format(avg_cls_train_loss))\n",
    "        return avg_gen_train_loss, avg_dis_train_loss, avg_cls_train_loss\n",
    "\n",
    "    def validation(self, loader):\n",
    "        # self.generator.eval()\n",
    "        self.discriminator.eval()\n",
    "        self.classifier.eval()\n",
    "        self.bert.eval()\n",
    "        \n",
    "        results = defaultdict(list)\n",
    "        with torch.no_grad():\n",
    "            for batch, (corpus, embs, labels, masks) in enumerate(loader):\n",
    "                embs, labels = embs.to(self.device), labels.to(self.device)\n",
    "                recons = self.discriminator(embs)\n",
    "                \n",
    "                # Precision for reconstruct\n",
    "                precision_scores = retrieval_precision_all(recons, labels, k=config['topk'])\n",
    "                for k, v in precision_scores.items():\n",
    "                    results['[Recon] Precision v1@{}'.format(k)].append(v)\n",
    "                \n",
    "                precision_scores = retrieval_precision_all_v2(recons, labels, k=config['topk'])\n",
    "                for k, v in precision_scores.items():\n",
    "                    results['[Recon] Precision v2@{}'.format(k)].append(v)\n",
    "\n",
    "                # NDCG for reconstruct\n",
    "                ndcg_scores = retrieval_normalized_dcg_all(recons, labels, k=config['topk'])\n",
    "                for k, v in ndcg_scores.items():\n",
    "                    results['[Recon] ndcg@{}'.format(k)].append(v)\n",
    "\n",
    "        for k in results:\n",
    "            results[k] = np.mean(results[k])\n",
    "                \n",
    "        return results\n",
    "\n",
    "    def fit(self):\n",
    "        # self.generator.to(self.device)\n",
    "        self.discriminator.to(self.device)\n",
    "        self.classifier.to(self.device)\n",
    "        self.bert.to(self.device)\n",
    "\n",
    "        train_loader = DataLoader(self.train_set, batch_size=self.config['batch_size'], shuffle=True, num_workers=self.num_data_loader_workers)\n",
    "        valid_loader = DataLoader(self.valid_set, batch_size=self.config['batch_size'], shuffle=False, num_workers=self.num_data_loader_workers)\n",
    "\n",
    "        for epoch in range(self.config['epochs']):\n",
    "            gen_train_loss, dis_train_loss, cls_train_loss = self.training(epoch, train_loader)\n",
    "            #break\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                val_res = self.validation(valid_loader)\n",
    "                record = open('./ide_gan_'+self.config['experiment']+'_'+self.config['dataset']+'_'+self.config['encoder']+'_'+self.config['target']+'_loss_'+self.config['loss']+'_lr'+str(self.config['lr'])+'_optim'+self.config['optim']+'_batch'+str(self.config['batch_size'])+'_weightdecay'+str(self.config['weight_decay'])+'.txt', 'a')\n",
    "                print('---------------------------------------')\n",
    "                record.write('-------------------------------------------------\\n')\n",
    "                for key,val in val_res.items():\n",
    "                    print(f\"{key}:{val:.4f}\")\n",
    "                    record.write(f\"{key}:{val:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55993378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3000 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss generetor: 13.162\n",
      "  Average training loss discriminator: 9.617\n",
      "  Average training loss classifier: 1.237\n",
      "\n",
      "======== Epoch 2 / 3000 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss generetor: 4.246\n",
      "  Average training loss discriminator: 10.198\n",
      "  Average training loss classifier: 1.394\n",
      "\n",
      "======== Epoch 3 / 3000 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss generetor: 3.706\n",
      "  Average training loss discriminator: 10.508\n",
      "  Average training loss classifier: 1.401\n",
      "\n",
      "======== Epoch 4 / 3000 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss generetor: 3.594\n",
      "  Average training loss discriminator: 10.502\n",
      "  Average training loss classifier: 1.386\n",
      "\n",
      "======== Epoch 5 / 3000 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss generetor: 3.569\n",
      "  Average training loss discriminator: 10.443\n",
      "  Average training loss classifier: 1.382\n",
      "\n",
      "======== Epoch 6 / 3000 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss generetor: 3.565\n",
      "  Average training loss discriminator: 10.479\n",
      "  Average training loss classifier: 1.376\n",
      "\n",
      "======== Epoch 7 / 3000 ========\n",
      "Training...\n"
     ]
    }
   ],
   "source": [
    "model = IDEGanDecoder(config, training_set, validation_set, vocabularys, id2token, gensim_dct, device)\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fa25be41",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.empty([32, 100], dtype=torch.long).random_(len(vocabularys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1eaa3d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_docs = []\n",
    "for i in range(32):\n",
    "    noise_doc = []\n",
    "    for j in range(100):\n",
    "        noise_doc.append(id2token[int(noise[i][j])])\n",
    "    noise_docs.append(noise_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0f47df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs = [\" \".join(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "17ba5e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_corpus = [gensim_dct.doc2bow(doc) for doc in noise_docs]\n",
    "model = TfidfModel(gensim_corpus, normalize=False)\n",
    "gensim_vector = model[gensim_corpus]\n",
    "gensim_tf_idf_vector = corpus2dense(gensim_vector, num_terms=len(gensim_dct.keys()), num_docs=32)\n",
    "gensim_tf_idf_vector = np.array(gensim_tf_idf_vector).T.tolist()\n",
    "labels = torch.FloatTensor(gensim_tf_idf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "85ad6078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4829])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad921d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_ide)",
   "language": "python",
   "name": "conda_ide"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
