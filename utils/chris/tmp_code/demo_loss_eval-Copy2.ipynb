{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24ecc693",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "815ade9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/chrisliu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Used to get the data\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "sys.path.append('../')\n",
    "from utils.eval import retrieval_normalized_dcg_all, retrieval_precision_all\n",
    "from utils.loss import ListNet, listNet_origin, MultiLabelMarginLossPos, MSE, KL\n",
    "from utils.data_processing import get_process_data\n",
    "\n",
    "seed = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f477c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2embedding from ../data/glove.6B.100d.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1373c691014ddba70d506a26c8df9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of words:400000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b6db71941c407da5eec18b53c0597d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Start buiding vocabulary...', max=18846, style=ProgressStyle(â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "doc num 18846\n",
      "eliminate freq words\n",
      "Load from saving\n",
      "delete items 150\n"
     ]
    }
   ],
   "source": [
    "embedding_type = ''\n",
    "dataset = '20news'\n",
    "documentembedding_normalize = True\n",
    "\n",
    "embedding_dim = 128\n",
    "data = get_process_data(dataset='20news', agg='IDF', embedding_type=embedding_type, \n",
    "                     word2embedding_path='../data/glove.6B.100d.txt', word2embedding_normalize=False,\n",
    "                     documentembedding_normalize=documentembedding_normalize,\n",
    "                     embedding_dim=embedding_dim, max_seq_length=128,\n",
    "                     load_embedding=True)\n",
    "\n",
    "document_TFIDF = np.array(data[\"document_word_weight\"])\n",
    "document_vectors = np.array(data[\"document_embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09b305fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config[\"topk\"] = [10, 30, 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04a4563",
   "metadata": {},
   "source": [
    "## MLP Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f3b320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84f60a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDecoderDataset1(Dataset):\n",
    "    # MultiLabelMarginLoss\n",
    "    # ListNet\n",
    "    def __init__(self, \n",
    "                 doc_vectors,\n",
    "                 weight_ans,\n",
    "                 topk=50):\n",
    "        \n",
    "        assert len(doc_vectors) == len(weight_ans)\n",
    "\n",
    "        self.doc_vectors = torch.FloatTensor(doc_vectors)\n",
    "        self.weight_ans = torch.FloatTensor(weight_ans)        \n",
    "        self.weight_ans_s = torch.argsort(self.weight_ans, dim=1, descending=True)\n",
    "        self.weight_ans_s[:, topk:] = -1\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.doc_vectors[idx], self.weight_ans[idx], self.weight_ans_s[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "816a920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDecoderDataset2(Dataset):\n",
    "    # MultiLabelMarginLossPos\n",
    "    def __init__(self, \n",
    "                 doc_vectors,\n",
    "                 weight_ans,\n",
    "                 topk=50):\n",
    "        self.doc_vectors = torch.FloatTensor(doc_vectors)\n",
    "        self.weight_ans = torch.FloatTensor(weight_ans)\n",
    "        self.weight_ans_s = torch.argsort(self.weight_ans, dim=1, descending=True)\n",
    "        \n",
    "        self.weight_ans_s_pos = self.weight_ans_s[:, :topk]\n",
    "        self.weight_ans_s_neg = self.weight_ans_s[:, topk:]\n",
    "        \n",
    "        assert len(doc_vectors) == len(weight_ans)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.doc_vectors[idx], self.weight_ans[idx], (self.weight_ans_s_pos[idx], self.weight_ans_s_neg[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cd83a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(batch_size=100, train_size_ratio=0.8, topk=50, TFIDF_normalize=False):\n",
    "    train_size = int(len(document_vectors) * train_size_ratio)\n",
    "    \n",
    "    print('train size', train_size)\n",
    "    print('valid size', len(document_vectors) - train_size)\n",
    "\n",
    "    if TFIDF_normalize:\n",
    "        norm = document_TFIDF.sum(axis=1).reshape(-1, 1)\n",
    "        document_TFIDF_ = (document_TFIDF / norm)\n",
    "#         norm = np.linalg.norm(document_TFIDF, axis=1).reshape(-1, 1)\n",
    "#         document_TFIDF_ = (document_TFIDF / norm)\n",
    "    else:\n",
    "        document_TFIDF_ = document_TFIDF\n",
    "        \n",
    "    randomize = np.arange(len(document_vectors))\n",
    "    np.random.shuffle(randomize)\n",
    "    \n",
    "    document_vectors_ = document_vectors[randomize]\n",
    "    document_TFIDF_ = document_TFIDF_[randomize]\n",
    "    \n",
    "    train_dataset = MLPDecoderDataset1(document_vectors_[:train_size], document_TFIDF_[:train_size], topk=topk)\n",
    "    train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "    valid_dataset = MLPDecoderDataset1(document_vectors_[train_size:], document_TFIDF_[train_size:], topk=topk)\n",
    "    valid_loader  = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "    train_dataset2 = MLPDecoderDataset2(document_vectors_[:train_size], document_TFIDF_[:train_size], topk=topk)\n",
    "    train_loader2  = torch.utils.data.DataLoader(train_dataset2, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "    valid_dataset2 = MLPDecoderDataset2(document_vectors_[train_size:], document_TFIDF_[train_size:], topk=topk)\n",
    "    valid_loader2  = torch.utils.data.DataLoader(valid_dataset2, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    \n",
    "    return train_loader, valid_loader, train_loader2, valid_loader2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39e0e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDecoder(nn.Module):\n",
    "    def __init__(self, doc_emb_dim, num_words, h_dim=300):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(doc_emb_dim, h_dim) \n",
    "        self.fc4 = nn.Linear(h_dim, num_words)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b13e75e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_MLPDecoder(model, data_loader):\n",
    "    results = {}\n",
    "    model.eval()\n",
    "    \n",
    "    pred_all = []\n",
    "    target_all = []\n",
    "    \n",
    "    # predict all data\n",
    "    for data in data_loader:\n",
    "        doc_embs, target, _ = data\n",
    "        \n",
    "        doc_embs = doc_embs.to(device)\n",
    "        target = target.to(device)\n",
    "                \n",
    "        pred = model(doc_embs)\n",
    "        pred_all.append(pred)\n",
    "        target_all.append(target)\n",
    "        \n",
    "    pred_all = torch.cat(pred_all, dim=0)\n",
    "    target_all = torch.cat(target_all, dim=0)\n",
    "    \n",
    "    # Precision\n",
    "    precision_scores = retrieval_precision_all(pred_all, target_all, k=config[\"topk\"])\n",
    "    for k, v in precision_scores.items():\n",
    "        results['precision@{}'.format(k)] = v\n",
    "        \n",
    "    # NDCG\n",
    "    ndcg_scores = retrieval_normalized_dcg_all(pred_all, target_all, k=config[\"topk\"])\n",
    "    for k, v in ndcg_scores.items():\n",
    "        results['ndcg@{}'.format(k)] = v\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60ccb653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_MLPDecoder(model, data_loader):\n",
    "    results = defaultdict(list)\n",
    "    model.eval()\n",
    "        \n",
    "    # predict all data\n",
    "    for data in data_loader:\n",
    "        doc_embs, target, _ = data\n",
    "        \n",
    "        doc_embs = doc_embs.to(device)\n",
    "        target = target.to(device)\n",
    "                \n",
    "        pred = model(doc_embs)\n",
    "    \n",
    "        # Precision\n",
    "        precision_scores = retrieval_precision_all(pred, target, k=config[\"topk\"])\n",
    "        for k, v in precision_scores.items():\n",
    "            results['precision@{}'.format(k)].append(v)\n",
    "        \n",
    "        # NDCG\n",
    "        ndcg_scores = retrieval_normalized_dcg_all(pred, target, k=config[\"topk\"])\n",
    "        for k, v in ndcg_scores.items():\n",
    "            results['ndcg@{}'.format(k)].append(v)\n",
    "        \n",
    "    for k in results:\n",
    "        results[k] = np.mean(results[k])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5472d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decoder(config):\n",
    "    model = MLPDecoder(doc_emb_dim=document_vectors.shape[1], num_words=document_TFIDF.shape[1], h_dim=config[\"h_dim\"]).to(device)\n",
    "    model.train()\n",
    "\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"],\\\n",
    "                          weight_decay=config[\"weight_decay\"])\n",
    "    # prepare loss\n",
    "    if config[\"criterion\"] == \"MultiLabelMarginLoss\":\n",
    "        criterion = nn.MultiLabelMarginLoss(reduction='mean')\n",
    "    elif config[\"criterion\"] == \"MultiLabelMarginLossPos\":\n",
    "        criterion = MultiLabelMarginLossPos\n",
    "    elif config[\"criterion\"] == \"ListNet\":\n",
    "        criterion = listNet_origin\n",
    "    elif config[\"criterion\"] == \"KL\":\n",
    "        criterion = KL\n",
    "    elif config[\"criterion\"] == \"MSE\":\n",
    "        criterion = MSE\n",
    "        \n",
    "    # prepare dataloader\n",
    "    train_loader1, valid_loader1, train_loader2, valid_loader2 = prepare_dataloader(batch_size=100,\\\n",
    "                                                                              train_size_ratio=0.8, topk=config[\"topk\"],\n",
    "                                                                              TFIDF_normalize=config[\"TFIDF_normalize\"])\n",
    "    train_loader = train_loader1\n",
    "    valid_loader = valid_loader1\n",
    "    if config[\"criterion\"] == \"MultiLabelMarginLossPos\":\n",
    "        train_loader = train_loader2\n",
    "        valid_loader = valid_loader2\n",
    "    \n",
    "    results = []\n",
    "    n_epoch = config[\"n_epoch\"]\n",
    "    valid_epoch = config[\"valid_epoch\"]\n",
    "    verbose = config[\"verbose\"]\n",
    "    \n",
    "    for epoch in tqdm(range(n_epoch)):    \n",
    "        train_loss_his = []\n",
    "        valid_loss_his = []\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for data in train_loader:\n",
    "            doc_embs, target, target_rank = data\n",
    "            doc_embs = doc_embs.to(device)\n",
    "            \n",
    "            # loss\n",
    "            pred = model(doc_embs)    \n",
    "            if config[\"criterion\"] == \"MultiLabelMarginLoss\":\n",
    "                loss = criterion(pred, target_rank.to(device))\n",
    "            elif config[\"criterion\"] == \"MultiLabelMarginLossPos\":\n",
    "                loss = criterion(pred, target_rank[0].to(device), target_rank[1].to(device))\n",
    "            elif config[\"criterion\"] == \"ListNet\":\n",
    "                loss = criterion(pred, target.to(device))\n",
    "            elif config[\"criterion\"] == \"KL\":\n",
    "                loss = criterion(pred, target.to(device))\n",
    "            elif config[\"criterion\"] == \"MSE\":\n",
    "                loss = criterion(pred, target.to(device))\n",
    "                \n",
    "            # Model backwarding\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            train_loss_his.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        for data in valid_loader:\n",
    "            doc_embs, target, target_rank = data\n",
    "            doc_embs = doc_embs.to(device)\n",
    "\n",
    "            # loss\n",
    "            pred = model(doc_embs)    \n",
    "            if config[\"criterion\"] == \"MultiLabelMarginLoss\":\n",
    "                loss = criterion(pred, target_rank.to(device))\n",
    "            elif config[\"criterion\"] == \"MultiLabelMarginLossPos\":\n",
    "                loss = criterion(pred, target_rank[0].to(device), target_rank[1].to(device))\n",
    "            elif config[\"criterion\"] == \"ListNet\":\n",
    "                loss = criterion(pred, target.to(device))\n",
    "            elif config[\"criterion\"] == \"KL\":\n",
    "                loss = criterion(pred, target.to(device))\n",
    "            elif config[\"criterion\"] == \"MSE\":\n",
    "                loss = criterion(pred, target.to(device))\n",
    "\n",
    "            valid_loss_his.append(loss.item())\n",
    "\n",
    "        print(\"Epoch\", epoch, np.mean(train_loss_his), np.mean(valid_loss_his))\n",
    "\n",
    "        if epoch % valid_epoch == 0:\n",
    "            res = {}\n",
    "            res['epoch'] = epoch\n",
    "\n",
    "            train_res_ndcg = evaluate_MLPDecoder(model, train_loader)\n",
    "            valid_res_ndcg = evaluate_MLPDecoder(model, valid_loader)\n",
    "\n",
    "            res.update(valid_res_ndcg)\n",
    "            results.append(res)\n",
    "\n",
    "            if verbose:\n",
    "                print()\n",
    "                print('train', train_res_ndcg)\n",
    "                print('valid', valid_res_ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346a54c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 14956\n",
      "valid size 3740\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a59a3411f449b2b4f26da3484f3713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=600), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 12.894923826853434 10.416469624167995\n",
      "\n",
      "train defaultdict(<class 'list'>, {'precision@10': 0.01845285722054541, 'precision@30': 0.024642540545513233, 'precision@50': 0.03920685471345981, 'ndcg@10': 0.012078703642667582, 'ndcg@30': 0.01772142723513146, 'ndcg@50': 0.030151826726893583, 'ndcg@all': 0.31091484864552815})\n",
      "valid defaultdict(<class 'list'>, {'precision@10': 0.01831578951034891, 'precision@30': 0.02416666790744976, 'precision@50': 0.03832631362111945, 'ndcg@10': 0.012313411080915677, 'ndcg@30': 0.017787261571931213, 'ndcg@50': 0.02971278567259249, 'ndcg@all': 0.308475524187088})\n",
      "Epoch 1 8.6802729733785 8.635530647478605\n",
      "Epoch 2 6.937849006652832 7.688166743830631\n"
     ]
    }
   ],
   "source": [
    "train_config = {\n",
    "    \"lr\": 0.05,\n",
    "    \"momentum\": 0.0,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \n",
    "    \"n_epoch\": 600,\n",
    "    \"verbose\": True,\n",
    "    \"valid_epoch\": 10,\n",
    "    \n",
    "    \"topk\": 50,\n",
    "    \n",
    "    \"h_dim\": 3000,\n",
    "    \"criterion\": \"MultiLabelMarginLoss\", # \"ListNet\", \"MultiLabelMarginLossPos\"\n",
    "    \"TFIDF_normalize\": False\n",
    "}\n",
    "\n",
    "train_decoder(train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38740cc8",
   "metadata": {},
   "source": [
    "* train defaultdict(<class 'list'>, {'precision@10': 0.4468299978971481, 'precision@30': 0.4765825545787811, 'precision@50': 0.4932941671212514, 'ndcg@10': 0.35141815225283307, 'ndcg@30': 0.4388885551691055, 'ndcg@50': 0.5403420054912567, 'ndcg@all': 0.6485507190227509})\n",
    "* valid defaultdict(<class 'list'>, {'precision@10': 0.28502631030584635, 'precision@30': 0.21985088249570445, 'precision@50': 0.19671841318670072, 'ndcg@10': 0.2641608922889358, 'ndcg@30': 0.2646897682233861, 'ndcg@50': 0.28631100842827245, 'ndcg@all': 0.5073321830285223})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86d4daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config[\"criterion\"] = \"ListNet\"\n",
    "train_config[\"TFIDF_normalize\"] = False\n",
    "train_decoder(train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0c21c2",
   "metadata": {},
   "source": [
    "* train defaultdict(<class 'list'>, {'precision@10': 0.5408376163244247, 'precision@30': 0.2708717542886734, 'precision@50': 0.19191084623336793, 'ndcg@10': 0.7237670310338338, 'ndcg@30': 0.5995183233420054, 'ndcg@50': 0.5832904160022736, 'ndcg@all': 0.7355261572202046})\n",
    "* valid defaultdict(<class 'list'>, {'precision@10': 0.4000394736465655, 'precision@30': 0.21653948371347628, 'precision@50': 0.15969472613773847, 'ndcg@10': 0.49008390542707947, 'ndcg@30': 0.4215040944124523, 'ndcg@50': 0.41643378530677994, 'ndcg@all': 0.5964698571907846})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0802ee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config[\"criterion\"] = \"MultiLabelMarginLossPos\"\n",
    "train_config[\"TFIDF_normalize\"] = False\n",
    "train_decoder(train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3eac45",
   "metadata": {},
   "source": [
    "* train defaultdict(<class 'list'>, {'precision@10': 0.48348952174186705, 'precision@30': 0.4728728707631429, 'precision@50': 0.48005197564760843, 'ndcg@10': 0.38053884088993073, 'ndcg@30': 0.4542527727286021, 'ndcg@50': 0.5518478057781855, 'ndcg@all': 0.6627423493067424})\n",
    "* valid defaultdict(<class 'list'>, {'precision@10': 0.3156052620003098, 'precision@30': 0.22210965619275444, 'precision@50': 0.19529209207547338, 'ndcg@10': 0.29573119588588415, 'ndcg@30': 0.281982432854803, 'ndcg@50': 0.3005151176138928, 'ndcg@all': 0.5244677521680531})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f5324",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_config[\"criterion\"] = \"MultiLabelMarginLossPos\"\n",
    "# train_config[\"TFIDF_normalize\"] = True\n",
    "# train_decoder(train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4fdb8c",
   "metadata": {},
   "source": [
    "* train defaultdict(<class 'list'>, {'precision@10': 0.5554014225800832, 'precision@30': 0.5590281081199646, 'precision@50': 0.5573797816038132, 'ndcg@10': 0.444824323852857, 'ndcg@30': 0.5369966959953308, 'ndcg@50': 0.6349835149447123, 'ndcg@all': 0.7023906556765238})\n",
    "* valid defaultdict(<class 'list'>, {'precision@10': 0.32681578397750854, 'precision@30': 0.23064035804648148, 'precision@50': 0.20169999097522937, 'ndcg@10': 0.3154387238778566, 'ndcg@30': 0.30100254165498835, 'ndcg@50': 0.31951345973893214, 'ndcg@all': 0.5373694473191312})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d579fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config[\"criterion\"] = \"MSE\"\n",
    "train_config[\"TFIDF_normalize\"] = False\n",
    "train_decoder(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee111ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config[\"criterion\"] = \"KL\"\n",
    "train_config[\"TFIDF_normalize\"] = False\n",
    "train_decoder(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576de073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
