{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982acb8f",
   "metadata": {},
   "source": [
    "## Siamese network \n",
    "Steps:\n",
    "1. load word embeding and document embedding\n",
    "2. create pytorch dataset and dataloader\n",
    "3. Try Contrastive loss and triplet loss\n",
    "4. further improve negative sampling (e.g. hard negative or word2vec negative sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95b262e",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Goal:\n",
    "    for dataloader\n",
    "1. Read raw file\n",
    "2. \n",
    "\n",
    "CBOW:\n",
    "    local words and center word\n",
    "DNN:\n",
    "    Document vectors and words emb\n",
    "Triplet:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67d8234",
   "metadata": {},
   "source": [
    "#### raw data\n",
    "* word embedding: glove\n",
    "* doc text: ./data/IMDB.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b007799a",
   "metadata": {},
   "source": [
    "### preprocess\n",
    "1. truncate smallest k word in IDF\n",
    "2. stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6a82ca",
   "metadata": {},
   "source": [
    "### model\n",
    "1. k highest freq words\n",
    "2. CBOW\n",
    "3. Triplet\n",
    "4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9967b",
   "metadata": {},
   "source": [
    "### evaluation\n",
    "1. F1\n",
    "2. F1 weighted by TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9beff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np \n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from itertools import cycle\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5dc01df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "config[\"n_document\"] = 10000\n",
    "config[\"min_word_freq_threshold\"] = 20\n",
    "config[\"topk_word_freq_threshold\"] = 0\n",
    "config[\"document_vector_agg\"] = 'TF-IDF'\n",
    "config[\"select_topk_TFIDF\"] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05c1627d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea999cb143045fe96366756377d137e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:400000\n"
     ]
    }
   ],
   "source": [
    "# load word embedding\n",
    "embedding_file = \"../data/glove.6B.100d.txt\"\n",
    "\n",
    "word2embedding = dict()\n",
    "word_dim = int(re.findall(r\".(\\d+)d\",embedding_file)[0])\n",
    "\n",
    "with open(embedding_file,\"r\") as f:\n",
    "    for line in tqdm(f):\n",
    "        line = line.strip().split()\n",
    "        word = line[0]\n",
    "        embedding = list(map(float,line[1:]))\n",
    "        word2embedding[word] = embedding\n",
    "\n",
    "print(\"Number of words:%d\" % len(word2embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1233053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, word2embedding, min_word_freq_threshold=0, topk_word_freq_threshold=0):\n",
    "        # The low frequency words will be assigned as <UNK> token\n",
    "        self.itos = {0: \"<UNK>\"}\n",
    "        self.stoi = {\"<UNK>\": 0}\n",
    "        \n",
    "        self.word2embedding = word2embedding\n",
    "        self.min_word_freq_threshold = min_word_freq_threshold\n",
    "        self.topk_word_freq_threshold = topk_word_freq_threshold\n",
    "        \n",
    "        self.word_freq_in_corpus = defaultdict(int)\n",
    "        self.IDF = {}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_eng(text):\n",
    "        text = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n",
    "        return text.strip().split()\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        \n",
    "        self.doc_freq = defaultdict(int) # # of document a word appear\n",
    "        self.document_num = len(sentence_list)\n",
    "        self.word_vectors = [[0]*word_dim] # unknown word emb\n",
    "        \n",
    "        for sentence in tqdm(sentence_list, desc=\"Preprocessing documents\"):\n",
    "            # for doc_freq\n",
    "            document_words = set()\n",
    "            \n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                # pass unknown word\n",
    "                if word not in self.word2embedding:\n",
    "                    continue\n",
    "                    \n",
    "                # calculate word freq\n",
    "                self.word_freq_in_corpus[word] += 1\n",
    "                document_words.add(word)\n",
    "                \n",
    "            for word in document_words:\n",
    "                self.doc_freq[word] += 1\n",
    "        \n",
    "        # calculate IDF\n",
    "        print('doc num', self.document_num)\n",
    "        for word, freq in self.doc_freq.items():\n",
    "            self.IDF[word] = math.log(self.document_num / (freq+1))\n",
    "        \n",
    "        # delete less freq words:\n",
    "        delete_words = []\n",
    "        for word, v in self.word_freq_in_corpus.items():\n",
    "            if v < self.min_word_freq_threshold:\n",
    "                delete_words.append(word)     \n",
    "        for word in delete_words:\n",
    "            del self.IDF[word]    \n",
    "            del self.word_freq_in_corpus[word]    \n",
    "        \n",
    "        # delete too freq words\n",
    "        print('eliminate freq words')\n",
    "        IDF = [(word, freq) for word, freq in self.IDF.items()]\n",
    "        IDF.sort(key=lambda x: x[1])\n",
    "\n",
    "        for i in range(self.topk_word_freq_threshold):\n",
    "            print(word)\n",
    "            word = IDF[i][0]\n",
    "            del self.IDF[word]\n",
    "            del self.word_freq_in_corpus[word]\n",
    "        \n",
    "        # construct word_vectors\n",
    "        idx = 1\n",
    "        for word in self.word_freq_in_corpus:\n",
    "            self.word_vectors.append(self.word2embedding[word])\n",
    "            self.stoi[word] = idx\n",
    "            self.itos[idx] = word\n",
    "            idx += 1\n",
    "            \n",
    "    def calculate_document_vector(self, sentence_list, agg, select_topk_TFIDF=None):\n",
    "        document_vectors = []\n",
    "        document_answers = []\n",
    "        \n",
    "        for sentence in tqdm(sentence_list, desc=\"calculate document vectors\"):\n",
    "            document_vector = np.zeros(len(self.word_vectors[0]))\n",
    "            select_words = []\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                # pass unknown word\n",
    "                if word not in self.stoi:\n",
    "                    continue\n",
    "                else:\n",
    "                    select_words.append(word)\n",
    "\n",
    "            # select topk TDIDF\n",
    "            if select_topk_TFIDF is not None:\n",
    "                doc_TFIDF = defaultdict(float)\n",
    "                for word in select_words:    \n",
    "                    doc_TFIDF[word] += self.IDF[word]\n",
    "\n",
    "                doc_TFIDF_l = [(word, TFIDF) for word, TFIDF in doc_TFIDF.items()]\n",
    "                doc_TFIDF_l.sort(key=lambda x:x[1], reverse=True)\n",
    "                \n",
    "                select_topk_words = set(list(map(lambda x:x[0], doc_TFIDF_l[:select_topk_TFIDF])))\n",
    "                select_words = [word for word in select_words if word in select_topk_words]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            # aggregate to doc vectors\n",
    "            for word in select_words:\n",
    "                if agg == 'mean':\n",
    "                    document_vector += self.word2embedding[word]\n",
    "                elif agg == 'TF-IDF':\n",
    "                    document_vector += np.array(self.word2embedding[word]) * self.IDF[word]\n",
    "\n",
    "            if len(select_words) == 0:\n",
    "                print('error', sentence)\n",
    "                return -1\n",
    "            else:\n",
    "                document_vector /= len(select_words)\n",
    "            \n",
    "            document_vectors.append(document_vector)\n",
    "            document_answers.append(select_words)\n",
    "        \n",
    "        # get answers\n",
    "        document_answers_idx = []    \n",
    "        for ans in document_answers:\n",
    "            ans_idx = []\n",
    "            for token in ans:\n",
    "                if token in self.stoi:\n",
    "                    ans_idx.append(self.stoi[token])                    \n",
    "            document_answers_idx.append(ans_idx)\n",
    "            \n",
    "        return document_vectors, document_answers_idx\n",
    "        \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c5706a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBowDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 raw_data_file_path,\n",
    "                 word2embedding,\n",
    "                 skip_header = False,\n",
    "                 n_document = None, # read first n document\n",
    "                 min_word_freq_threshold = 20, # eliminate less freq words\n",
    "                 topk_word_freq_threshold = 5, # eliminate smallest k IDF words\n",
    "                 select_topk_TFIDF = None, # select topk tf-idf as ground-truth\n",
    "                 document_vector_agg = 'mean',\n",
    "                 ):\n",
    "\n",
    "        assert document_vector_agg in ['mean', 'TF-IDF']\n",
    "        \n",
    "        # raw documents\n",
    "        self.documents = []\n",
    "        # document vectors\n",
    "        self.document_vectors = []\n",
    "        \n",
    "        with open(raw_data_file_path,'r',encoding='utf-8') as f:\n",
    "            if skip_header:\n",
    "                f.readline()\n",
    "            for line in tqdm(f, desc=\"Loading documents\"):\n",
    "                # read firt n document\n",
    "                if n_document is not None and len(self.documents) >= n_document:\n",
    "                    break    \n",
    "                self.documents.append(line.strip(\"\\n\"))\n",
    "\n",
    "        # build vocabulary\n",
    "        self.vocab = Vocabulary(word2embedding, min_word_freq_threshold, topk_word_freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.documents)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "#         self.words_tokenized = [self.vocab.numericalize(text) for text in self.documents]\n",
    "        \n",
    "        # calculate document vectors\n",
    "        self.document_vectors, self.words_tokenized = self.vocab.calculate_document_vector(self.documents, \\\n",
    "                                                                                           document_vector_agg, select_topk_TFIDF)\n",
    "        # train-test split\n",
    "        # training\n",
    "        self.train_length = int(len(self.words_tokenized)*0.8)\n",
    "        self.train_vectors = self.document_vectors[:self.train_length]\n",
    "        self.train_words = self.words_tokenized[:self.train_length]\n",
    "        self.document_ids = list(range(self.train_length))\n",
    "        self.generator = cycle(self.context_target_generator())\n",
    "        self.dataset_size = sum([len(s) for s in self.train_words])\n",
    "        \n",
    "        # testing\n",
    "        self.test_vectors = self.document_vectors[self.train_length:]\n",
    "        self.test_words = self.words_tokenized[self.train_length:]\n",
    "\n",
    "    def context_target_generator(self):\n",
    "        np.random.shuffle(self.document_ids) # inplace shuffle\n",
    "\n",
    "        # randomly select a document and create its training example\n",
    "        for document_id in self.document_ids: \n",
    "            word_list = set(self.train_words[document_id])\n",
    "            negative_sample_space = list(set(range(self.vocab_size)) - word_list)\n",
    "            negative_samples = np.random.choice(negative_sample_space,size=len(word_list),replace = False)\n",
    "            for word_id, negative_wordID in zip(word_list, negative_samples):\n",
    "                yield [document_id, word_id, negative_wordID]\n",
    "                \n",
    "    def __getitem__(self, idx):\n",
    "        doc_id, word_id, negative_wordID = next(self.generator)\n",
    "        doc_id = torch.FloatTensor(self.document_vectors[doc_id])\n",
    "        word_id = torch.FloatTensor(self.vocab.word_vectors[word_id])\n",
    "        negative_word = torch.FloatTensor(self.vocab.word_vectors[negative_wordID])\n",
    "\n",
    "        return doc_id, word_id, negative_word\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1a7186c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5073ab4fd74294a6c4c1423f8d1ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading documents: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c40d94df444387a3bf2a2b7ba4179e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing documents:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc num 10000\n",
      "eliminate freq words\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00345aeb48984d1a9d12cd49bd832465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "calculate document vectors:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish building dataset!\n",
      "Number of documents:10000\n",
      "Number of words:7533\n"
     ]
    }
   ],
   "source": [
    "# load and build torch dataset\n",
    "data_file_path = '../data/IMDB.txt'\n",
    "\n",
    "print(\"Building dataset....\")\n",
    "dataset = CBowDataset(\n",
    "                    raw_data_file_path=data_file_path,\n",
    "                    word2embedding=word2embedding,\n",
    "                    skip_header=False,\n",
    "                    n_document = config[\"n_document\"],\n",
    "                    min_word_freq_threshold = config[\"min_word_freq_threshold\"],\n",
    "                    topk_word_freq_threshold = config[\"topk_word_freq_threshold\"],\n",
    "                    document_vector_agg = config[\"document_vector_agg\"],\n",
    "                    select_topk_TFIDF = config[\"select_topk_TFIDF\"]\n",
    "                    )\n",
    "print(\"Finish building dataset!\")\n",
    "print(f\"Number of documents:{len(dataset.documents)}\")\n",
    "print(f\"Number of words:{dataset.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf6bf5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 doc_vectors,\n",
    "                 ans_words,\n",
    "                 ):\n",
    "        self.doc_vectors = doc_vectors\n",
    "        self.ans_words = ans_words\n",
    "        assert len(doc_vectors) == len(ans_words)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        doc_vec = torch.FloatTensor(self.doc_vectors[idx])\n",
    "        ans_w = torch.tensor(list(set(self.ans_words[idx])))\n",
    "        return doc_vec, ans_w\n",
    "\n",
    "    def collate_fn(self,batch):\n",
    "        # Batch: List of tuples [(batch1), (batch2)]\n",
    "        \n",
    "        doc_vec = torch.cat([item[0].unsqueeze(0) for item in batch], dim=0)\n",
    "        ans_w = [item[1] for item in batch]\n",
    "        ans_w = pad_sequence(ans_w, batch_first=True, padding_value=-1)\n",
    "        \n",
    "        return doc_vec, ans_w \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "457c5d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletNet(nn.Module):\n",
    "    def __init__(self, hdim):\n",
    "        super(TripletNet, self).__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(hdim, 256),\n",
    "                        nn.PReLU(),\n",
    "                        nn.Linear(256, 256),\n",
    "                        nn.PReLU(),\n",
    "                        nn.Linear(256, 2)\n",
    "                        )\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        output1 = self.fc(x1)\n",
    "        output2 = self.fc(x2)\n",
    "        output3 = self.fc(x3)\n",
    "        return output1, output2, output3\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59ec9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet loss\n",
    "    Takes embeddings of an anchor sample, a positive sample and a negative sample\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative, size_average=True):\n",
    "        distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)\n",
    "        distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)\n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean() if size_average else losses.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b60ca7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 1.\n",
    "BATCH_SIZE = 1024\n",
    "EPOCH = 300\n",
    "\n",
    "device = \"cuda:0\"\n",
    "model = TripletNet(word_dim).to(device)\n",
    "loss_fn = TripletLoss(margin).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22a934f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "                        dataset, \n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        num_workers=4,\n",
    "                        shuffle=True,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2aa0290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docvec = dataset.test_vectors\n",
    "test_ans = dataset.test_words\n",
    "test_dataset = TestDataset(test_docvec,test_ans)\n",
    "test_loader = DataLoader(test_dataset,                         \n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         num_workers=4,\n",
    "                         collate_fn=test_dataset.collate_fn)\n",
    "word_embedding_tensor = torch.FloatTensor(dataset.vocab.word_vectors).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05bb89ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_word_emb, loader,Ks = [50,100,150,200]):\n",
    "    avg_precision, avg_recall = [], []\n",
    "    for batch in test_loader:\n",
    "        batch = [item.to(device) for item in batch]\n",
    "        emb, ans = batch\n",
    "        emb = model.get_embedding(emb)\n",
    "        scores = torch.cdist(emb, test_word_emb)\n",
    "        ans_length = torch.sum((~ans.eq(-1)).float(), dim=-1)\n",
    "        mask = ~ans.eq(-1).unsqueeze(-1)\n",
    "        \n",
    "        # calculate precision and recall\n",
    "        tmp_pr, tmp_re = [],[]\n",
    "        for K in Ks:\n",
    "            top_indices = torch.argsort(scores,dim=1)[:,:K]\n",
    "            hit = top_indices.unsqueeze(-2) == ans.unsqueeze(-1)\n",
    "            hit = torch.sum((hit * mask).flatten(1),dim=-1)\n",
    "            precision = hit / K\n",
    "            recall = hit / ans_length\n",
    "            tmp_pr.append(precision)\n",
    "            tmp_re.append(recall)\n",
    "        tmp_pr = torch.stack(tmp_pr).T.detach().cpu().numpy().tolist()\n",
    "        tmp_re = torch.stack(tmp_re).T.detach().cpu().numpy().tolist()\n",
    "        avg_precision.extend(tmp_pr)\n",
    "        avg_recall.extend(tmp_re)\n",
    "        \n",
    "    avg_precision = np.mean(avg_precision,axis=0)\n",
    "    avg_recall = np.mean(avg_recall, axis=0)\n",
    "    for idx, kval in enumerate(Ks):\n",
    "        print(f\"[K={kval}] Precision:{avg_precision[idx]:.4f} Recall:{avg_recall[idx]:.4f}\")\n",
    "    return avg_precision, avg_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c124d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(EPOCH):\n",
    "#     avg_loss = []\n",
    "#     model.train()\n",
    "#     for batch in tqdm(train_loader):\n",
    "#         batch = [item.to(device) for item in batch]\n",
    "#         doc_id,pos_w,neg_w = batch\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = loss_fn(*model(doc_id,pos_w,neg_w))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         avg_loss.append(loss.item())\n",
    "#     avg_loss = np.mean(avg_loss)\n",
    "#     print(f\"Loss:{avg_loss:4f}\")\n",
    "    \n",
    "#     # evaluate\n",
    "#     model.eval()\n",
    "#     test_word_emb = model.get_embedding(word_embedding_tensor)\n",
    "#     res = evaluate(test_word_emb,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7bc50afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "def evaluate_NDCG(test_word_emb, loader, topk=None):\n",
    "    NDCGs = defaultdict(list)\n",
    "    \n",
    "    for batch in (test_loader):\n",
    "        batch = [item.to(device) for item in batch]\n",
    "        emb, ans = batch\n",
    "        TFIDF_ans = np.zeros((len(ans), test_word_emb.shape[0]))\n",
    "        for i in range(len(ans)):\n",
    "            ans_row = ans[i]\n",
    "            ans_row = ans_row[~ans_row.eq(-1)]\n",
    "            ans_row = ans_row[~ans_row.eq(0)]\n",
    "            for word_id in ans_row:\n",
    "                word_id = word_id.item()\n",
    "                word = dataset.vocab.itos[word_id]\n",
    "                TFIDF_ans[i][word_id] += dataset.vocab.IDF[word]\n",
    "             \n",
    "        emb = model.get_embedding(emb)\n",
    "        scores = -torch.cdist(emb, test_word_emb).cpu().detach().numpy()\n",
    "        true_relevance = TFIDF_ans\n",
    "\n",
    "        NDCGs['top50'].append(ndcg_score(true_relevance, scores, k=50))\n",
    "        NDCGs['top100'].append(ndcg_score(true_relevance, scores, k=100))\n",
    "        NDCGs['top200'].append(ndcg_score(true_relevance, scores, k=200))\n",
    "        NDCGs['ALL'].append(ndcg_score(true_relevance, scores, k=None))\n",
    "    \n",
    "    print('NDCG top50', np.mean(NDCGs['top50']))\n",
    "    print('NDCG top100', np.mean(NDCGs['top100']))\n",
    "    print('NDCG top200', np.mean(NDCGs['top200']))\n",
    "    print('NDCG ALL', np.mean(NDCGs['ALL']))\n",
    "    return NDCGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8f96c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_history = []\n",
    "\n",
    "# for epoch in range(EPOCH):\n",
    "#     avg_loss = []\n",
    "#     model.train()\n",
    "#     for batch in tqdm(train_loader):\n",
    "#         batch = [item.to(device) for item in batch]\n",
    "#         doc_id,pos_w,neg_w = batch\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = loss_fn(*model(doc_id,pos_w,neg_w))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         avg_loss.append(loss.item())\n",
    "#     avg_loss = np.mean(avg_loss)\n",
    "#     print(f\"Loss:{avg_loss:4f}\")\n",
    "    \n",
    "#     # evaluate\n",
    "#     model.eval()\n",
    "#     test_word_emb = model.get_embedding(word_embedding_tensor)\n",
    "#     ndcg_res = evaluate_NDCG(test_word_emb,test_loader)\n",
    "#     validation_history.append(ndcg_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3767a29",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4cbd29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select #answer largest pred\n",
    "def metric4(binary_x, answer, w_idx=None, topk=50, verbose=0):\n",
    "    select_num = topk\n",
    "    answer = list(set(answer))\n",
    "    \n",
    "    if w_idx is not None:\n",
    "        pred = w_idx[np.argsort(binary_x)[-select_num:]]\n",
    "    else:\n",
    "        pred = np.arange(len(binary_x))[np.argsort(binary_x)[-select_num:]]\n",
    "    \n",
    "    hit = np.intersect1d(pred, answer)\n",
    "    hit_num = len(hit)\n",
    "    recall = hit_num / len(answer)\n",
    "    precision = hit_num / len(pred)\n",
    "    if verbose == 1:\n",
    "        print('answer:', word_list[answer])\n",
    "        print('hit:', word_list[hit])\n",
    "    return {\"recall\": recall, \"precision\": precision}\n",
    "\n",
    "# select #answer largest pred\n",
    "def metric_ndcg(binary_x, answer, topk=50, verbose=0):\n",
    "\n",
    "    TFIDF_ans = np.zeros(len(binary_x))\n",
    "    for word_idx in answer:\n",
    "        if word_idx == 0:\n",
    "            continue\n",
    "        word = dataset.vocab.itos[word_idx]\n",
    "        TFIDF_ans[word_idx] += dataset.vocab.IDF[word]\n",
    "    NDCG_score = ndcg_score(TFIDF_ans.reshape(1,-1), binary_x.reshape(1,-1), k=topk)\n",
    "    \n",
    "    if verbose == 1:\n",
    "        print('NDCG_score:', NDCG_score)\n",
    "    return NDCG_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1318878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "class PyTorchLinearRegression:\n",
    "    ''' Class that implemnets Multiple Linear Regression with PyTorch'''\n",
    "    def __init__(self, num_of_features, lr, constraintHigh, constraintLow, total, init_type=0, L1=0, L2=0):\n",
    "        if init_type == 0:\n",
    "            self.w = torch.zeros(num_of_features, requires_grad=True)\n",
    "        elif init_type == 1:\n",
    "            self.w = torch.ones(num_of_features, requires_grad=True)\n",
    "        elif init_type == 2:  \n",
    "            self.w = torch.rand(num_of_features, requires_grad=True)\n",
    "        elif init_type == 3:\n",
    "            self.w = -torch.ones(num_of_features, requires_grad=True)\n",
    "\n",
    "        self.learning_rate = lr\n",
    "        self.high = constraintHigh\n",
    "        self.low = constraintLow\n",
    "        self.total = total\n",
    "        self.rg2 = total / num_of_features\n",
    "        self.L1 = L1\n",
    "        self.L2 = L2\n",
    "        \n",
    "    def _model(self, X):\n",
    "        return X @ self.w.t()# + self.b\n",
    "    \n",
    "    def _mse(self, pred, real):\n",
    "        difference = pred - real\n",
    "        return torch.sum(difference * difference) / difference.numel()\n",
    "    \n",
    "    def _regularization_weightdist(self):\n",
    "        difference = self.w - 1\n",
    "        return -torch.sum(difference * difference) / difference.numel()\n",
    "    \n",
    "    def _regularization_weightsum(self):\n",
    "        difference = torch.sum(self.w) - self.total\n",
    "        return difference * difference / self.w.numel()\n",
    "    \n",
    "    def fit(self, X, y, epochs):\n",
    "        print(loss_weight)\n",
    "        X = torch.from_numpy(X).float()\n",
    "        y = torch.from_numpy(y).float()\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            predictions = self._model(X)\n",
    "            loss1 = self._mse(predictions, y)\n",
    "            loss2 = self._regularization_weightdist()\n",
    "            loss3 = self._regularization_weightsum()\n",
    "            loss = loss1 * loss_weight[0] + loss2 * loss_weight[1] + loss3 * loss_weight[2]\n",
    "           \n",
    "            if (i % (epochs//20)) == 0:\n",
    "                print(f'Epoch: {i} - Loss: {loss1}')\n",
    "            \n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                self.w -= (self.w.grad) * self.learning_rate + torch.sign(self.w)*self.L1 + self.w*self.L2\n",
    "                self.w.grad.zero_()\n",
    "                self.w.data.clamp_(min=self.low, max=self.high)\n",
    "\n",
    "#             x = 100\n",
    "#             if i % x == x-1:\n",
    "# #                 self.w=torch.tensor(self.low + (self.high-self.low)*(self.w - torch.min(self.w))/(torch.max(self.w) - torch.min(self.w)), requires_grad=True)\n",
    "#                 self.w.data.clamp_(min=self.low, max=self.high)\n",
    "#                 pass\n",
    "                \n",
    "    def predict(self, X):\n",
    "        X = torch.from_numpy(X).float()\n",
    "        return self._model(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        X = torch.from_numpy(X).float()\n",
    "        y_pred = self._model(X).detach().numpy()\n",
    "        return r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a7e8da6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7533, 100)\n",
      "(10000, 100)\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "word_embs = np.array(dataset.vocab.word_vectors)\n",
    "doc_embs = np.array(dataset.document_vectors)\n",
    "doc_answers = dataset.words_tokenized\n",
    "word_list = dataset.vocab.itos\n",
    "\n",
    "print(word_embs.shape)\n",
    "print(doc_embs.shape)\n",
    "print(len(doc_answers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "27944d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.4937790334224701\n",
      "Epoch: 50 - Loss: 0.008251945488154888\n",
      "Epoch: 100 - Loss: 0.004072097595781088\n",
      "Epoch: 150 - Loss: 0.002774542896077037\n",
      "Epoch: 200 - Loss: 0.002185232238844037\n",
      "Epoch: 250 - Loss: 0.0018591865664348006\n",
      "Epoch: 300 - Loss: 0.0016590289305895567\n",
      "Epoch: 350 - Loss: 0.001522210892289877\n",
      "Epoch: 400 - Loss: 0.0014211998786777258\n",
      "Epoch: 450 - Loss: 0.0013432650594040751\n",
      "Epoch: 500 - Loss: 0.0012790452456101775\n",
      "Epoch: 550 - Loss: 0.0012316247448325157\n",
      "Epoch: 600 - Loss: 0.0011922030244022608\n",
      "Epoch: 650 - Loss: 0.0011571780778467655\n",
      "Epoch: 700 - Loss: 0.0011258076410740614\n",
      "Epoch: 750 - Loss: 0.0010991323506459594\n",
      "Epoch: 800 - Loss: 0.0010731748770922422\n",
      "Epoch: 850 - Loss: 0.0010474775917828083\n",
      "Epoch: 900 - Loss: 0.0010238197864964604\n",
      "Epoch: 950 - Loss: 0.0010019266046583652\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.3999672830104828\n",
      "Epoch: 50 - Loss: 0.0017954212380573153\n",
      "Epoch: 100 - Loss: 0.0011469894088804722\n",
      "Epoch: 150 - Loss: 0.0009346501319669187\n",
      "Epoch: 200 - Loss: 0.0008334583835676312\n",
      "Epoch: 250 - Loss: 0.0007788938819430768\n",
      "Epoch: 300 - Loss: 0.0007459816988557577\n",
      "Epoch: 350 - Loss: 0.0007231326308101416\n",
      "Epoch: 400 - Loss: 0.0007034226437099278\n",
      "Epoch: 450 - Loss: 0.0006867917836643755\n",
      "Epoch: 500 - Loss: 0.0006737388903275132\n",
      "Epoch: 550 - Loss: 0.0006631141295656562\n",
      "Epoch: 600 - Loss: 0.0006533075356855989\n",
      "Epoch: 650 - Loss: 0.0006440529250539839\n",
      "Epoch: 700 - Loss: 0.0006368585163727403\n",
      "Epoch: 750 - Loss: 0.0006274567567743361\n",
      "Epoch: 800 - Loss: 0.000622437393758446\n",
      "Epoch: 850 - Loss: 0.0006133241695351899\n",
      "Epoch: 900 - Loss: 0.0006031500524841249\n",
      "Epoch: 950 - Loss: 0.0005986690521240234\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.32694846391677856\n",
      "Epoch: 50 - Loss: 0.0019220768008381128\n",
      "Epoch: 100 - Loss: 0.0013083856320008636\n",
      "Epoch: 150 - Loss: 0.0011130486382171512\n",
      "Epoch: 200 - Loss: 0.001019862131215632\n",
      "Epoch: 250 - Loss: 0.0009708249708637595\n",
      "Epoch: 300 - Loss: 0.0009342475095763803\n",
      "Epoch: 350 - Loss: 0.0009117752779275179\n",
      "Epoch: 400 - Loss: 0.0008863410330377519\n",
      "Epoch: 450 - Loss: 0.0008687480003573\n",
      "Epoch: 500 - Loss: 0.0008512316853739321\n",
      "Epoch: 550 - Loss: 0.0008401012164540589\n",
      "Epoch: 600 - Loss: 0.0008277432643808424\n",
      "Epoch: 650 - Loss: 0.000816534215118736\n",
      "Epoch: 700 - Loss: 0.0008050777832977474\n",
      "Epoch: 750 - Loss: 0.0007944324752315879\n",
      "Epoch: 800 - Loss: 0.0007834192947484553\n",
      "Epoch: 850 - Loss: 0.0007730823708698153\n",
      "Epoch: 900 - Loss: 0.0007616180228069425\n",
      "Epoch: 950 - Loss: 0.0007548641879111528\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.29119786620140076\n",
      "Epoch: 50 - Loss: 0.0025044665671885014\n",
      "Epoch: 100 - Loss: 0.0016979282954707742\n",
      "Epoch: 150 - Loss: 0.0014450117014348507\n",
      "Epoch: 200 - Loss: 0.0013305750908330083\n",
      "Epoch: 250 - Loss: 0.0012652644654735923\n",
      "Epoch: 300 - Loss: 0.001221969141624868\n",
      "Epoch: 350 - Loss: 0.0011900800745934248\n",
      "Epoch: 400 - Loss: 0.0011596299009397626\n",
      "Epoch: 450 - Loss: 0.0011334122391417623\n",
      "Epoch: 500 - Loss: 0.001111935474909842\n",
      "Epoch: 550 - Loss: 0.0010922738583758473\n",
      "Epoch: 600 - Loss: 0.0010727336630225182\n",
      "Epoch: 650 - Loss: 0.0010539502836763859\n",
      "Epoch: 700 - Loss: 0.0010410684626549482\n",
      "Epoch: 750 - Loss: 0.0010238897521048784\n",
      "Epoch: 800 - Loss: 0.001012560911476612\n",
      "Epoch: 850 - Loss: 0.001002390868961811\n",
      "Epoch: 900 - Loss: 0.0009964695200324059\n",
      "Epoch: 950 - Loss: 0.000988759915344417\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.2629016637802124\n",
      "Epoch: 50 - Loss: 0.002354658441618085\n",
      "Epoch: 100 - Loss: 0.0016033499268814921\n",
      "Epoch: 150 - Loss: 0.0013683055294677615\n",
      "Epoch: 200 - Loss: 0.0012552635744214058\n",
      "Epoch: 250 - Loss: 0.0011910124449059367\n",
      "Epoch: 300 - Loss: 0.0011467740405350924\n",
      "Epoch: 350 - Loss: 0.0011116501409560442\n",
      "Epoch: 400 - Loss: 0.0010843795025721192\n",
      "Epoch: 450 - Loss: 0.0010601762915030122\n",
      "Epoch: 500 - Loss: 0.001042020390741527\n",
      "Epoch: 550 - Loss: 0.0010220838012173772\n",
      "Epoch: 600 - Loss: 0.0010076300241053104\n",
      "Epoch: 650 - Loss: 0.0009933747351169586\n",
      "Epoch: 700 - Loss: 0.0009821455460041761\n",
      "Epoch: 750 - Loss: 0.0009697832283563912\n",
      "Epoch: 800 - Loss: 0.0009618433541618288\n",
      "Epoch: 850 - Loss: 0.0009517974685877562\n",
      "Epoch: 900 - Loss: 0.0009440898429602385\n",
      "Epoch: 950 - Loss: 0.0009354938520118594\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.3583441972732544\n",
      "Epoch: 50 - Loss: 0.002647332614287734\n",
      "Epoch: 100 - Loss: 0.0016524064121767879\n",
      "Epoch: 150 - Loss: 0.0013423427008092403\n",
      "Epoch: 200 - Loss: 0.0011986750178039074\n",
      "Epoch: 250 - Loss: 0.0011142895091325045\n",
      "Epoch: 300 - Loss: 0.001052851788699627\n",
      "Epoch: 350 - Loss: 0.0010112072341144085\n",
      "Epoch: 400 - Loss: 0.0009816365782171488\n",
      "Epoch: 450 - Loss: 0.0009532861295156181\n",
      "Epoch: 500 - Loss: 0.000930046197026968\n",
      "Epoch: 550 - Loss: 0.0009116661385633051\n",
      "Epoch: 600 - Loss: 0.0008957588579505682\n",
      "Epoch: 650 - Loss: 0.0008836498600430787\n",
      "Epoch: 700 - Loss: 0.0008747319807298481\n",
      "Epoch: 750 - Loss: 0.0008653928525745869\n",
      "Epoch: 800 - Loss: 0.0008559629204683006\n",
      "Epoch: 850 - Loss: 0.0008446273859590292\n",
      "Epoch: 900 - Loss: 0.000836823892313987\n",
      "Epoch: 950 - Loss: 0.0008276557782664895\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.39501309394836426\n",
      "Epoch: 50 - Loss: 0.0037351304199546576\n",
      "Epoch: 100 - Loss: 0.002259194850921631\n",
      "Epoch: 150 - Loss: 0.0017787945689633489\n",
      "Epoch: 200 - Loss: 0.0015510154189541936\n",
      "Epoch: 250 - Loss: 0.001426683273166418\n",
      "Epoch: 300 - Loss: 0.0013438842725008726\n",
      "Epoch: 350 - Loss: 0.0012869283091276884\n",
      "Epoch: 400 - Loss: 0.0012441794387996197\n",
      "Epoch: 450 - Loss: 0.001213420182466507\n",
      "Epoch: 500 - Loss: 0.0011845126282423735\n",
      "Epoch: 550 - Loss: 0.0011651318054646254\n",
      "Epoch: 600 - Loss: 0.001145129674114287\n",
      "Epoch: 650 - Loss: 0.0011277118464931846\n",
      "Epoch: 700 - Loss: 0.0011125578312203288\n",
      "Epoch: 750 - Loss: 0.0010971113806590438\n",
      "Epoch: 800 - Loss: 0.0010855728760361671\n",
      "Epoch: 850 - Loss: 0.0010793294059112668\n",
      "Epoch: 900 - Loss: 0.0010658105602487922\n",
      "Epoch: 950 - Loss: 0.0010591590544208884\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.4594181180000305\n",
      "Epoch: 50 - Loss: 0.002254315884783864\n",
      "Epoch: 100 - Loss: 0.001435530954040587\n",
      "Epoch: 150 - Loss: 0.0012029792414978147\n",
      "Epoch: 200 - Loss: 0.0010996880009770393\n",
      "Epoch: 250 - Loss: 0.0010481005301699042\n",
      "Epoch: 300 - Loss: 0.001013350673019886\n",
      "Epoch: 350 - Loss: 0.0009914691327139735\n",
      "Epoch: 400 - Loss: 0.000967304571531713\n",
      "Epoch: 450 - Loss: 0.0009468133794143796\n",
      "Epoch: 500 - Loss: 0.0009284998523071408\n",
      "Epoch: 550 - Loss: 0.0009135672589763999\n",
      "Epoch: 600 - Loss: 0.000900890096090734\n",
      "Epoch: 650 - Loss: 0.0008887860458344221\n",
      "Epoch: 700 - Loss: 0.0008826540433801711\n",
      "Epoch: 750 - Loss: 0.0008763791411183774\n",
      "Epoch: 800 - Loss: 0.0008676156285218894\n",
      "Epoch: 850 - Loss: 0.0008621936431154609\n",
      "Epoch: 900 - Loss: 0.0008561586728319526\n",
      "Epoch: 950 - Loss: 0.0008519787807017565\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.3703095614910126\n",
      "Epoch: 50 - Loss: 0.0020268200896680355\n",
      "Epoch: 100 - Loss: 0.0013420144096016884\n",
      "Epoch: 150 - Loss: 0.0011559233535081148\n",
      "Epoch: 200 - Loss: 0.0010757598793134093\n",
      "Epoch: 250 - Loss: 0.0010283271549269557\n",
      "Epoch: 300 - Loss: 0.0009978418238461018\n",
      "Epoch: 350 - Loss: 0.0009747930453158915\n",
      "Epoch: 400 - Loss: 0.0009557858575135469\n",
      "Epoch: 450 - Loss: 0.0009411644423380494\n",
      "Epoch: 500 - Loss: 0.0009322267724201083\n",
      "Epoch: 550 - Loss: 0.0009235602337867022\n",
      "Epoch: 600 - Loss: 0.000914264761377126\n",
      "Epoch: 650 - Loss: 0.000905621622223407\n",
      "Epoch: 700 - Loss: 0.0008948813192546368\n",
      "Epoch: 750 - Loss: 0.0008847233839333057\n",
      "Epoch: 800 - Loss: 0.0008800513460300863\n",
      "Epoch: 850 - Loss: 0.0008735032170079648\n",
      "Epoch: 900 - Loss: 0.0008648122893646359\n",
      "Epoch: 950 - Loss: 0.0008592505473643541\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.36652711033821106\n",
      "Epoch: 50 - Loss: 0.0013726179022341967\n",
      "Epoch: 100 - Loss: 0.0009666256373748183\n",
      "Epoch: 150 - Loss: 0.0008601145236752927\n",
      "Epoch: 200 - Loss: 0.000813510559964925\n",
      "Epoch: 250 - Loss: 0.0007887581014074385\n",
      "Epoch: 300 - Loss: 0.0007781960302963853\n",
      "Epoch: 350 - Loss: 0.0007688406039960682\n",
      "Epoch: 400 - Loss: 0.0007613676716573536\n",
      "Epoch: 450 - Loss: 0.000753280648496002\n",
      "Epoch: 500 - Loss: 0.000746903067920357\n",
      "Epoch: 550 - Loss: 0.0007420237525366247\n",
      "Epoch: 600 - Loss: 0.0007331682718358934\n",
      "Epoch: 650 - Loss: 0.0007267149048857391\n",
      "Epoch: 700 - Loss: 0.0007209449540823698\n",
      "Epoch: 750 - Loss: 0.0007158452062867582\n",
      "Epoch: 800 - Loss: 0.0007095229811966419\n",
      "Epoch: 850 - Loss: 0.0007046457612887025\n",
      "Epoch: 900 - Loss: 0.000696918461471796\n",
      "Epoch: 950 - Loss: 0.0006924797780811787\n"
     ]
    }
   ],
   "source": [
    "pr, re = [[],[],[]], [[],[],[]]\n",
    "ndcgs = defaultdict(list)\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 1000\n",
    "constraintHigh=1\n",
    "constraintLow=0\n",
    "# constraintHigh=float('inf')\n",
    "# constraintLow=-float('inf')\n",
    "loss_weight = [1, 0, 0]\n",
    "L1, L2 = 1e-5, 0\n",
    "rand_type = 0\n",
    "\n",
    "total_mul = 1\n",
    "\n",
    "for uid, uemb in enumerate(tqdm(doc_embs[:10])):\n",
    "    x = word_embs.T\n",
    "    y = uemb\n",
    "    total = len(doc_answers[uid])\n",
    "\n",
    "    torch_model = PyTorchLinearRegression(x.shape[1], lr, constraintHigh, constraintLow, int(total*total_mul), rand_type, L1, L2)\n",
    "    torch_model.fit(x, y, epochs)\n",
    "    \n",
    "    m1 = metric4(torch_model.w.detach().numpy(), doc_answers[uid], w_idx=None, topk=50, verbose=0)\n",
    "    m2 = metric4(torch_model.w.detach().numpy(), doc_answers[uid], w_idx=None, topk=100, verbose=0)\n",
    "    m3 = metric4(torch_model.w.detach().numpy(), doc_answers[uid], w_idx=None, topk=200, verbose=0)\n",
    "    ndcg1 = metric_ndcg(torch_model.w.detach().numpy(), doc_answers[uid], topk=50, verbose=0)\n",
    "    ndcg2 = metric_ndcg(torch_model.w.detach().numpy(), doc_answers[uid], topk=100, verbose=0)\n",
    "    ndcg3 = metric_ndcg(torch_model.w.detach().numpy(), doc_answers[uid], topk=200, verbose=0)\n",
    "    ndcg4 = metric_ndcg(torch_model.w.detach().numpy(), doc_answers[uid], topk=None, verbose=0)\n",
    "    pr[0].append(m1[\"precision\"])\n",
    "    re[0].append(m1[\"recall\"])\n",
    "    pr[1].append(m2[\"precision\"])\n",
    "    re[1].append(m2[\"recall\"])\n",
    "    pr[2].append(m3[\"precision\"])\n",
    "    re[2].append(m3[\"recall\"])\n",
    "    \n",
    "    ndcgs[\"50\"].append(ndcg1)\n",
    "    ndcgs[\"100\"].append(ndcg2)\n",
    "    ndcgs[\"200\"].append(ndcg3)\n",
    "    ndcgs[\"-1\"].append(ndcg4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9ac61124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.4260 Recall:0.1797\n",
      "Precision:0.3290 Recall:0.2738\n",
      "Precision:0.2495 Recall:0.4015\n",
      "NDCG 50:0.2574\n",
      "NDCG 100:0.2772\n",
      "NDCG 200:0.3099\n",
      "NDCG all:0.5547\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision:{np.mean(pr[0]):.4f} Recall:{np.mean(re[0]):.4f}\")\n",
    "print(f\"Precision:{np.mean(pr[1]):.4f} Recall:{np.mean(re[1]):.4f}\")\n",
    "print(f\"Precision:{np.mean(pr[2]):.4f} Recall:{np.mean(re[2]):.4f}\")\n",
    "print(f\"NDCG 50:{np.mean(ndcgs['50']):.4f}\")\n",
    "print(f\"NDCG 100:{np.mean(ndcgs['100']):.4f}\")\n",
    "print(f\"NDCG 200:{np.mean(ndcgs['200']):.4f}\")\n",
    "print(f\"NDCG all:{np.mean(ndcgs['-1']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781f1011",
   "metadata": {},
   "source": [
    "## Top K freq word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97bdd19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 137959),\n",
       " ('and', 71459),\n",
       " ('a', 66359),\n",
       " ('of', 61514),\n",
       " ('to', 52823),\n",
       " ('is', 45511),\n",
       " ('in', 39808),\n",
       " ('it', 31652),\n",
       " ('i', 29011),\n",
       " ('this', 27891)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = [(word, freq) for word, freq in dataset.vocab.word_freq_in_corpus.items()]\n",
    "word_freq.sort(key=lambda x:x[1], reverse=True)\n",
    "word_freq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d065f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed919f7cee347729918866a9da890cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 50 word\n",
      "percision 0.57312\n",
      "recall 0.25995568997152513\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b03336b4b340f58b4a70d8b22d9ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 100 word\n",
      "percision 0.42140999999999995\n",
      "recall 0.3705359126710024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24884d87ce954a3982b488b210249076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 200 word\n",
      "percision 0.2830875\n",
      "recall 0.48635047389378483\n"
     ]
    }
   ],
   "source": [
    "def topk_word_evaluation(k=50):\n",
    "    topk_word = [word for (word, freq) in word_freq[:k]]\n",
    "\n",
    "    pr, re = [], []\n",
    "    for ans in tqdm(test_ans):\n",
    "        ans = set(ans)\n",
    "        ans = [dataset.vocab.itos[a] for a in ans]\n",
    "\n",
    "        hit = []\n",
    "        for word in ans:\n",
    "            if word in topk_word:\n",
    "                hit.append(word)\n",
    "\n",
    "        precision = len(hit) / k\n",
    "        recall = len(hit) / len(ans)\n",
    "        pr.append(precision)\n",
    "        re.append(recall)\n",
    "\n",
    "    print('top {} word'.format(k))\n",
    "    print('percision', np.mean(pr))\n",
    "    print('recall', np.mean(re))\n",
    "\n",
    "topk_word_evaluation(k=50)\n",
    "topk_word_evaluation(k=100)\n",
    "topk_word_evaluation(k=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7aaca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## test dcg\n",
    "# from sklearn.metrics import ndcg_score, dcg_score\n",
    "# k=2\n",
    "\n",
    "# true_relevance = np.asarray([[1, 2, 3, 4]])\n",
    "# scores = np.asarray([[1, 2, 3, 2.5]])\n",
    "# print('dcg',dcg_score(true_relevance, scores,k=k))\n",
    "# print('ndcg',ndcg_score(true_relevance, scores,k=k))\n",
    "\n",
    "\n",
    "# w = 1 / (np.log(np.arange(true_relevance.shape[1])[:k] + 2) / np.log(2))\n",
    "# dcg = true_relevance[0][np.argsort(scores)[0][::-1][:k]].dot(w)\n",
    "# print(dcg)\n",
    "\n",
    "# idcg = np.sort(true_relevance[0])[::-1][:k].dot(w)\n",
    "# print(dcg/idcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f17a67be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab16a2d84774ecab69f8cecf5bd8aa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 50 NDCG:0.08714991654386776\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc17bfb9f7d74d80bbdb036798295244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 100 NDCG:0.11619786532653996\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d66afb09f8492fa187df78899cd7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 200 NDCG:0.15694766452897216\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332e72e1ad41457a835605fd6b4f4aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top None NDCG:0.43048368192931924\n"
     ]
    }
   ],
   "source": [
    "def topk_word_evaluation_NDCG(k=50):\n",
    "    freq_word =[word for (word, freq) in word_freq]\n",
    "    freq_word_idx = [dataset.vocab.stoi[word] for word in freq_word if word in dataset.vocab.stoi]\n",
    "    \n",
    "    scores = np.zeros(len(dataset.vocab.word_vectors))\n",
    "    for rank, idx in enumerate(freq_word_idx):\n",
    "        scores[idx] = len(dataset.vocab.word_vectors) - rank\n",
    "    \n",
    "    NDCGs = []\n",
    "    \n",
    "    for ans in tqdm(test_ans):\n",
    "        TFIDF_ans = np.zeros(len(dataset.vocab.word_vectors))\n",
    "        \n",
    "        for word_idx in ans:\n",
    "            if word_idx == 0:\n",
    "                continue\n",
    "            word = dataset.vocab.itos[word_idx]\n",
    "            TFIDF_ans[word_idx] += dataset.vocab.IDF[word]\n",
    "\n",
    "        NDCG_score = ndcg_score(TFIDF_ans.reshape(1,-1), scores.reshape(1,-1), k=k)\n",
    "        NDCGs.append(NDCG_score)\n",
    "\n",
    "    print('top {} NDCG:{}'.format(k, np.mean(NDCGs)))\n",
    "\n",
    "topk_word_evaluation_NDCG(k=50)\n",
    "topk_word_evaluation_NDCG(k=100)\n",
    "topk_word_evaluation_NDCG(k=200)\n",
    "topk_word_evaluation_NDCG(k=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3827045d",
   "metadata": {},
   "source": [
    "## Nearest Guessing, so bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0bd99ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embs_IDF = word_embs.copy()\n",
    "\n",
    "for word, IDF in dataset.vocab.IDF.items():\n",
    "    word_idx = dataset.vocab.stoi[word]\n",
    "    word_embs_IDF[word_idx] *= IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1db6d370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3608a56b9c74867afbf0d4835244c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pr, re = [[],[],[]], [[],[],[]]\n",
    "ndcgs = defaultdict(list)\n",
    "\n",
    "for uid, uemb in enumerate(tqdm(doc_embs[:100])):\n",
    "    y = uemb\n",
    "    word_embs_IDF\n",
    "    word_weight = np.dot(word_embs_IDF, y)\n",
    "    \n",
    "    m1 = metric4(word_weight, doc_answers[uid], w_idx=None, topk=50, verbose=0)\n",
    "    m2 = metric4(word_weight, doc_answers[uid], w_idx=None, topk=100, verbose=0)\n",
    "    m3 = metric4(word_weight, doc_answers[uid], w_idx=None, topk=200, verbose=0)\n",
    "    ndcg1 = metric_ndcg(word_weight, doc_answers[uid], topk=50, verbose=0)\n",
    "    ndcg2 = metric_ndcg(word_weight, doc_answers[uid], topk=100, verbose=0)\n",
    "    ndcg3 = metric_ndcg(word_weight, doc_answers[uid], topk=200, verbose=0)\n",
    "    ndcg4 = metric_ndcg(word_weight, doc_answers[uid], topk=None, verbose=0)\n",
    "    pr[0].append(m1[\"precision\"])\n",
    "    re[0].append(m1[\"recall\"])\n",
    "    pr[1].append(m2[\"precision\"])\n",
    "    re[1].append(m2[\"recall\"])\n",
    "    pr[2].append(m3[\"precision\"])\n",
    "    re[2].append(m3[\"recall\"])\n",
    "    \n",
    "    ndcgs[\"50\"].append(ndcg1)\n",
    "    ndcgs[\"100\"].append(ndcg2)\n",
    "    ndcgs[\"200\"].append(ndcg3)\n",
    "    ndcgs[\"-1\"].append(ndcg4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "71d4d93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.0120 Recall:0.0054\n",
      "Precision:0.0110 Recall:0.0089\n",
      "Precision:0.0100 Recall:0.0164\n",
      "NDCG 50:0.0159\n",
      "NDCG 100:0.0193\n",
      "NDCG 200:0.0262\n",
      "NDCG all:0.3377\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision:{np.mean(pr[0]):.4f} Recall:{np.mean(re[0]):.4f}\")\n",
    "print(f\"Precision:{np.mean(pr[1]):.4f} Recall:{np.mean(re[1]):.4f}\")\n",
    "print(f\"Precision:{np.mean(pr[2]):.4f} Recall:{np.mean(re[2]):.4f}\")\n",
    "print(f\"NDCG 50:{np.mean(ndcgs['50']):.4f}\")\n",
    "print(f\"NDCG 100:{np.mean(ndcgs['100']):.4f}\")\n",
    "print(f\"NDCG 200:{np.mean(ndcgs['200']):.4f}\")\n",
    "print(f\"NDCG all:{np.mean(ndcgs['-1']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23e9c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
