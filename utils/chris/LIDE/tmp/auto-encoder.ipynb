{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a67d8234",
   "metadata": {},
   "source": [
    "### raw data\n",
    "* word embedding: glove\n",
    "* doc text: ./data/IMDB.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792dad29",
   "metadata": {},
   "source": [
    "### dataset\n",
    "1. IMDB\n",
    "2. CNNNews\n",
    "3. [PubMed](https://github.com/LIAAD/KeywordExtractor-Datasets/blob/master/datasets/PubMed.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b007799a",
   "metadata": {},
   "source": [
    "### preprocess\n",
    "1. filter too frequent and less frequent words\n",
    "2. stemming\n",
    "3. document vector aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6a82ca",
   "metadata": {},
   "source": [
    "### model\n",
    "1. TopK\n",
    "2. Sklearn\n",
    "3. Our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9967b",
   "metadata": {},
   "source": [
    "### evaluation\n",
    "1. F1\n",
    "2. NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9beff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/chrisliu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np \n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from itertools import cycle\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Used to get the data\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "\n",
    "import sklearn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2341254",
   "metadata": {},
   "source": [
    "## Preprocess config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dc01df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "config[\"dataset\"] = \"CNN\" # \"IMDB\" \"CNN\", \"PubMed\"\n",
    "config[\"n_document\"] = 5000\n",
    "config[\"normalize_word_embedding\"] = True\n",
    "config[\"min_word_freq_threshold\"] = 20\n",
    "config[\"topk_word_freq_threshold\"] = 100\n",
    "config[\"document_vector_agg_weight\"] = 'IDF' # ['mean', 'IDF', 'uniform', 'gaussian', 'exponential', 'pmi']\n",
    "config[\"select_topk_TFIDF\"] = None\n",
    "config[\"embedding_file\"] = \"../data/glove.6B.100d.txt\"\n",
    "config[\"topk\"] = [10, 30, 50]\n",
    "\n",
    "sk_lasso_epoch = 10000\n",
    "our_lasso_epoch = 50000\n",
    "is_notebook = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c1627d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529e515a1ec843d0a14fde01b1f34a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:400000\n"
     ]
    }
   ],
   "source": [
    "# load word embedding\n",
    "embedding_file = config[\"embedding_file\"]\n",
    "word2embedding = dict()\n",
    "word_dim = int(re.findall(r\".(\\d+)d\",embedding_file)[0])\n",
    "\n",
    "with open(embedding_file,\"r\") as f:\n",
    "    for line in tqdm(f):\n",
    "        line = line.strip().split()\n",
    "        word = line[0]\n",
    "        embedding = list(map(float,line[1:]))\n",
    "        word2embedding[word] = np.array(embedding)\n",
    "\n",
    "print(\"Number of words:%d\" % len(word2embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47bfc175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb8721403e34f81bdccf9b903475a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def normalize_wordemb(word2embedding):\n",
    "    word_emb = []\n",
    "    word_list = []\n",
    "    for word, emb in word2embedding.items():\n",
    "        word_list.append(word)\n",
    "        word_emb.append(emb)\n",
    "\n",
    "    word_emb = np.array(word_emb)\n",
    "\n",
    "    for i in range(len(word_emb)):\n",
    "        norm = np.linalg.norm(word_emb[i])\n",
    "        word_emb[i] = word_emb[i] / norm\n",
    "\n",
    "    for word, emb in tqdm(zip(word_list, word_emb)):\n",
    "        word2embedding[word] = emb\n",
    "    return word2embedding\n",
    "\n",
    "if config[\"normalize_word_embedding\"]:\n",
    "    normalize_wordemb(word2embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1233053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, word2embedding, min_word_freq_threshold=0, topk_word_freq_threshold=0):\n",
    "        # The low frequency words will be assigned as <UNK> token\n",
    "        self.itos = {0: \"<UNK>\"}\n",
    "        self.stoi = {\"<UNK>\": 0}\n",
    "        \n",
    "        self.word2embedding = word2embedding\n",
    "        self.min_word_freq_threshold = min_word_freq_threshold\n",
    "        self.topk_word_freq_threshold = topk_word_freq_threshold\n",
    "        \n",
    "        self.word_freq_in_corpus = defaultdict(int)\n",
    "        self.IDF = {}\n",
    "        self.ps = PorterStemmer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "#     @staticmethod\n",
    "    def tokenizer_eng(self, text):\n",
    "        text = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n",
    "        text = text.strip().split()\n",
    "        \n",
    "        return [self.ps.stem(w) for w in text if w.lower() not in self.stop_words]\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        self.doc_freq = defaultdict(int) # # of document a word appear\n",
    "        self.document_num = len(sentence_list)\n",
    "        self.word_vectors = [[0]*word_dim] # unknown word emb\n",
    "        \n",
    "        for sentence in tqdm(sentence_list, desc=\"Preprocessing documents\"):\n",
    "            # for doc_freq\n",
    "            document_words = set()\n",
    "            \n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                # pass unknown word\n",
    "                if word not in self.word2embedding:\n",
    "                    continue\n",
    "                    \n",
    "                # calculate word freq\n",
    "                self.word_freq_in_corpus[word] += 1\n",
    "                document_words.add(word)\n",
    "                \n",
    "            for word in document_words:\n",
    "                self.doc_freq[word] += 1\n",
    "        \n",
    "        # calculate IDF\n",
    "        print('doc num', self.document_num)\n",
    "        for word, freq in self.doc_freq.items():\n",
    "            self.IDF[word] = math.log(self.document_num / (freq+1))\n",
    "        \n",
    "        # delete less freq words:\n",
    "        delete_words = []\n",
    "        for word, v in self.word_freq_in_corpus.items():\n",
    "            if v < self.min_word_freq_threshold:\n",
    "                delete_words.append(word)     \n",
    "        for word in delete_words:\n",
    "            del self.IDF[word]    \n",
    "            del self.word_freq_in_corpus[word]    \n",
    "        \n",
    "        # delete too freq words\n",
    "        print('eliminate freq words')\n",
    "        IDF = [(word, freq) for word, freq in self.IDF.items()]\n",
    "        IDF.sort(key=lambda x: x[1])\n",
    "\n",
    "        for i in range(self.topk_word_freq_threshold):\n",
    "            print(word)\n",
    "            word = IDF[i][0]\n",
    "            del self.IDF[word]\n",
    "            del self.word_freq_in_corpus[word]\n",
    "        \n",
    "        # construct word_vectors\n",
    "        idx = 1\n",
    "        for word in self.word_freq_in_corpus:\n",
    "            self.word_vectors.append(self.word2embedding[word])\n",
    "            self.stoi[word] = idx\n",
    "            self.itos[idx] = word\n",
    "            idx += 1\n",
    "            \n",
    "    def init_word_weight(self,sentence_list, agg):\n",
    "        if agg == 'mean':\n",
    "            self.word_weight = {word: 1 for word in self.IDF.keys()}\n",
    "        elif agg == 'IDF':\n",
    "            self.word_weight = self.IDF\n",
    "        elif agg == 'uniform':\n",
    "            self.word_weight = {word: np.random.uniform(low=0.0, high=1.0) for word in self.IDF.keys()}\n",
    "        elif agg == 'gaussian':\n",
    "            mu, sigma = 10, 1 # mean and standard deviation\n",
    "            self.word_weight = {word: np.random.normal(mu, sigma) for word in self.IDF.keys()}\n",
    "        elif agg == 'exponential':\n",
    "            self.word_weight = {word: np.random.exponential(scale=1.0) for word in self.IDF.keys()}\n",
    "        elif agg == 'pmi':\n",
    "            trigram_measures = BigramAssocMeasures()\n",
    "            self.word_weight = defaultdict(int)\n",
    "            corpus = []\n",
    "\n",
    "            for text in tqdm(sentence_list):\n",
    "                corpus.extend(text.split())\n",
    "\n",
    "            finder = BigramCollocationFinder.from_words(corpus)\n",
    "            for pmi_score in finder.score_ngrams(trigram_measures.pmi):\n",
    "                pair, score = pmi_score\n",
    "                self.word_weight[pair[0]] += score\n",
    "                self.word_weight[pair[1]] += score\n",
    "                \n",
    "    def calculate_document_vector(self, sentence_list, agg, n_document, select_topk_TFIDF=None):\n",
    "        document_vectors = []\n",
    "        document_answers = []\n",
    "        document_answers_w = []\n",
    "        \n",
    "        self.init_word_weight(sentence_list, agg)\n",
    "        \n",
    "        for sentence in tqdm(sentence_list[:min(n_document, len(sentence_list))], desc=\"calculate document vectors\"):\n",
    "            document_vector = np.zeros(len(self.word_vectors[0]))\n",
    "            select_words = []\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                # pass unknown word\n",
    "                if word not in self.stoi:\n",
    "                    continue\n",
    "                else:\n",
    "                    select_words.append(word)\n",
    "\n",
    "            # select topk TDIDF\n",
    "            if select_topk_TFIDF is not None:\n",
    "                doc_TFIDF = defaultdict(float)\n",
    "                for word in select_words:    \n",
    "                    doc_TFIDF[word] += self.IDF[word]\n",
    "\n",
    "                doc_TFIDF_l = [(word, TFIDF) for word, TFIDF in doc_TFIDF.items()]\n",
    "                doc_TFIDF_l.sort(key=lambda x:x[1], reverse=True)\n",
    "                \n",
    "                select_topk_words = set(list(map(lambda x:x[0], doc_TFIDF_l[:select_topk_TFIDF])))\n",
    "                select_words = [word for word in select_words if word in select_topk_words]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            total_weight = 0\n",
    "            # aggregate to doc vectors\n",
    "            for word in select_words:\n",
    "                document_vector += np.array(self.word2embedding[word]) * self.word_weight[word]\n",
    "                total_weight += self.word_weight[word]\n",
    "                \n",
    "            if len(select_words) == 0:\n",
    "                print('error', sentence)\n",
    "                continue\n",
    "            else:\n",
    "                document_vector /= total_weight\n",
    "            \n",
    "            document_vectors.append(document_vector)\n",
    "            document_answers.append(select_words)\n",
    "            document_answers_w.append(total_weight)\n",
    "        \n",
    "        # get answers\n",
    "        document_answers_idx = []    \n",
    "        for ans in document_answers:\n",
    "            ans_idx = []\n",
    "            for token in ans:\n",
    "                if token in self.stoi:\n",
    "                    ans_idx.append(self.stoi[token])                    \n",
    "            document_answers_idx.append(ans_idx)\n",
    "\n",
    "        return document_vectors, document_answers_idx, document_answers_w\n",
    "        \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c5706a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBowDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 raw_data_file_path,\n",
    "                 word2embedding,\n",
    "                 skip_header = False,\n",
    "                 n_document = None, # read first n document\n",
    "                 min_word_freq_threshold = 20, # eliminate less freq words\n",
    "                 topk_word_freq_threshold = 5, # eliminate smallest k IDF words\n",
    "                 select_topk_TFIDF = None, # select topk tf-idf as ground-truth\n",
    "                 document_vector_agg_weight = 'mean',\n",
    "                 ):\n",
    "\n",
    "        assert document_vector_agg_weight in ['mean', 'IDF', 'uniform', 'gaussian', 'exponential', 'pmi']\n",
    "        \n",
    "        # raw documents\n",
    "        self.documents = []\n",
    "        \n",
    "        with open(raw_data_file_path,'r',encoding='utf-8') as f:\n",
    "            if skip_header:\n",
    "                f.readline()\n",
    "            for line in tqdm(f, desc=\"Loading documents\"):\n",
    "                # read firt n document\n",
    "                # if n_document is not None and len(self.documents) >= n_document:\n",
    "                #     break    \n",
    "                self.documents.append(line.strip(\"\\n\"))\n",
    "\n",
    "        # build vocabulary\n",
    "        self.vocab = Vocabulary(word2embedding, min_word_freq_threshold, topk_word_freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.documents)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "        # calculate document vectors\n",
    "        self.document_vectors, self.document_answers, self.document_answers_w = self.vocab.calculate_document_vector(self.documents, \\\n",
    "                                                                                           document_vector_agg_weight, n_document, select_topk_TFIDF)\n",
    "                \n",
    "        # train-test split\n",
    "        # training\n",
    "        self.train_split_ratio = 0.8\n",
    "        self.train_length = int(len(self.document_answers) * self.train_split_ratio)\n",
    "        self.train_vectors = self.document_vectors[:self.train_length]\n",
    "        self.train_words = self.document_answers[:self.train_length]\n",
    "        self.document_ids = list(range(self.train_length))\n",
    "        self.generator = cycle(self.context_target_generator())\n",
    "        self.dataset_size = sum([len(s) for s in self.train_words])\n",
    "        \n",
    "        # testing\n",
    "        self.test_vectors = self.document_vectors[self.train_length:]\n",
    "        self.test_words = self.document_answers[self.train_length:]\n",
    "\n",
    "    def context_target_generator(self):\n",
    "        np.random.shuffle(self.document_ids) # inplace shuffle\n",
    "\n",
    "        # randomly select a document and create its training example\n",
    "        for document_id in self.document_ids: \n",
    "            word_list = set(self.train_words[document_id])\n",
    "            negative_sample_space = list(set(range(self.vocab_size)) - word_list)\n",
    "            negative_samples = np.random.choice(negative_sample_space,size=len(word_list),replace = False)\n",
    "            for word_id, negative_wordID in zip(word_list, negative_samples):\n",
    "                yield [document_id, word_id, negative_wordID]\n",
    "                \n",
    "    def __getitem__(self, idx):\n",
    "        doc_id, word_id, negative_wordID = next(self.generator)\n",
    "        doc_id = torch.FloatTensor(self.document_vectors[doc_id])\n",
    "        word_id = torch.FloatTensor(self.vocab.word_vectors[word_id])\n",
    "        negative_word = torch.FloatTensor(self.vocab.word_vectors[negative_wordID])\n",
    "\n",
    "        return doc_id, word_id, negative_word\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1a7186c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c130ee3b324470b15524602a23c4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading documents: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36112f8080b4cfb841b19c41351a398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing documents:   0%|          | 0/19026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc num 19026\n",
      "eliminate freq words\n",
      "paroxysm\n",
      "subject\n",
      "line\n",
      "organ\n",
      "write\n",
      "univers\n",
      "one\n",
      "would\n",
      "use\n",
      "like\n",
      "get\n",
      "know\n",
      "dont\n",
      "think\n",
      "time\n",
      "make\n",
      "also\n",
      "say\n",
      "go\n",
      "im\n",
      "could\n",
      "want\n",
      "new\n",
      "work\n",
      "good\n",
      "well\n",
      "way\n",
      "need\n",
      "look\n",
      "even\n",
      "anyon\n",
      "thing\n",
      "see\n",
      "tri\n",
      "thank\n",
      "much\n",
      "year\n",
      "world\n",
      "system\n",
      "right\n",
      "problem\n",
      "may\n",
      "take\n",
      "mani\n",
      "two\n",
      "first\n",
      "seem\n",
      "question\n",
      "pleas\n",
      "1\n",
      "state\n",
      "us\n",
      "come\n",
      "2\n",
      "post\n",
      "help\n",
      "call\n",
      "usa\n",
      "point\n",
      "sinc\n",
      "find\n",
      "read\n",
      "still\n",
      "back\n",
      "mean\n",
      "ive\n",
      "give\n",
      "email\n",
      "sure\n",
      "differ\n",
      "might\n",
      "run\n",
      "cant\n",
      "reason\n",
      "last\n",
      "day\n",
      "interest\n",
      "case\n",
      "let\n",
      "person\n",
      "said\n",
      "never\n",
      "start\n",
      "doesnt\n",
      "tell\n",
      "better\n",
      "ask\n",
      "got\n",
      "without\n",
      "follow\n",
      "part\n",
      "lot\n",
      "3\n",
      "number\n",
      "put\n",
      "fact\n",
      "gener\n",
      "inform\n",
      "actual\n",
      "that\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b02bb6a9c54637ba84b4cd402fe2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "calculate document vectors:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error |> \n",
      "error \n",
      "error \n",
      "error Mikael Fredriksson\n",
      "error \n",
      "error -------------------------------------------------\n",
      "error email: mikael_fredriksson@macexchange.se\n",
      "error \n",
      "error FIDO 2:203/211\n",
      "error  \n"
     ]
    }
   ],
   "source": [
    "# load and build torch dataset\n",
    "if config[\"dataset\"] == 'IMDB':\n",
    "    data_file_path = '../data/IMDB.txt'\n",
    "elif config[\"dataset\"] == 'CNN':\n",
    "    data_file_path = '../data/CNN.txt'\n",
    "elif config[\"dataset\"] == 'PubMed':\n",
    "    data_file_path = '../data/PubMed.txt'\n",
    "\n",
    "print(\"Building dataset....\")\n",
    "dataset = CBowDataset(\n",
    "                    raw_data_file_path=data_file_path,\n",
    "                    word2embedding=word2embedding,\n",
    "                    skip_header=False,\n",
    "                    n_document = config[\"n_document\"],\n",
    "                    min_word_freq_threshold = config[\"min_word_freq_threshold\"],\n",
    "                    topk_word_freq_threshold = config[\"topk_word_freq_threshold\"],\n",
    "                    document_vector_agg_weight = config[\"document_vector_agg_weight\"],\n",
    "                    select_topk_TFIDF = config[\"select_topk_TFIDF\"]\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7e6d657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish building dataset!\n",
      "Number of documents:19026\n",
      "Number of words:7602\n",
      "Average length of document: 86.76372745490983\n"
     ]
    }
   ],
   "source": [
    "print(\"Finish building dataset!\")\n",
    "print(f\"Number of documents:{len(dataset.documents)}\")\n",
    "print(f\"Number of words:{dataset.vocab_size}\")\n",
    "\n",
    "l = list(map(len, dataset.document_answers))\n",
    "print(\"Average length of document:\", np.mean(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba4758da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# check test doc vectors' correctness\n",
    "word_vectors = np.array(dataset.vocab.word_vectors)\n",
    "word_vectors.shape\n",
    "\n",
    "pred = np.zeros(word_vectors.shape[1])\n",
    "cnt = 0\n",
    "for word_idx in dataset.test_words[0]:\n",
    "    pred += word_vectors[word_idx] * dataset.vocab.word_weight[dataset.vocab.itos[word_idx]]\n",
    "    cnt += dataset.vocab.word_weight[dataset.vocab.itos[word_idx]]\n",
    "print(dataset.test_vectors[0] - pred/cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe65f37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4990, 7602)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b7b0a80d984eb3bb127de810d38e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4990 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## create weight_ans\n",
    "document_answers = dataset.document_answers\n",
    "\n",
    "onehot_ans = np.zeros((len(document_answers), word_vectors.shape[0]))\n",
    "weight_ans = np.zeros((len(document_answers), word_vectors.shape[0]))\n",
    "print(weight_ans.shape)\n",
    "\n",
    "for i in tqdm(range(len(document_answers))):\n",
    "    for word_idx in document_answers[i]:\n",
    "        weight_ans[i, word_idx] += dataset.vocab.word_weight[dataset.vocab.itos[word_idx]]\n",
    "        onehot_ans[i, word_idx] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eef4431",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vectors = np.array(dataset.document_vectors)\n",
    "document_answers_w = np.array(dataset.document_answers_w).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca2e49",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a52979e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model',\n",
       " 'F1@10',\n",
       " 'F1@30',\n",
       " 'F1@50',\n",
       " 'ndcg@10',\n",
       " 'ndcg@30',\n",
       " 'ndcg@50',\n",
       " 'ndcg@all']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results = []\n",
    "select_columns = ['model']\n",
    "for topk in config[\"topk\"]:\n",
    "    select_columns.append('F1@{}'.format(topk))\n",
    "for topk in config[\"topk\"]:\n",
    "    select_columns.append('ndcg@{}'.format(topk))\n",
    "select_columns.append('ndcg@all')\n",
    "select_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f24de9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , 28.63294427,  6.94349145, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87d5379",
   "metadata": {},
   "source": [
    "## setting training size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f9ddbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4491"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size_ratio = 0.9\n",
    "train_size = int(len(dataset.document_answers) * train_size_ratio)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52570ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4990, 7602)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_ans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1da89694",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_ans = weight_ans.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2ffb00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = weight_ans[:train_size]\n",
    "valid_data = weight_ans[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c9f11d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalize\n",
    "train_data_normalize, norm = sklearn.preprocessing.normalize(train_data, norm='l2', axis=0, copy=True, return_norm=True)\n",
    "valid_data_normalize = valid_data / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76a006c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data - train_data_normalize * norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8387cf56",
   "metadata": {},
   "source": [
    "## Simple AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b9954fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 500),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(500, 200),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(200, 100)\n",
    "        )\n",
    "          \n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(100, 200),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(200, 500),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(500, input_dim),\n",
    "        )\n",
    "  \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd8a1d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 500),\n",
    "        )\n",
    "          \n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(500, input_dim),\n",
    "        )\n",
    "  \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6fdfd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE_Dataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 weight_ans\n",
    "                 ):\n",
    "        self.weight_ans = weight_ans\n",
    "        \n",
    "    def __getitem__(self, idx):        \n",
    "        return self.weight_ans[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.weight_ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e529ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AE_Dataset(train_data)\n",
    "valid_dataset = AE_Dataset(valid_data)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "valid_loader  = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "130f2dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "model = PCA(weight_ans.shape[1])\n",
    "  \n",
    "# Validation using MSE Loss function\n",
    "loss_function = torch.nn.MSELoss()\n",
    "  \n",
    "# Using an Adam Optimizer with lr = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(),\n",
    "                             lr = 1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a6ac0b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0, Train loss 1.865369272728761\n",
      "Epoch 0, Valid loss 4.59704565256834\n",
      "\n",
      "Epoch 1, Train loss 1.8701866194605827\n",
      "Epoch 1, Valid loss 4.574359051883221\n",
      "\n",
      "Epoch 2, Train loss 1.8527282584044669\n",
      "Epoch 2, Valid loss 4.552716501057148\n",
      "\n",
      "Epoch 3, Train loss 1.8730254015988774\n",
      "Epoch 3, Valid loss 4.532012373209\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-62bdcb1505d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtual_env/py37/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtual_env/py37/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "outputs = []\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    train_epoch_loss = []\n",
    "    \n",
    "    for tf_idf_arr in train_loader:\n",
    "        # Output of Autoencoder\n",
    "        reconstructed = model(tf_idf_arr)\n",
    "\n",
    "        # Calculating the loss function\n",
    "        loss = loss_function(reconstructed, tf_idf_arr)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Storing the losses in a list for plotting\n",
    "        losses.append(loss)\n",
    "        train_epoch_loss.append(loss.item())\n",
    "        \n",
    "    print()\n",
    "    print('Epoch {}, Train loss {}'.format(epoch, np.mean(train_epoch_loss)))\n",
    "    \n",
    "    valid_epoch_loss = []\n",
    "    for tf_idf_arr in valid_loader:\n",
    "        # Output of Autoencoder\n",
    "        reconstructed = model(tf_idf_arr)\n",
    "\n",
    "        # Calculating the loss function\n",
    "        loss = loss_function(reconstructed, tf_idf_arr)\n",
    "        valid_epoch_loss.append(loss.item())\n",
    "    \n",
    "    print('Epoch {}, Valid loss {}'.format(epoch, np.mean(valid_epoch_loss)))\n",
    "    \n",
    "    outputs.append((epochs, tf_idf_arr, reconstructed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844e063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Plot Style\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "  \n",
    "# Plotting the last 100 values\n",
    "plt.plot(train_epoch_loss[-100:])\n",
    "plt.plot(valid_epoch_loss[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a9cdf3",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1b9dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sklearn(pred, ans):\n",
    "    results = {}\n",
    "        \n",
    "    one_hot_ans = np.arange(ans.shape[0])[ans > 0]\n",
    "    \n",
    "    for topk in config[\"topk\"]:\n",
    "        one_hot_pred = np.argsort(pred)[-topk:]\n",
    "        hit = np.intersect1d(one_hot_pred, one_hot_ans)\n",
    "        percision = len(hit) / topk\n",
    "        recall = len(hit) / len(one_hot_ans)\n",
    "        f1 = 2 * percision * recall / (percision + recall) if (percision + recall) > 0 else 0\n",
    "        \n",
    "        results['F1@{}'.format(topk)] = f1\n",
    "        \n",
    "    ans = ans.reshape(1, -1)\n",
    "    pred = pred.reshape(1, -1)\n",
    "    for topk in config[\"topk\"]:\n",
    "        results['ndcg@{}'.format(topk)] = ndcg_score(ans, pred, k=topk)\n",
    "\n",
    "    results['ndcg@all'] = (ndcg_score(ans, pred, k=None))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb630b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dict_learning] ."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   8 | elapsed:   25.7s remaining:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:   26.5s finished\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SparsePCA' object has no attribute 'inverse_transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-eaac0717d727>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrain_data_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_data_reconstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_pca\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mvalid_data_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparsePCA' object has no attribute 'inverse_transform'"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "n_components = 100\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(train_data)\n",
    "train_data_pca = pca.transform(train_data)\n",
    "train_data_reconstruct = pca.inverse_transform(train_data_pca)\n",
    "\n",
    "valid_data_pca = pca.transform(valid_data)\n",
    "valid_data_reconstruct = pca.inverse_transform(valid_data_pca)\n",
    "\n",
    "print('train reconstruct error', np.square(train_data - train_data_reconstruct).mean())\n",
    "print('valid reconstruct error', np.square(valid_data - valid_data_reconstruct).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5a7188e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09738a46e4a7412cb997a50786fe4541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869bddf2b0d24edbad135af1913da1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "F1@10       0.128635\n",
       "F1@30       0.167418\n",
       "F1@50       0.175942\n",
       "ndcg@10     0.290266\n",
       "ndcg@30     0.277581\n",
       "ndcg@50     0.283878\n",
       "ndcg@all    0.504568\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "F1@10       0.124016\n",
       "F1@30       0.158305\n",
       "F1@50       0.165345\n",
       "ndcg@10     0.276674\n",
       "ndcg@30     0.263011\n",
       "ndcg@50     0.269519\n",
       "ndcg@all    0.492219\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_train = []\n",
    "results_valid = []\n",
    "\n",
    "for i in tqdm(range(train_data.shape[0])):\n",
    "    r = evaluate_sklearn(train_data_reconstruct[i], train_data[i])\n",
    "    results_train.append(r)\n",
    "\n",
    "for i in tqdm(range(valid_data.shape[0])):\n",
    "    r = evaluate_sklearn(valid_data_reconstruct[i], valid_data[i])\n",
    "    results_valid.append(r)\n",
    "\n",
    "results_train = pd.DataFrame(results_train).mean()\n",
    "results_valid = pd.DataFrame(results_valid).mean()\n",
    "\n",
    "display(results_train)\n",
    "display(results_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcd3886",
   "metadata": {},
   "outputs": [],
   "source": [
    "## with normalization\n",
    "\n",
    "from sklearn.decomposition import PCA, SparsePCA, TruncatedSVD\n",
    "n_components = 100\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(train_data_normalize)\n",
    "train_data_pca = pca.transform(train_data_normalize)\n",
    "train_data_reconstruct = pca.inverse_transform(train_data_pca)\n",
    "\n",
    "valid_data_pca = pca.transform(valid_data_normalize)\n",
    "valid_data_reconstruct = pca.inverse_transform(valid_data_pca)\n",
    "\n",
    "print('train reconstruct error', np.square(train_data - train_data_reconstruct * norm).mean())\n",
    "print('valid reconstruct error', np.square(valid_data - valid_data_reconstruct * norm).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e5d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = []\n",
    "results_valid = []\n",
    "\n",
    "for i in tqdm(range(train_data.shape[0])):\n",
    "    r = evaluate_sklearn(train_data_reconstruct[i], train_data[i])\n",
    "    results_train.append(r)\n",
    "\n",
    "for i in tqdm(range(valid_data.shape[0])):\n",
    "    r = evaluate_sklearn(valid_data_reconstruct[i], valid_data[i])\n",
    "    results_valid.append(r)\n",
    "\n",
    "results_train = pd.DataFrame(results_train).mean()\n",
    "results_valid = pd.DataFrame(results_valid).mean()\n",
    "\n",
    "display(results_train)\n",
    "display(results_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158af1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
