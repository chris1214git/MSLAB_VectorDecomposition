{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982acb8f",
   "metadata": {},
   "source": [
    "## Siamese network \n",
    "Steps:\n",
    "1. load word embeding and document embedding\n",
    "2. create pytorch dataset and dataloader\n",
    "3. Try Contrastive loss and triplet loss\n",
    "4. further improve negative sampling (e.g. hard negative or word2vec negative sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67d8234",
   "metadata": {},
   "source": [
    "### raw data\n",
    "* word embedding: glove\n",
    "* doc text: ./data/IMDB.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b007799a",
   "metadata": {},
   "source": [
    "### preprocess\n",
    "1. filter too frequent and less frequent words\n",
    "2. stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6a82ca",
   "metadata": {},
   "source": [
    "### model\n",
    "1. Siamese\n",
    "2. TopK\n",
    "3. DNN\n",
    "4. Lasso\n",
    "5. LassoGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9967b",
   "metadata": {},
   "source": [
    "### evaluation\n",
    "1. F1\n",
    "2. NDCG\n",
    "    1. MAP\n",
    "    2. MRR\n",
    "    3. ERR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e9beff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np \n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from itertools import cycle\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.metrics import ndcg_score\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2341254",
   "metadata": {},
   "source": [
    "## Preprocess config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5dc01df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "config[\"n_document\"] = 10000\n",
    "config[\"min_word_freq_threshold\"] = 20\n",
    "config[\"topk_word_freq_threshold\"] = 100\n",
    "config[\"document_vector_agg\"] = 'TF-IDF'\n",
    "config[\"select_topk_TFIDF\"] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05c1627d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8988dc6d6dc34597b5d8910a15a57ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:400000\n"
     ]
    }
   ],
   "source": [
    "# load word embedding\n",
    "embedding_file = \"../data/glove.6B.100d.txt\"\n",
    "\n",
    "word2embedding = dict()\n",
    "word_dim = int(re.findall(r\".(\\d+)d\",embedding_file)[0])\n",
    "\n",
    "with open(embedding_file,\"r\") as f:\n",
    "    for line in tqdm(f):\n",
    "        line = line.strip().split()\n",
    "        word = line[0]\n",
    "        embedding = list(map(float,line[1:]))\n",
    "        word2embedding[word] = embedding\n",
    "\n",
    "print(\"Number of words:%d\" % len(word2embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1233053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, word2embedding, min_word_freq_threshold=0, topk_word_freq_threshold=0):\n",
    "        # The low frequency words will be assigned as <UNK> token\n",
    "        self.itos = {0: \"<UNK>\"}\n",
    "        self.stoi = {\"<UNK>\": 0}\n",
    "        \n",
    "        self.word2embedding = word2embedding\n",
    "        self.min_word_freq_threshold = min_word_freq_threshold\n",
    "        self.topk_word_freq_threshold = topk_word_freq_threshold\n",
    "        \n",
    "        self.word_freq_in_corpus = defaultdict(int)\n",
    "        self.IDF = {}\n",
    "        self.ps = PorterStemmer()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "#     @staticmethod\n",
    "    def tokenizer_eng(self, text):\n",
    "        text = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n",
    "        text = text.strip().split()\n",
    "        \n",
    "        return [self.ps.stem(w) for w in text]\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        \n",
    "        self.doc_freq = defaultdict(int) # # of document a word appear\n",
    "        self.document_num = len(sentence_list)\n",
    "        self.word_vectors = [[0]*word_dim] # unknown word emb\n",
    "        \n",
    "        for sentence in tqdm(sentence_list, desc=\"Preprocessing documents\"):\n",
    "            # for doc_freq\n",
    "            document_words = set()\n",
    "            \n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                # pass unknown word\n",
    "                if word not in self.word2embedding:\n",
    "                    continue\n",
    "                    \n",
    "                # calculate word freq\n",
    "                self.word_freq_in_corpus[word] += 1\n",
    "                document_words.add(word)\n",
    "                \n",
    "            for word in document_words:\n",
    "                self.doc_freq[word] += 1\n",
    "        \n",
    "        # calculate IDF\n",
    "        print('doc num', self.document_num)\n",
    "        for word, freq in self.doc_freq.items():\n",
    "            self.IDF[word] = math.log(self.document_num / (freq+1))\n",
    "        \n",
    "        # delete less freq words:\n",
    "        delete_words = []\n",
    "        for word, v in self.word_freq_in_corpus.items():\n",
    "            if v < self.min_word_freq_threshold:\n",
    "                delete_words.append(word)     \n",
    "        for word in delete_words:\n",
    "            del self.IDF[word]    \n",
    "            del self.word_freq_in_corpus[word]    \n",
    "        \n",
    "        # delete too freq words\n",
    "        print('eliminate freq words')\n",
    "        IDF = [(word, freq) for word, freq in self.IDF.items()]\n",
    "        IDF.sort(key=lambda x: x[1])\n",
    "\n",
    "        for i in range(self.topk_word_freq_threshold):\n",
    "            print(word)\n",
    "            word = IDF[i][0]\n",
    "            del self.IDF[word]\n",
    "            del self.word_freq_in_corpus[word]\n",
    "        \n",
    "        # construct word_vectors\n",
    "        idx = 1\n",
    "        for word in self.word_freq_in_corpus:\n",
    "            self.word_vectors.append(self.word2embedding[word])\n",
    "            self.stoi[word] = idx\n",
    "            self.itos[idx] = word\n",
    "            idx += 1\n",
    "            \n",
    "    def calculate_document_vector(self, sentence_list, agg, select_topk_TFIDF=None):\n",
    "        document_vectors = []\n",
    "        document_answers = []\n",
    "        document_answers_w = []\n",
    "        \n",
    "        for sentence in tqdm(sentence_list, desc=\"calculate document vectors\"):\n",
    "            document_vector = np.zeros(len(self.word_vectors[0]))\n",
    "            select_words = []\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                # pass unknown word\n",
    "                if word not in self.stoi:\n",
    "                    continue\n",
    "                else:\n",
    "                    select_words.append(word)\n",
    "\n",
    "            # select topk TDIDF\n",
    "            if select_topk_TFIDF is not None:\n",
    "                doc_TFIDF = defaultdict(float)\n",
    "                for word in select_words:    \n",
    "                    doc_TFIDF[word] += self.IDF[word]\n",
    "\n",
    "                doc_TFIDF_l = [(word, TFIDF) for word, TFIDF in doc_TFIDF.items()]\n",
    "                doc_TFIDF_l.sort(key=lambda x:x[1], reverse=True)\n",
    "                \n",
    "                select_topk_words = set(list(map(lambda x:x[0], doc_TFIDF_l[:select_topk_TFIDF])))\n",
    "                select_words = [word for word in select_words if word in select_topk_words]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            total_weight = 0\n",
    "            # aggregate to doc vectors\n",
    "            for word in select_words:\n",
    "                if agg == 'mean':\n",
    "                    document_vector += self.word2embedding[word]\n",
    "                    total_weight += 1\n",
    "                elif agg == 'TF-IDF':\n",
    "                    document_vector += np.array(self.word2embedding[word]) * self.IDF[word]\n",
    "                    total_weight += self.IDF[word]\n",
    "\n",
    "            if len(select_words) == 0:\n",
    "                print('error', sentence)\n",
    "                continue\n",
    "            else:\n",
    "                document_vector /= len(select_words)\n",
    "                total_weight /= len(select_words)\n",
    "            \n",
    "            document_vectors.append(document_vector)\n",
    "            document_answers.append(select_words)\n",
    "            document_answers_w.append(total_weight)\n",
    "        \n",
    "        # get answers\n",
    "        document_answers_idx = []    \n",
    "        for ans in document_answers:\n",
    "            ans_idx = []\n",
    "            for token in ans:\n",
    "                if token in self.stoi:\n",
    "                    ans_idx.append(self.stoi[token])                    \n",
    "            document_answers_idx.append(ans_idx)\n",
    "            \n",
    "        return document_vectors, document_answers_idx, document_answers_w\n",
    "        \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c5706a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBowDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 raw_data_file_path,\n",
    "                 word2embedding,\n",
    "                 skip_header = False,\n",
    "                 n_document = None, # read first n document\n",
    "                 min_word_freq_threshold = 20, # eliminate less freq words\n",
    "                 topk_word_freq_threshold = 5, # eliminate smallest k IDF words\n",
    "                 select_topk_TFIDF = None, # select topk tf-idf as ground-truth\n",
    "                 document_vector_agg = 'mean',\n",
    "                 ):\n",
    "\n",
    "        assert document_vector_agg in ['mean', 'TF-IDF']\n",
    "        \n",
    "        # raw documents\n",
    "        self.documents = []\n",
    "        \n",
    "        with open(raw_data_file_path,'r',encoding='utf-8') as f:\n",
    "            if skip_header:\n",
    "                f.readline()\n",
    "            for line in tqdm(f, desc=\"Loading documents\"):\n",
    "                # read firt n document\n",
    "                if n_document is not None and len(self.documents) >= n_document:\n",
    "                    break    \n",
    "                self.documents.append(line.strip(\"\\n\"))\n",
    "\n",
    "        # build vocabulary\n",
    "        self.vocab = Vocabulary(word2embedding, min_word_freq_threshold, topk_word_freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.documents)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "        # calculate document vectors\n",
    "        self.document_vectors, self.document_answers, self.document_answers_w = self.vocab.calculate_document_vector(self.documents, \\\n",
    "                                                                                           document_vector_agg, select_topk_TFIDF)\n",
    "        # train-test split\n",
    "        # training\n",
    "        self.train_split_ratio = 0.8\n",
    "        self.train_length = int(len(self.document_answers) * self.train_split_ratio)\n",
    "        self.train_vectors = self.document_vectors[:self.train_length]\n",
    "        self.train_words = self.document_answers[:self.train_length]\n",
    "        self.document_ids = list(range(self.train_length))\n",
    "        self.generator = cycle(self.context_target_generator())\n",
    "        self.dataset_size = sum([len(s) for s in self.train_words])\n",
    "        \n",
    "        # testing\n",
    "        self.test_vectors = self.document_vectors[self.train_length:]\n",
    "        self.test_words = self.document_answers[self.train_length:]\n",
    "\n",
    "    def context_target_generator(self):\n",
    "        np.random.shuffle(self.document_ids) # inplace shuffle\n",
    "\n",
    "        # randomly select a document and create its training example\n",
    "        for document_id in self.document_ids: \n",
    "            word_list = set(self.train_words[document_id])\n",
    "            negative_sample_space = list(set(range(self.vocab_size)) - word_list)\n",
    "            negative_samples = np.random.choice(negative_sample_space,size=len(word_list),replace = False)\n",
    "            for word_id, negative_wordID in zip(word_list, negative_samples):\n",
    "                yield [document_id, word_id, negative_wordID]\n",
    "                \n",
    "    def __getitem__(self, idx):\n",
    "        doc_id, word_id, negative_wordID = next(self.generator)\n",
    "        doc_id = torch.FloatTensor(self.document_vectors[doc_id])\n",
    "        word_id = torch.FloatTensor(self.vocab.word_vectors[word_id])\n",
    "        negative_word = torch.FloatTensor(self.vocab.word_vectors[negative_wordID])\n",
    "\n",
    "        return doc_id, word_id, negative_word\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1a7186c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89bbb852ceac4ff3acaa89ce7a46ddc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading documents: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac27a80c61354131b33b237efbac187c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing documents:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc num 10000\n",
      "eliminate freq words\n",
      "boel\n",
      "the\n",
      "and\n",
      "a\n",
      "of\n",
      "to\n",
      "is\n",
      "it\n",
      "thi\n",
      "in\n",
      "that\n",
      "i\n",
      "for\n",
      "with\n",
      "but\n",
      "as\n",
      "on\n",
      "wa\n",
      "be\n",
      "film\n",
      "one\n",
      "not\n",
      "are\n",
      "have\n",
      "all\n",
      "an\n",
      "you\n",
      "at\n",
      "by\n",
      "from\n",
      "who\n",
      "like\n",
      "hi\n",
      "ha\n",
      "so\n",
      "he\n",
      "time\n",
      "about\n",
      "out\n",
      "there\n",
      "if\n",
      "veri\n",
      "see\n",
      "good\n",
      "what\n",
      "more\n",
      "they\n",
      "when\n",
      "just\n",
      "some\n",
      "or\n",
      "make\n",
      "watch\n",
      "great\n",
      "get\n",
      "well\n",
      "my\n",
      "other\n",
      "up\n",
      "can\n",
      "love\n",
      "also\n",
      "which\n",
      "would\n",
      "their\n",
      "will\n",
      "even\n",
      "most\n",
      "her\n",
      "me\n",
      "had\n",
      "much\n",
      "than\n",
      "first\n",
      "do\n",
      "way\n",
      "into\n",
      "play\n",
      "end\n",
      "were\n",
      "no\n",
      "best\n",
      "scene\n",
      "think\n",
      "been\n",
      "how\n",
      "go\n",
      "look\n",
      "show\n",
      "made\n",
      "she\n",
      "after\n",
      "we\n",
      "year\n",
      "mani\n",
      "work\n",
      "know\n",
      "too\n",
      "seen\n",
      "act\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4158f073843444dab4fa117fff30249c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "calculate document vectors:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish building dataset!\n",
      "Number of documents:10000\n",
      "Number of words:4527\n"
     ]
    }
   ],
   "source": [
    "# load and build torch dataset\n",
    "data_file_path = '../data/IMDB.txt'\n",
    "\n",
    "print(\"Building dataset....\")\n",
    "dataset = CBowDataset(\n",
    "                    raw_data_file_path=data_file_path,\n",
    "                    word2embedding=word2embedding,\n",
    "                    skip_header=False,\n",
    "                    n_document = config[\"n_document\"],\n",
    "                    min_word_freq_threshold = config[\"min_word_freq_threshold\"],\n",
    "                    topk_word_freq_threshold = config[\"topk_word_freq_threshold\"],\n",
    "                    document_vector_agg = config[\"document_vector_agg\"],\n",
    "                    select_topk_TFIDF = config[\"select_topk_TFIDF\"]\n",
    "                    )\n",
    "print(\"Finish building dataset!\")\n",
    "print(f\"Number of documents:{len(dataset.documents)}\")\n",
    "print(f\"Number of words:{dataset.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba4758da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# check test doc vectors' correctness\n",
    "word_vectors = np.array(dataset.vocab.word_vectors)\n",
    "word_vectors.shape\n",
    "\n",
    "pred = np.zeros(100)\n",
    "cnt = 0\n",
    "for word_idx in dataset.test_words[0]:\n",
    "    pred += word_vectors[word_idx] * dataset.vocab.IDF[dataset.vocab.itos[word_idx]]\n",
    "    cnt += 1\n",
    "print(dataset.test_vectors[0] - pred/cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe65f37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 4527)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3976532c2024275bfc367def1fd831f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## create tfidf_ans\n",
    "document_answers = dataset.document_answers\n",
    "\n",
    "onehot_ans = np.zeros((len(document_answers), word_vectors.shape[0]))\n",
    "tfidf_ans = np.zeros((len(document_answers), word_vectors.shape[0]))\n",
    "print(tfidf_ans.shape)\n",
    "\n",
    "for i in tqdm(range(len(document_answers))):\n",
    "    for word_idx in document_answers[i]:\n",
    "        tfidf_ans[i, word_idx] += dataset.vocab.IDF[dataset.vocab.itos[word_idx]]\n",
    "        onehot_ans[i, word_idx] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf6bf5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 doc_vectors,\n",
    "                 ans_words,\n",
    "                 ):\n",
    "        self.doc_vectors = doc_vectors\n",
    "        self.ans_words = ans_words\n",
    "        assert len(doc_vectors) == len(ans_words)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        doc_vec = torch.FloatTensor(self.doc_vectors[idx])\n",
    "        ans_w = torch.tensor(list(set(self.ans_words[idx])))\n",
    "        return doc_vec, ans_w\n",
    "\n",
    "    def collate_fn(self,batch):\n",
    "        # Batch: List of tuples [(batch1), (batch2)]\n",
    "        \n",
    "        doc_vec = torch.cat([item[0].unsqueeze(0) for item in batch], dim=0)\n",
    "        ans_w = [item[1] for item in batch]\n",
    "        ans_w = pad_sequence(ans_w, batch_first=True, padding_value=-1)\n",
    "        \n",
    "        return doc_vec, ans_w \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "457c5d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletNet(nn.Module):\n",
    "    def __init__(self, hdim):\n",
    "        super(TripletNet, self).__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(hdim, 256),\n",
    "                        nn.PReLU(),\n",
    "                        nn.Linear(256, 256),\n",
    "                        nn.PReLU(),\n",
    "                        nn.Linear(256, 2)\n",
    "                        )\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        output1 = self.fc(x1)\n",
    "        output2 = self.fc(x2)\n",
    "        output3 = self.fc(x3)\n",
    "        return output1, output2, output3\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "59ec9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet loss\n",
    "    Takes embeddings of an anchor sample, a positive sample and a negative sample\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative, size_average=True):\n",
    "        distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)\n",
    "        distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)\n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean() if size_average else losses.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b60ca7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 1.\n",
    "BATCH_SIZE = 1024\n",
    "EPOCH = 20\n",
    "\n",
    "device = \"cuda:0\"\n",
    "model = TripletNet(word_dim).to(device)\n",
    "loss_fn = TripletLoss(margin).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "22a934f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "                        dataset, \n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        num_workers=4,\n",
    "                        shuffle=True,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2aa0290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docvec = dataset.test_vectors\n",
    "test_ans = dataset.test_words\n",
    "test_dataset = TestDataset(test_docvec,test_ans)\n",
    "test_loader = DataLoader(test_dataset,                         \n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         num_workers=4,\n",
    "                         collate_fn=test_dataset.collate_fn)\n",
    "word_embedding_tensor = torch.FloatTensor(dataset.vocab.word_vectors).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05bb89ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_word_emb, loader,Ks = [50,100,150,200]):\n",
    "    avg_precision, avg_recall = [], []\n",
    "    for batch in test_loader:\n",
    "        batch = [item.to(device) for item in batch]\n",
    "        emb, ans = batch\n",
    "        emb = model.get_embedding(emb)\n",
    "        scores = torch.cdist(emb, test_word_emb)\n",
    "        ans_length = torch.sum((~ans.eq(-1)).float(), dim=-1)\n",
    "        mask = ~ans.eq(-1).unsqueeze(-1)\n",
    "        \n",
    "        # calculate precision and recall\n",
    "        tmp_pr, tmp_re = [],[]\n",
    "        for K in Ks:\n",
    "            top_indices = torch.argsort(scores,dim=1)[:,:K]\n",
    "            hit = top_indices.unsqueeze(-2) == ans.unsqueeze(-1)\n",
    "            hit = torch.sum((hit * mask).flatten(1),dim=-1)\n",
    "            precision = hit / K\n",
    "            recall = hit / ans_length\n",
    "            tmp_pr.append(precision)\n",
    "            tmp_re.append(recall)\n",
    "        tmp_pr = torch.stack(tmp_pr).T.detach().cpu().numpy().tolist()\n",
    "        tmp_re = torch.stack(tmp_re).T.detach().cpu().numpy().tolist()\n",
    "        avg_precision.extend(tmp_pr)\n",
    "        avg_recall.extend(tmp_re)\n",
    "        \n",
    "    avg_precision = np.mean(avg_precision,axis=0)\n",
    "    avg_recall = np.mean(avg_recall, axis=0)\n",
    "    for idx, kval in enumerate(Ks):\n",
    "        print(f\"[K={kval}] Precision:{avg_precision[idx]:.4f} Recall:{avg_recall[idx]:.4f}\")\n",
    "    return avg_precision, avg_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c124d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(EPOCH):\n",
    "#     avg_loss = []\n",
    "#     model.train()\n",
    "#     for batch in tqdm(train_loader):\n",
    "#         batch = [item.to(device) for item in batch]\n",
    "#         doc_id,pos_w,neg_w = batch\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = loss_fn(*model(doc_id,pos_w,neg_w))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         avg_loss.append(loss.item())\n",
    "#     avg_loss = np.mean(avg_loss)\n",
    "#     print(f\"Loss:{avg_loss:4f}\")\n",
    "    \n",
    "#     # evaluate\n",
    "#     model.eval()\n",
    "#     test_word_emb = model.get_embedding(word_embedding_tensor)\n",
    "#     res = evaluate(test_word_emb,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7bc50afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "def evaluate_NDCG(test_word_emb, loader, topk=None):\n",
    "    NDCGs = defaultdict(list)\n",
    "    \n",
    "    for batch in (test_loader):\n",
    "        batch = [item.to(device) for item in batch]\n",
    "        emb, ans = batch\n",
    "        TFIDF_ans = np.zeros((len(ans), test_word_emb.shape[0]))\n",
    "        for i in range(len(ans)):\n",
    "            ans_row = ans[i]\n",
    "            ans_row = ans_row[~ans_row.eq(-1)]\n",
    "            ans_row = ans_row[~ans_row.eq(0)]\n",
    "            for word_id in ans_row:\n",
    "                word_id = word_id.item()\n",
    "                word = dataset.vocab.itos[word_id]\n",
    "                TFIDF_ans[i][word_id] += dataset.vocab.IDF[word]\n",
    "             \n",
    "        emb = model.get_embedding(emb)\n",
    "        scores = -torch.cdist(emb, test_word_emb).cpu().detach().numpy()\n",
    "        true_relevance = TFIDF_ans\n",
    "\n",
    "        NDCGs['top50'].append(ndcg_score(true_relevance, scores, k=50))\n",
    "        NDCGs['top100'].append(ndcg_score(true_relevance, scores, k=100))\n",
    "        NDCGs['top200'].append(ndcg_score(true_relevance, scores, k=200))\n",
    "        NDCGs['ALL'].append(ndcg_score(true_relevance, scores, k=None))\n",
    "    \n",
    "    print('NDCG top50', np.mean(NDCGs['top50']))\n",
    "    print('NDCG top100', np.mean(NDCGs['top100']))\n",
    "    print('NDCG top200', np.mean(NDCGs['top200']))\n",
    "    print('NDCG ALL', np.mean(NDCGs['ALL']))\n",
    "    return NDCGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8f96c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_history = []\n",
    "\n",
    "# for epoch in range(EPOCH):\n",
    "#     avg_loss = []\n",
    "#     model.train()\n",
    "#     for batch in tqdm(train_loader):\n",
    "#         batch = [item.to(device) for item in batch]\n",
    "#         doc_id,pos_w,neg_w = batch\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = loss_fn(*model(doc_id,pos_w,neg_w))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         avg_loss.append(loss.item())\n",
    "#     avg_loss = np.mean(avg_loss)\n",
    "#     print(f\"Loss:{avg_loss:4f}\")\n",
    "    \n",
    "#     # evaluate\n",
    "#     model.eval()\n",
    "#     test_word_emb = model.get_embedding(word_embedding_tensor)\n",
    "#     ndcg_res = evaluate_NDCG(test_word_emb,test_loader)\n",
    "#     validation_history.append(ndcg_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d31a21",
   "metadata": {},
   "source": [
    "## Top K freq word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d1efec49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('life', 3144),\n",
       " ('them', 2970),\n",
       " ('two', 2910),\n",
       " ('perform', 2906),\n",
       " ('thing', 2722),\n",
       " ('come', 2704),\n",
       " ('still', 2694),\n",
       " ('take', 2650),\n",
       " ('dont', 2645),\n",
       " ('then', 2621)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = [(word, freq) for word, freq in dataset.vocab.word_freq_in_corpus.items()]\n",
    "word_freq.sort(key=lambda x:x[1], reverse=True)\n",
    "word_freq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a04793e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33742cd411484c13b6a1723ddcee41cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 50 word\n",
      "percision 0.17685000000000003\n",
      "recall 0.14615038006302417\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b11bcd44db4b858817487bcb4a880d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 100 word\n",
      "percision 0.150655\n",
      "recall 0.24751405829247747\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81bcad290ab43d28a6325827e5e5d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 200 word\n",
      "percision 0.1197475\n",
      "recall 0.39073226702556624\n"
     ]
    }
   ],
   "source": [
    "def topk_word_evaluation(k=50):\n",
    "    topk_word = [word for (word, freq) in word_freq[:k]]\n",
    "\n",
    "    pr, re = [], []\n",
    "    for ans in tqdm(test_ans):\n",
    "        ans = set(ans)\n",
    "        ans = [dataset.vocab.itos[a] for a in ans]\n",
    "\n",
    "        hit = []\n",
    "        for word in ans:\n",
    "            if word in topk_word:\n",
    "                hit.append(word)\n",
    "\n",
    "        precision = len(hit) / k\n",
    "        recall = len(hit) / len(ans)\n",
    "        pr.append(precision)\n",
    "        re.append(recall)\n",
    "\n",
    "    print('top {} word'.format(k))\n",
    "    print('percision', np.mean(pr))\n",
    "    print('recall', np.mean(re))\n",
    "\n",
    "topk_word_evaluation(k=50)\n",
    "topk_word_evaluation(k=100)\n",
    "topk_word_evaluation(k=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "26308789",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4480965d26543bba5a0861f207e1838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 50 NDCG:0.07380315951151092\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65c7f4434014b39bb008724b788d1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 100 NDCG:0.10213112315945727\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c002bf2f04f498e832da8cbe24dc19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 200 NDCG:0.14703990632890476\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde6460829a9482f940f87310ef1f297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top None NDCG:0.38227211688525775\n"
     ]
    }
   ],
   "source": [
    "def topk_word_evaluation_NDCG(k=50):\n",
    "    freq_word =[word for (word, freq) in word_freq]\n",
    "    freq_word_idx = [dataset.vocab.stoi[word] for word in freq_word if word in dataset.vocab.stoi]\n",
    "    \n",
    "    scores = np.zeros(len(dataset.vocab.word_vectors))\n",
    "    for rank, idx in enumerate(freq_word_idx):\n",
    "        scores[idx] = len(dataset.vocab.word_vectors) - rank\n",
    "    \n",
    "    NDCGs = []\n",
    "    \n",
    "    for ans in tqdm(test_ans):\n",
    "        TFIDF_ans = np.zeros(len(dataset.vocab.word_vectors))\n",
    "        \n",
    "        for word_idx in ans:\n",
    "            if word_idx == 0:\n",
    "                continue\n",
    "            word = dataset.vocab.itos[word_idx]\n",
    "            TFIDF_ans[word_idx] += dataset.vocab.IDF[word]\n",
    "\n",
    "        NDCG_score = ndcg_score(TFIDF_ans.reshape(1,-1), scores.reshape(1,-1), k=k)\n",
    "        NDCGs.append(NDCG_score)\n",
    "\n",
    "    print('top {} NDCG:{}'.format(k, np.mean(NDCGs)))\n",
    "\n",
    "topk_word_evaluation_NDCG(k=50)\n",
    "topk_word_evaluation_NDCG(k=100)\n",
    "topk_word_evaluation_NDCG(k=200)\n",
    "topk_word_evaluation_NDCG(k=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4fcb3a",
   "metadata": {},
   "source": [
    "## Linear regression with GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f05d86f",
   "metadata": {},
   "source": [
    "1. train lasso\n",
    "    1. with mse\n",
    "    2. with BCE to D\n",
    "2. train D\n",
    "    1. positive\n",
    "    2. negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "854f9586",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 doc_vectors,\n",
    "                 tfidf_ans,\n",
    "                 doc_ans_w,\n",
    "                 ):\n",
    "        self.doc_vectors = doc_vectors\n",
    "        self.tfidf_ans = tfidf_ans\n",
    "        self.doc_ans_w = doc_ans_w\n",
    "        assert len(doc_vectors) == len(tfidf_ans)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # doc vec, tfidf-ans, doc_id\n",
    "        doc_vec = torch.FloatTensor(self.doc_vectors[idx])\n",
    "        tfidf_ans = torch.FloatTensor(self.tfidf_ans[idx])\n",
    "        doc_ans_w = torch.FloatTensor(self.doc_ans_w[idx])\n",
    "                \n",
    "        return doc_vec, tfidf_ans, idx, doc_ans_w\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0aa07e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lasso(nn.Module):\n",
    "    \"\"\"\n",
    "    Input shape: (N, 3, 64, 64)\n",
    "    Output shape: (N, )\n",
    "    \"\"\"\n",
    "    def __init__(self, num_doc, num_words, L1=0, L2=0):\n",
    "        super(Lasso, self).__init__()\n",
    "        weight = torch.zeros(num_doc, num_words)\n",
    "        self.emb = torch.nn.Embedding.from_pretrained(weight, freeze=False, max_norm=None, norm_type=1)#(num_doc, num_words)\n",
    "        \n",
    "    def forward(self, doc_ids, word_vectors):\n",
    "        return self.emb(doc_ids) @ word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3ad46742",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_words, h_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(num_words, h_dim) \n",
    "        self.fc2 = nn.Linear(h_dim, h_dim)\n",
    "#         self.fc3 = nn.Linear(h_dim, h_dim)\n",
    "        self.fc4 = nn.Linear(h_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "#         x = F.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c135bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_GAN_NDCG(model, train_loader, verbose=1):\n",
    "    results = {}\n",
    "    model.eval()\n",
    "    \n",
    "    scores = np.array(model.emb.weight.data)\n",
    "    true_relevance = train_loader.dataset.tfidf_ans\n",
    "        \n",
    "    results['ndcg@50'] = (ndcg_score(true_relevance, scores, k=50))\n",
    "    results['ndcg@100'] = (ndcg_score(true_relevance, scores, k=100))\n",
    "    results['ndcg@200'] = (ndcg_score(true_relevance, scores, k=200))\n",
    "    results['ndcg@all'] = (ndcg_score(true_relevance, scores, k=None))\n",
    "    \n",
    "    if verbose == 1:\n",
    "        print('NDCG top50', results['ndcg@50'])\n",
    "        print('NDCG top100', results['ndcg@100'])\n",
    "        print('NDCG top200', results['ndcg@200'])\n",
    "        print('NDCG ALL', results['ndcg@all'])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3ac91f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "document_vectors = np.array(dataset.document_vectors)\n",
    "document_answers_w = np.array(dataset.document_answers_w).reshape(-1, 1)\n",
    "\n",
    "len(document_vectors)\n",
    "train_test_split_ratio = 0.005\n",
    "select_num = int(len(document_vectors) * train_test_split_ratio)\n",
    "\n",
    "train_dataset = GANDataset(document_vectors[:select_num], tfidf_ans[:select_num], document_answers_w[:select_num])\n",
    "valid_dataset = GANDataset(document_vectors[select_num:], tfidf_ans[select_num:], document_answers_w[select_num:])\n",
    "\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, pin_memory=True)\n",
    "test_loader  = torch.utils.data.DataLoader(valid_dataset, batch_size=10, shuffle=True, pin_memory=True)\n",
    "\n",
    "print(select_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0ad1ef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_tensor = torch.FloatTensor(word_vectors)\n",
    "word_vectors_tensor.shape\n",
    "\n",
    "test_word_weight_tensor = torch.FloatTensor(tfidf_ans[select_num:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e533d31",
   "metadata": {},
   "source": [
    "## start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d389257f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af4b1b9e36b403fa24671554b02c77a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "# setting\n",
    "lr = 0.02\n",
    "momentum = 0.99\n",
    "weight_decay = 0\n",
    "nesterov = False # True\n",
    "\n",
    "bs = 10\n",
    "n_critic = 1\n",
    "\n",
    "n_epoch = 5000\n",
    "\n",
    "loss_weight = [0, 1, 1e-3]\n",
    "weight_sum = 3\n",
    "weight_sum_mul = 0.8\n",
    "\n",
    "clip_value = 0\n",
    "\n",
    "verbose = 0\n",
    "\n",
    "model = Lasso(num_doc=select_num, num_words=word_vectors.shape[0])\n",
    "D = Discriminator(num_words=word_vectors.shape[0], h_dim=64)\n",
    "\n",
    "model.train()\n",
    "D.train()\n",
    "\n",
    "# opt_G = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "# opt_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "opt_G = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=nesterov)\n",
    "opt_D = torch.optim.SGD(D.parameters(), lr=lr*10)\n",
    "\n",
    "G_criterion = nn.MSELoss(reduction='mean')\n",
    "D_criterion = nn.BCELoss()\n",
    "\n",
    "results = []\n",
    "step = 0\n",
    "for epoch in tqdm(range(n_epoch)):\n",
    "    \n",
    "    D_loss = []\n",
    "    G_loss_D = []\n",
    "    G_loss_MSE = []\n",
    "    \n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        doc_embs, _, doc_ids, doc_ans_w = data\n",
    "\n",
    "        perm = torch.randperm(test_word_weight_tensor.size(0))\n",
    "        true_word_weights_sample = test_word_weight_tensor[perm[:bs]]\n",
    "\n",
    "        # Label\n",
    "        r_label = torch.ones((bs))\n",
    "        f_label = torch.zeros((doc_embs.size(0)))\n",
    "\n",
    "        r_logit = D(true_word_weights_sample.detach())\n",
    "        f_logit = D(model.emb(doc_ids).detach())\n",
    "        \n",
    "        # Compute the loss for the discriminator.\n",
    "        # r_loss = D_criterion(r_logit.squeeze(), r_label)\n",
    "        # f_loss = D_criterion(f_logit.squeeze(), f_label)\n",
    "        # loss_D = (r_loss + f_loss) / 2\n",
    "\n",
    "        # WGAN Loss\n",
    "        loss_D = -torch.mean(r_logit) + torch.mean(f_logit)\n",
    "\n",
    "        # Model backwarding\n",
    "        D.zero_grad()\n",
    "        loss_D.backward()\n",
    "\n",
    "        # Update the discriminator.\n",
    "        opt_D.step()\n",
    "        \n",
    "        D_loss.append(loss_D.item())\n",
    "        \"\"\" Clip weights of discriminator. \"\"\"\n",
    "        for p in D.parameters():\n",
    "            p.data.clamp_(-10, 10)\n",
    "\n",
    "        # ============================================\n",
    "        #  Train G\n",
    "        # ============================================\n",
    "        \n",
    "        if (epoch % n_critic == 0) and (epoch > 500):\n",
    "            # Generate some fake images.\n",
    "            f_imgs = model.emb(doc_ids)\n",
    "            f_logit = D(f_imgs)\n",
    "            \n",
    "            # Compute the loss for the generator.\n",
    "            # loss_G_Dis = D_criterion(f_logit.squeeze(), torch.ones((doc_embs.size(0))))\n",
    "            # WGAN Loss\n",
    "            loss_G_Dis = -torch.mean(f_logit)\n",
    "            \n",
    "            # MSE loss\n",
    "            pred_doc_embs = model(doc_ids, word_vectors_tensor)     \n",
    "            loss_G_MSE = G_criterion(pred_doc_embs, doc_embs)\n",
    "            \n",
    "            emb_w_sum = torch.sum(model.emb(doc_ids), axis=1)\n",
    "            \n",
    "            loss_G_SUM = G_criterion(emb_w_sum, doc_ans_w * weight_sum_mul)\n",
    "#             loss_G_SUM = G_criterion(emb_w_sum, torch.zeros(emb_w_sum.size(0)) + weight_sum)\n",
    "            \n",
    "            loss_G = loss_G_Dis * loss_weight[0] + loss_G_MSE * loss_weight[1] + loss_G_SUM * loss_weight[2]\n",
    "\n",
    "            # Model backwarding\n",
    "            model.zero_grad()\n",
    "            loss_G.backward()\n",
    "\n",
    "            # Update the generator.\n",
    "            opt_G.step()\n",
    "            \n",
    "            G_loss_D.append(loss_G_Dis.item())\n",
    "            G_loss_MSE.append(loss_G_MSE.item())\n",
    "            \n",
    "            \n",
    "            for p in model.parameters():\n",
    "                p.data.clamp_(clip_value, 100)\n",
    "        step += 1\n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "        res = {}\n",
    "        res['epoch'] = epoch\n",
    "        res['loss_D'] = np.mean(D_loss)\n",
    "        res['loss_G_D'] = np.mean(G_loss_D)\n",
    "        res['loss_G_MSE'] = np.mean(G_loss_MSE)\n",
    "\n",
    "        if verbose==1:\n",
    "            print('epoch', res['epoch'])\n",
    "            print('loss D', res['loss_D'])\n",
    "            print('loss G D', res['loss_G_D'])\n",
    "            print('loss G MSE', res['loss_G_MSE'])\n",
    "\n",
    "        res_ndcg = evaluate_GAN_NDCG(model, train_loader, verbose)\n",
    "        res.update(res_ndcg)\n",
    "        results.append(res)\n",
    "        \n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c1c90f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_D</th>\n",
       "      <th>loss_G_D</th>\n",
       "      <th>loss_G_MSE</th>\n",
       "      <th>ndcg@50</th>\n",
       "      <th>ndcg@100</th>\n",
       "      <th>ndcg@200</th>\n",
       "      <th>ndcg@all</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.015324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010262</td>\n",
       "      <td>0.015120</td>\n",
       "      <td>0.024702</td>\n",
       "      <td>0.308172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-0.998056</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010262</td>\n",
       "      <td>0.015120</td>\n",
       "      <td>0.024702</td>\n",
       "      <td>0.308172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>-0.999498</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010262</td>\n",
       "      <td>0.015120</td>\n",
       "      <td>0.024702</td>\n",
       "      <td>0.308172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>-0.996832</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010262</td>\n",
       "      <td>0.015120</td>\n",
       "      <td>0.024702</td>\n",
       "      <td>0.308172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>-0.999763</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010262</td>\n",
       "      <td>0.015120</td>\n",
       "      <td>0.024702</td>\n",
       "      <td>0.308172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>-0.999839</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010262</td>\n",
       "      <td>0.015120</td>\n",
       "      <td>0.024702</td>\n",
       "      <td>0.308172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>-0.999922</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>0.611372</td>\n",
       "      <td>0.360801</td>\n",
       "      <td>0.380051</td>\n",
       "      <td>0.421286</td>\n",
       "      <td>0.605617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>-0.999906</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>0.178665</td>\n",
       "      <td>0.390019</td>\n",
       "      <td>0.408109</td>\n",
       "      <td>0.455092</td>\n",
       "      <td>0.626893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>-0.999852</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>1.042383</td>\n",
       "      <td>0.390289</td>\n",
       "      <td>0.413458</td>\n",
       "      <td>0.454242</td>\n",
       "      <td>0.627531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>-0.999954</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>0.279460</td>\n",
       "      <td>0.425262</td>\n",
       "      <td>0.450033</td>\n",
       "      <td>0.490144</td>\n",
       "      <td>0.652847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>-0.999952</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>0.196548</td>\n",
       "      <td>0.424329</td>\n",
       "      <td>0.446231</td>\n",
       "      <td>0.486333</td>\n",
       "      <td>0.650942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>-0.999963</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>0.189363</td>\n",
       "      <td>0.426810</td>\n",
       "      <td>0.452330</td>\n",
       "      <td>0.494578</td>\n",
       "      <td>0.655196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>-0.999881</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>1.819297</td>\n",
       "      <td>0.423806</td>\n",
       "      <td>0.445388</td>\n",
       "      <td>0.486664</td>\n",
       "      <td>0.649761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>-0.999971</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.319678</td>\n",
       "      <td>0.432935</td>\n",
       "      <td>0.452503</td>\n",
       "      <td>0.491465</td>\n",
       "      <td>0.654858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>-0.999974</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>0.195427</td>\n",
       "      <td>0.431941</td>\n",
       "      <td>0.455984</td>\n",
       "      <td>0.499653</td>\n",
       "      <td>0.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>-0.999968</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.617863</td>\n",
       "      <td>0.432952</td>\n",
       "      <td>0.458833</td>\n",
       "      <td>0.503163</td>\n",
       "      <td>0.660549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>-0.999978</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>0.173317</td>\n",
       "      <td>0.437401</td>\n",
       "      <td>0.460540</td>\n",
       "      <td>0.503854</td>\n",
       "      <td>0.661383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>-0.999979</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0.229799</td>\n",
       "      <td>0.432973</td>\n",
       "      <td>0.454557</td>\n",
       "      <td>0.496822</td>\n",
       "      <td>0.656559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>-0.999976</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>0.542747</td>\n",
       "      <td>0.437669</td>\n",
       "      <td>0.458997</td>\n",
       "      <td>0.499763</td>\n",
       "      <td>0.659038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>-0.999976</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>0.380410</td>\n",
       "      <td>0.435712</td>\n",
       "      <td>0.458558</td>\n",
       "      <td>0.498954</td>\n",
       "      <td>0.658957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>-0.999983</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>0.397223</td>\n",
       "      <td>0.433363</td>\n",
       "      <td>0.456847</td>\n",
       "      <td>0.498291</td>\n",
       "      <td>0.657755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2100</th>\n",
       "      <td>-0.999978</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>0.765048</td>\n",
       "      <td>0.432568</td>\n",
       "      <td>0.455781</td>\n",
       "      <td>0.497241</td>\n",
       "      <td>0.655365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>-0.999981</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>0.495262</td>\n",
       "      <td>0.438741</td>\n",
       "      <td>0.459248</td>\n",
       "      <td>0.501881</td>\n",
       "      <td>0.658630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>-0.999983</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.272624</td>\n",
       "      <td>0.435080</td>\n",
       "      <td>0.455041</td>\n",
       "      <td>0.500292</td>\n",
       "      <td>0.659289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>-0.999972</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.206016</td>\n",
       "      <td>0.439453</td>\n",
       "      <td>0.459548</td>\n",
       "      <td>0.497690</td>\n",
       "      <td>0.658514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.318646</td>\n",
       "      <td>0.443397</td>\n",
       "      <td>0.466820</td>\n",
       "      <td>0.508458</td>\n",
       "      <td>0.663838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>-0.999984</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.065521</td>\n",
       "      <td>0.434935</td>\n",
       "      <td>0.460448</td>\n",
       "      <td>0.503344</td>\n",
       "      <td>0.662135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2700</th>\n",
       "      <td>-0.999984</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.361961</td>\n",
       "      <td>0.435035</td>\n",
       "      <td>0.459038</td>\n",
       "      <td>0.498599</td>\n",
       "      <td>0.659463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2800</th>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.089292</td>\n",
       "      <td>0.440824</td>\n",
       "      <td>0.463073</td>\n",
       "      <td>0.503604</td>\n",
       "      <td>0.661364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.199135</td>\n",
       "      <td>0.429196</td>\n",
       "      <td>0.453630</td>\n",
       "      <td>0.491981</td>\n",
       "      <td>0.653346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>-0.999987</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.228326</td>\n",
       "      <td>0.431482</td>\n",
       "      <td>0.452503</td>\n",
       "      <td>0.490347</td>\n",
       "      <td>0.652815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>-0.999984</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.158199</td>\n",
       "      <td>0.425530</td>\n",
       "      <td>0.447221</td>\n",
       "      <td>0.484502</td>\n",
       "      <td>0.648622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.112759</td>\n",
       "      <td>0.428546</td>\n",
       "      <td>0.454013</td>\n",
       "      <td>0.495796</td>\n",
       "      <td>0.651805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3300</th>\n",
       "      <td>-0.999966</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>1.020118</td>\n",
       "      <td>0.424658</td>\n",
       "      <td>0.446826</td>\n",
       "      <td>0.486078</td>\n",
       "      <td>0.647076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3400</th>\n",
       "      <td>-0.999988</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.988689</td>\n",
       "      <td>0.422224</td>\n",
       "      <td>0.445466</td>\n",
       "      <td>0.482516</td>\n",
       "      <td>0.646396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3500</th>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.139656</td>\n",
       "      <td>0.433123</td>\n",
       "      <td>0.456843</td>\n",
       "      <td>0.493837</td>\n",
       "      <td>0.654053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3600</th>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.104372</td>\n",
       "      <td>0.431418</td>\n",
       "      <td>0.454013</td>\n",
       "      <td>0.491814</td>\n",
       "      <td>0.652569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3700</th>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.071234</td>\n",
       "      <td>0.425594</td>\n",
       "      <td>0.452990</td>\n",
       "      <td>0.492250</td>\n",
       "      <td>0.651265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.317976</td>\n",
       "      <td>0.429663</td>\n",
       "      <td>0.451953</td>\n",
       "      <td>0.490770</td>\n",
       "      <td>0.651762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3900</th>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.652962</td>\n",
       "      <td>0.419101</td>\n",
       "      <td>0.442330</td>\n",
       "      <td>0.475270</td>\n",
       "      <td>0.642313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.303988</td>\n",
       "      <td>0.433085</td>\n",
       "      <td>0.457103</td>\n",
       "      <td>0.492801</td>\n",
       "      <td>0.654097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4100</th>\n",
       "      <td>-0.999985</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.824635</td>\n",
       "      <td>0.435154</td>\n",
       "      <td>0.458027</td>\n",
       "      <td>0.494322</td>\n",
       "      <td>0.654987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4200</th>\n",
       "      <td>-0.999982</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.746764</td>\n",
       "      <td>0.425787</td>\n",
       "      <td>0.449149</td>\n",
       "      <td>0.480023</td>\n",
       "      <td>0.647396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4300</th>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.162172</td>\n",
       "      <td>0.427038</td>\n",
       "      <td>0.450496</td>\n",
       "      <td>0.484948</td>\n",
       "      <td>0.651551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.474559</td>\n",
       "      <td>0.420604</td>\n",
       "      <td>0.437948</td>\n",
       "      <td>0.471546</td>\n",
       "      <td>0.642219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4500</th>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.419571</td>\n",
       "      <td>0.410973</td>\n",
       "      <td>0.435076</td>\n",
       "      <td>0.467287</td>\n",
       "      <td>0.636401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>-0.999968</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.778429</td>\n",
       "      <td>0.425185</td>\n",
       "      <td>0.449661</td>\n",
       "      <td>0.485381</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4700</th>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.211018</td>\n",
       "      <td>0.434489</td>\n",
       "      <td>0.457810</td>\n",
       "      <td>0.493722</td>\n",
       "      <td>0.655599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4800</th>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.102171</td>\n",
       "      <td>0.421547</td>\n",
       "      <td>0.448193</td>\n",
       "      <td>0.483232</td>\n",
       "      <td>0.645801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4900</th>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.096301</td>\n",
       "      <td>0.425314</td>\n",
       "      <td>0.451601</td>\n",
       "      <td>0.484249</td>\n",
       "      <td>0.647825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss_D  loss_G_D  loss_G_MSE   ndcg@50  ndcg@100  ndcg@200  ndcg@all\n",
       "epoch                                                                        \n",
       "0     -0.015324       NaN         NaN  0.010262  0.015120  0.024702  0.308172\n",
       "100   -0.998056       NaN         NaN  0.010262  0.015120  0.024702  0.308172\n",
       "200   -0.999498       NaN         NaN  0.010262  0.015120  0.024702  0.308172\n",
       "300   -0.996832       NaN         NaN  0.010262  0.015120  0.024702  0.308172\n",
       "400   -0.999763       NaN         NaN  0.010262  0.015120  0.024702  0.308172\n",
       "500   -0.999839       NaN         NaN  0.010262  0.015120  0.024702  0.308172\n",
       "600   -0.999922 -0.000077    0.611372  0.360801  0.380051  0.421286  0.605617\n",
       "700   -0.999906 -0.000060    0.178665  0.390019  0.408109  0.455092  0.626893\n",
       "800   -0.999852 -0.000051    1.042383  0.390289  0.413458  0.454242  0.627531\n",
       "900   -0.999954 -0.000039    0.279460  0.425262  0.450033  0.490144  0.652847\n",
       "1000  -0.999952 -0.000038    0.196548  0.424329  0.446231  0.486333  0.650942\n",
       "1100  -0.999963 -0.000034    0.189363  0.426810  0.452330  0.494578  0.655196\n",
       "1200  -0.999881 -0.000117    1.819297  0.423806  0.445388  0.486664  0.649761\n",
       "1300  -0.999971 -0.000029    0.319678  0.432935  0.452503  0.491465  0.654858\n",
       "1400  -0.999974 -0.000026    0.195427  0.431941  0.455984  0.499653  0.658000\n",
       "1500  -0.999968 -0.000030    0.617863  0.432952  0.458833  0.503163  0.660549\n",
       "1600  -0.999978 -0.000022    0.173317  0.437401  0.460540  0.503854  0.661383\n",
       "1700  -0.999979 -0.000021    0.229799  0.432973  0.454557  0.496822  0.656559\n",
       "1800  -0.999976 -0.000022    0.542747  0.437669  0.458997  0.499763  0.659038\n",
       "1900  -0.999976 -0.000019    0.380410  0.435712  0.458558  0.498954  0.658957\n",
       "2000  -0.999983 -0.000017    0.397223  0.433363  0.456847  0.498291  0.657755\n",
       "2100  -0.999978 -0.000022    0.765048  0.432568  0.455781  0.497241  0.655365\n",
       "2200  -0.999981 -0.000019    0.495262  0.438741  0.459248  0.501881  0.658630\n",
       "2300  -0.999983 -0.000012    0.272624  0.435080  0.455041  0.500292  0.659289\n",
       "2400  -0.999972 -0.000012    0.206016  0.439453  0.459548  0.497690  0.658514\n",
       "2500  -0.999989 -0.000011    0.318646  0.443397  0.466820  0.508458  0.663838\n",
       "2600  -0.999984 -0.000010    0.065521  0.434935  0.460448  0.503344  0.662135\n",
       "2700  -0.999984 -0.000010    0.361961  0.435035  0.459038  0.498599  0.659463\n",
       "2800  -0.999989 -0.000009    0.089292  0.440824  0.463073  0.503604  0.661364\n",
       "2900  -0.999989 -0.000009    0.199135  0.429196  0.453630  0.491981  0.653346\n",
       "3000  -0.999987 -0.000009    0.228326  0.431482  0.452503  0.490347  0.652815\n",
       "3100  -0.999984 -0.000009    0.158199  0.425530  0.447221  0.484502  0.648622\n",
       "3200  -0.999990 -0.000008    0.112759  0.428546  0.454013  0.495796  0.651805\n",
       "3300  -0.999966 -0.000016    1.020118  0.424658  0.446826  0.486078  0.647076\n",
       "3400  -0.999988 -0.000011    0.988689  0.422224  0.445466  0.482516  0.646396\n",
       "3500  -0.999992 -0.000008    0.139656  0.433123  0.456843  0.493837  0.654053\n",
       "3600  -0.999992 -0.000008    0.104372  0.431418  0.454013  0.491814  0.652569\n",
       "3700  -0.999993 -0.000007    0.071234  0.425594  0.452990  0.492250  0.651265\n",
       "3800  -0.999992 -0.000008    0.317976  0.429663  0.451953  0.490770  0.651762\n",
       "3900  -0.999992 -0.000008    0.652962  0.419101  0.442330  0.475270  0.642313\n",
       "4000  -0.999989 -0.000007    0.303988  0.433085  0.457103  0.492801  0.654097\n",
       "4100  -0.999985 -0.000010    0.824635  0.435154  0.458027  0.494322  0.654987\n",
       "4200  -0.999982 -0.000009    0.746764  0.425787  0.449149  0.480023  0.647396\n",
       "4300  -0.999992 -0.000006    0.162172  0.427038  0.450496  0.484948  0.651551\n",
       "4400  -0.999993 -0.000007    0.474559  0.420604  0.437948  0.471546  0.642219\n",
       "4500  -0.999994 -0.000006    0.419571  0.410973  0.435076  0.467287  0.636401\n",
       "4600  -0.999968 -0.000032    1.778429  0.425185  0.449661  0.485381  0.650000\n",
       "4700  -0.999994 -0.000006    0.211018  0.434489  0.457810  0.493722  0.655599\n",
       "4800  -0.999994 -0.000006    0.102171  0.421547  0.448193  0.483232  0.645801\n",
       "4900  -0.999993 -0.000006    0.096301  0.425314  0.451601  0.484249  0.647825"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.groupby(by=['epoch']).mean().plot()\n",
    "results_df.groupby(by=['epoch']).mean()\n",
    "# results_df.groupby(by=['epoch']).mean().iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2115b07e",
   "metadata": {},
   "source": [
    "## Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5658c77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select doc_id and k\n",
    "doc_id = 4\n",
    "topk = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31562cf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth\n",
      "\u001b[48;5;3mwarren\u001b[0m \u001b[48;5;3mbrook\u001b[0m \u001b[48;5;3msailor\u001b[0m \u001b[48;5;3mmel\u001b[0m \u001b[48;5;3mflesh\u001b[0m \u001b[48;5;3mslapstick\u001b[0m \u001b[48;5;3mlesli\u001b[0m \u001b[48;5;3mprice\u001b[0m \u001b[48;5;3mann\u001b[0m \u001b[48;5;3mrent\u001b[0m \u001b[48;5;3mroom\u001b[0m \u001b[48;5;3mspeak\u001b[0m \u001b[48;5;3mcut\u001b[0m inherit richard warp process interpret montag ebert roger 3rd spectacular grin pg g x mislead humour couldnt mankind rapid complex spiritu notion 4th rid dean dimension krell list storm struck hors discern stone cum soap detect current \n",
      "\n",
      "prediction\n",
      "\u001b[48;5;3mbrook\u001b[0m \u001b[48;5;3mwarren\u001b[0m \u001b[48;5;3mmel\u001b[0m \u001b[48;5;3mslapstick\u001b[0m \u001b[48;5;3mlesli\u001b[0m \u001b[48;5;3mann\u001b[0m \u001b[48;5;3msailor\u001b[0m \u001b[48;5;3mprice\u001b[0m \u001b[48;5;3mflesh\u001b[0m \u001b[48;5;3mrent\u001b[0m \u001b[48;5;3mcut\u001b[0m pay cabin bedroom jim sit \u001b[48;5;3mroom\u001b[0m bill bacon detail \u001b[48;5;3mspeak\u001b[0m darker wood student merril fat oil craig freed spoke scarlett pound eat lower walk hurt chop heal london beast escapist fiction shop neeson disbelief mood duck dana jack spike "
     ]
    }
   ],
   "source": [
    "import colored\n",
    "from colored import stylize\n",
    "\n",
    "word_list = dataset.vocab.itos\n",
    "\n",
    "gt = [word_list[word_idx] for word_idx in np.argsort(tfidf_ans[doc_id])[::-1][:topk]]\n",
    "pred = [word_list[word_idx] for word_idx in np.argsort(model.emb.weight.data[doc_id].numpy())[::-1][:topk]]\n",
    "\n",
    "print('ground truth')\n",
    "for word in gt:\n",
    "    if word in pred:\n",
    "        print(stylize(word, colored.bg(\"yellow\")), end=' ')\n",
    "    else:\n",
    "        print(word, end=' ')\n",
    "\n",
    "print()\n",
    "print('\\nprediction')\n",
    "for word in pred:\n",
    "    if word in gt:\n",
    "        print(stylize(word, colored.bg(\"yellow\")), end=' ')\n",
    "    else:\n",
    "        print(word, end=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55fa0add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is not the typical mel brooks film .  it was much less slapstick than most of his movies and actually had a plot that was followable .  leslie ann warren made the movie ,  she is such a fantastic ,  under-rated actress .  there were some moments that could have been fleshed out a bit more ,  and some scenes that could probably have been cut to make the room to do so ,  but all in all ,  this is worth the price to rent and see it .  the acting was good overall ,  brooks himself did a good job without his characteristic speaking to directly to the audience .  again ,  warren was the best actor in the movie ,  but  \" fume \"  and  \" sailor \"  both played their parts well . '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.documents[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8fc21f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.9834934040761217\n",
      "NDCG top100 0.9834934040761217\n",
      "NDCG top200 0.9834934040761217\n",
      "NDCG ALL 0.9834934040761217\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "   \n",
    "scores = np.array(model.emb.weight.data)[doc_id].reshape(1, -1)\n",
    "true_relevance = train_loader.dataset.tfidf_ans[doc_id].reshape(1, -1)\n",
    "\n",
    "results['ndcg@50'] = (ndcg_score(true_relevance, scores, k=50))\n",
    "results['ndcg@100'] = (ndcg_score(true_relevance, scores, k=100))\n",
    "results['ndcg@200'] = (ndcg_score(true_relevance, scores, k=200))\n",
    "results['ndcg@all'] = (ndcg_score(true_relevance, scores, k=None))\n",
    "\n",
    "print('NDCG top50', results['ndcg@50'])\n",
    "print('NDCG top100', results['ndcg@100'])\n",
    "print('NDCG top200', results['ndcg@200'])\n",
    "print('NDCG ALL', results['ndcg@all'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc34eb9b",
   "metadata": {},
   "source": [
    "## train seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "233a4732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_GAN_NDCG(w, tfidf_ans, doc_id, verbose=1):\n",
    "#     results = {}\n",
    "    \n",
    "#     scores = w.detach().numpy().reshape(1, -1)\n",
    "#     true_relevance = tfidf_ans[doc_id].reshape(1, -1)\n",
    "\n",
    "    \n",
    "#     results['ndcg@50'] = (ndcg_score(true_relevance, scores, k=50))\n",
    "#     results['ndcg@100'] = (ndcg_score(true_relevance, scores, k=100))\n",
    "#     results['ndcg@200'] = (ndcg_score(true_relevance, scores, k=200))\n",
    "#     results['ndcg@all'] = (ndcg_score(true_relevance, scores, k=None))\n",
    "    \n",
    "#     if verbose == 1:\n",
    "#         print('NDCG top50', results['ndcg@50'])\n",
    "#         print('NDCG top100', results['ndcg@100'])\n",
    "#         print('NDCG top200', results['ndcg@200'])\n",
    "#         print('NDCG ALL', results['ndcg@all'])\n",
    "    \n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9fcf420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 0.001\n",
    "# n_epoch = 1000\n",
    "\n",
    "# n_critic = 1\n",
    "# clip_value = 1\n",
    "# bs = 1\n",
    "# loss_weight = [0, 1]\n",
    "\n",
    "# num_words = word_vectors.shape[0]\n",
    "# results_all = []\n",
    "\n",
    "# for doc_id in tqdm(range(len(document_vectors_tensor[:10]))):\n",
    "#     doc_emb = document_vectors_tensor[doc_id]\n",
    "#     test_word_weight_tensor = torch.FloatTensor(np.concatenate((tfidf_ans[:doc_id], tfidf_ans[doc_id+1:])))\n",
    "\n",
    "#     w = torch.zeros(num_words).requires_grad_(True)\n",
    "#     D = Discriminator(num_words=num_words, h_dim=64)\n",
    "#     D.train()\n",
    "\n",
    "#     # opt_G = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "#     # opt_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "#     opt_G = torch.optim.SGD([w], lr=lr)\n",
    "#     opt_D = torch.optim.SGD(D.parameters(), lr=lr)\n",
    "\n",
    "#     G_criterion = nn.MSELoss(reduction='mean')\n",
    "#     D_criterion = nn.BCELoss()\n",
    "\n",
    "#     results = []\n",
    "#     for epoch in range(n_epoch):\n",
    "\n",
    "#         D_loss = []\n",
    "#         G_loss_D = []\n",
    "#         G_loss_MSE = []\n",
    "\n",
    "#         perm = torch.randperm(test_word_weight_tensor.size(0))\n",
    "#         true_word_weights_sample = test_word_weight_tensor[perm[:bs]]\n",
    "#         \"\"\" Medium: Use WGAN Loss. \"\"\"\n",
    "#         # Label\n",
    "#         r_label = torch.ones((bs))\n",
    "#         f_label = torch.zeros((bs))\n",
    "\n",
    "#         # Model forwarding\n",
    "#         r_logit = D(true_word_weights_sample.detach())\n",
    "#         f_logit = D(w.detach())\n",
    "\n",
    "#         # Compute the loss for the discriminator.\n",
    "#         # r_loss = D_criterion(r_logit.squeeze(), r_label)\n",
    "#         # f_loss = D_criterion(f_logit.squeeze(), f_label)\n",
    "#         # loss_D = (r_loss + f_loss) / 2\n",
    "\n",
    "#         # WGAN Loss\n",
    "#         loss_D = -torch.mean(r_logit) + torch.mean(f_logit)\n",
    "\n",
    "#         # Model backwarding\n",
    "#         D.zero_grad()\n",
    "#         loss_D.backward()\n",
    "\n",
    "#         # Update the discriminator.\n",
    "#         opt_D.step()\n",
    "\n",
    "#         D_loss.append(loss_D.item())\n",
    "#         \"\"\" Medium: Clip weights of discriminator. \"\"\"\n",
    "#         for p in D.parameters():\n",
    "#             p.data.clamp_(-clip_value, clip_value)\n",
    "\n",
    "#         # ============================================\n",
    "#         #  Train G\n",
    "#         # ============================================\n",
    "#         if epoch % n_critic == 0:\n",
    "#             # Generate some fake images.\n",
    "#             f_logit = D(w)\n",
    "\n",
    "#             \"\"\" Medium: Use WGAN Loss\"\"\"\n",
    "#             # Compute the loss for the generator.\n",
    "#             # loss_G_Dis = D_criterion(f_logit.squeeze(), r_label)\n",
    "#             # WGAN Loss\n",
    "#             loss_G_Dis = -torch.mean(f_logit)\n",
    "\n",
    "#             # MSE loss\n",
    "#             pred_doc_embs = w @ word_vectors_tensor\n",
    "#             loss_G_MSE = G_criterion(pred_doc_embs, doc_emb)\n",
    "\n",
    "#             loss_G = loss_G_Dis * loss_weight[0] + loss_G_MSE * loss_weight[1]\n",
    "\n",
    "#             # Model backwarding\n",
    "#             if w.grad is not None:\n",
    "#                 w.grad.zero_()\n",
    "            \n",
    "#             loss_G.backward()\n",
    "            \n",
    "#             # Update the generator.\n",
    "#             opt_G.step()\n",
    "\n",
    "#             G_loss_D.append(loss_G_Dis.item())\n",
    "#             G_loss_MSE.append(loss_G_MSE.item())\n",
    "        \n",
    "#         if epoch % 20 == 0:\n",
    "#             res = {}\n",
    "#             res['epoch'] = epoch\n",
    "#             res['loss_D'] = np.mean(D_loss)\n",
    "#             res['loss_G_D'] = np.mean(G_loss_D)\n",
    "#             res['loss_G_MSE'] = np.mean(G_loss_MSE)\n",
    "            \n",
    "#             print('epoch', epoch)\n",
    "#             print('loss D', np.mean(D_loss))\n",
    "#             print('loss G D', np.mean(G_loss_D))\n",
    "#             print('loss G MSE', np.mean(G_loss_MSE))\n",
    "            \n",
    "#             res_ndcg = evaluate_GAN_NDCG(w, tfidf_ans, doc_id)\n",
    "#             res.update(res_ndcg)\n",
    "#             results.append(res)\n",
    "\n",
    "#     results_all.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40069345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = []\n",
    "# for r in results_all:\n",
    "#     r = pd.DataFrame(r)\n",
    "#     results_df.append(r)\n",
    "\n",
    "# results_df = pd.concat(results_df)\n",
    "# results_df.groupby(by=['epoch']).mean().plot()\n",
    "# results_df.groupby(by=['epoch']).mean()\n",
    "# # results_df.groupby(by=['epoch']).mean().iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc86d3",
   "metadata": {},
   "source": [
    "## DNN\n",
    "1. input: doc vec / output: tfidf weight\n",
    "2. eval: ndcg\n",
    "3. model: dnn, rank loss or mse loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f3b7c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 doc_vectors,\n",
    "                 tfidf_ans,\n",
    "                 ):\n",
    "        self.doc_vectors = doc_vectors\n",
    "        self.tfidf_ans = tfidf_ans\n",
    "        assert len(doc_vectors) == len(tfidf_ans)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # doc vec, tfidf-ans, doc_id\n",
    "        doc_vec = torch.FloatTensor(self.doc_vectors[idx])\n",
    "        tfidf_ans = torch.FloatTensor(self.tfidf_ans[idx])\n",
    "        \n",
    "        return doc_vec, tfidf_ans\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a7597c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vectors = np.array(dataset.document_vectors)\n",
    "\n",
    "len(document_vectors)\n",
    "train_test_split_ratio = 0.8\n",
    "select_num = int(len(document_vectors) * train_test_split_ratio)\n",
    "\n",
    "train_dataset = DNNDataset(document_vectors[:select_num], tfidf_ans[:select_num])\n",
    "valid_dataset = DNNDataset(document_vectors[select_num:], tfidf_ans[select_num:])\n",
    "\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True, pin_memory=True)\n",
    "test_loader  = torch.utils.data.DataLoader(valid_dataset, batch_size=100, shuffle=True, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a64515bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, doc_emb_shape, num_words, h_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(doc_emb_shape, h_dim) \n",
    "        self.fc2 = nn.Linear(h_dim, h_dim)\n",
    "        self.fc3 = nn.Linear(h_dim, num_words)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3faa1090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_DNN(test_loader, model):\n",
    "\n",
    "    results = {}\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = []\n",
    "    scores = []\n",
    "    true_relevance = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            doc_embs, target = data\n",
    "            predicted = model(doc_embs)  \n",
    "            loss = criterion(predicted, target)  # compare the predicted labels with ground-truth labels\n",
    "            total_loss.append(loss.item())\n",
    "            scores.append(predicted.detach().numpy())\n",
    "            true_relevance.append(target.detach().numpy())\n",
    "    \n",
    "    scores = np.concatenate(scores)\n",
    "    true_relevance = np.concatenate(true_relevance)\n",
    "    \n",
    "    results['ndcg@50'] = (ndcg_score(true_relevance, scores, k=50))\n",
    "    results['ndcg@100'] = (ndcg_score(true_relevance, scores, k=100))\n",
    "    results['ndcg@200'] = (ndcg_score(true_relevance, scores, k=200))\n",
    "    results['ndcg@all'] = (ndcg_score(true_relevance, scores, k=None))\n",
    "    results['mse'] = np.mean(total_loss)\n",
    "    \n",
    "    print('Valid loss:',results['mse'])\n",
    "    print('NDCG top50', results['ndcg@50'])\n",
    "    print('NDCG top100', results['ndcg@100'])\n",
    "    print('NDCG top200', results['ndcg@200'])\n",
    "    print('NDCG ALL', results['ndcg@all'])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac6f422f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lr = 0.001\n",
    "# h_dim = 64\n",
    "# model = DNN(document_vectors.shape[1], tfidf_ans.shape[1], h_dim)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)  # create a Adam optimizer\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# epochs = 100\n",
    "# results = []\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     total_loss = []\n",
    "#     for data in train_loader:\n",
    "#         doc_embs, target = data\n",
    "#         # training process\n",
    "#         optimizer.zero_grad()    \n",
    "#         predicted = model(doc_embs)  \n",
    "#         loss = criterion(predicted, target)  # compare the predicted labels with ground-truth labels\n",
    "#         loss.backward()      # compute the gradient\n",
    "#         optimizer.step()     # optimize the network\n",
    "#         total_loss.append(loss.item())\n",
    "        \n",
    "#     print(f'epoch:{epoch}')\n",
    "#     print(f'Training loss:{np.mean(total_loss)}')\n",
    "    \n",
    "#     res = evaluate_DNN(test_loader, model)\n",
    "#     results.append(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "abc01c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = pd.DataFrame(results)\n",
    "# results.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619c8c3f",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58ff2876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select #answer largest pred\n",
    "def metric4(binary_x, answer, w_idx=None, topk=50, verbose=0):\n",
    "    select_num = topk\n",
    "    answer = list(set(answer))\n",
    "    \n",
    "    if w_idx is not None:\n",
    "        pred = w_idx[np.argsort(binary_x)[-select_num:]]\n",
    "    else:\n",
    "        pred = np.arange(len(binary_x))[np.argsort(binary_x)[-select_num:]]\n",
    "    \n",
    "    hit = np.intersect1d(pred, answer)\n",
    "    hit_num = len(hit)\n",
    "    recall = hit_num / len(answer)\n",
    "    precision = hit_num / len(pred)\n",
    "    if verbose == 1:\n",
    "        print('answer:', word_list[answer])\n",
    "        print('hit:', word_list[hit])\n",
    "    return {\"recall\": recall, \"precision\": precision}\n",
    "\n",
    "def metric_ndcg(binary_x, answer, topk=50, verbose=0):\n",
    "\n",
    "    TFIDF_ans = np.zeros(len(binary_x))\n",
    "    for word_idx in answer:\n",
    "        if word_idx == 0:\n",
    "            continue\n",
    "        word = dataset.vocab.itos[word_idx]\n",
    "        TFIDF_ans[word_idx] += dataset.vocab.IDF[word]\n",
    "    NDCG_score = ndcg_score(TFIDF_ans.reshape(1,-1), binary_x.reshape(1,-1), k=topk)\n",
    "    \n",
    "    if verbose == 1:\n",
    "        print('NDCG_score:', NDCG_score)\n",
    "    return NDCG_score\n",
    "\n",
    "def metric_ndcg2(binary_x, tfidf_ans, doc_id, topk=50, verbose=0):\n",
    "    NDCG_score = ndcg_score(tfidf_ans[doc_id].reshape(1,-1), binary_x.reshape(1,-1), k=topk)\n",
    "    \n",
    "    if verbose == 1:\n",
    "        print('NDCG_score:', NDCG_score)\n",
    "    return NDCG_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e384e745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "class PyTorchLinearRegression:\n",
    "    ''' Class that implemnets Multiple Linear Regression with PyTorch'''\n",
    "    def __init__(self, num_of_features, lr, constraintHigh, constraintLow, total, init_type=0, L1=0, L2=0):\n",
    "        if init_type == 0:\n",
    "            self.w = torch.zeros(num_of_features, requires_grad=True)\n",
    "        elif init_type == 1:\n",
    "            self.w = torch.ones(num_of_features, requires_grad=True)\n",
    "        elif init_type == 2:  \n",
    "            self.w = torch.rand(num_of_features, requires_grad=True)\n",
    "        elif init_type == 3:\n",
    "            self.w = -torch.ones(num_of_features, requires_grad=True)\n",
    "\n",
    "        self.learning_rate = lr\n",
    "        self.high = constraintHigh\n",
    "        self.low = constraintLow\n",
    "        self.total = total\n",
    "        self.rg2 = total / num_of_features\n",
    "        self.L1 = L1\n",
    "        self.L2 = L2\n",
    "        \n",
    "    def _model(self, X):\n",
    "        return X @ self.w.t()# + self.b\n",
    "    \n",
    "    def _mse(self, pred, real):\n",
    "        difference = pred - real\n",
    "        return torch.sum(difference * difference) / difference.numel()\n",
    "    \n",
    "    def _regularization_weightdist(self):\n",
    "        difference = self.w - 1\n",
    "        return -torch.sum(difference * difference) / difference.numel()\n",
    "    \n",
    "    def _regularization_weightsum(self):\n",
    "        difference = torch.sum(self.w) - self.total\n",
    "        return difference * difference / self.w.numel()\n",
    "    \n",
    "    def fit(self, X, y, epochs):\n",
    "        print(loss_weight)\n",
    "        \n",
    "        X = torch.from_numpy(X).float()\n",
    "        y = torch.from_numpy(y).float()\n",
    "\n",
    "        for i in range(epochs):\n",
    "            predictions = self._model(X)\n",
    "            loss1 = self._mse(predictions, y)\n",
    "            loss2 = self._regularization_weightdist()\n",
    "            loss3 = self._regularization_weightsum()\n",
    "            loss = loss1 * loss_weight[0] + loss2 * loss_weight[1] + loss3 * loss_weight[2]\n",
    "\n",
    "            if (i % (epochs//20)) == 0:\n",
    "                print(f'Epoch: {i} - Loss: {loss1}')\n",
    "            \n",
    "            if self.w.grad is not None:\n",
    "                self.w.grad.zero_()\n",
    "            \n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                self.w -= (self.w.grad) * self.learning_rate + torch.sign(self.w)*self.L1 + self.w*self.L2\n",
    "                self.w.data.clamp_(min=self.low, max=self.high)\n",
    "#                 self.w.grad.zero_()\n",
    "#             x = 100\n",
    "#             if i % x == x-1:\n",
    "# #                 self.w=torch.tensor(self.low + (self.high-self.low)*(self.w - torch.min(self.w))/(torch.max(self.w) - torch.min(self.w)), requires_grad=True)\n",
    "#                 self.w.data.clamp_(min=self.low, max=self.high)\n",
    "#                 pass\n",
    "                \n",
    "    def predict(self, X):\n",
    "        X = torch.from_numpy(X).float()\n",
    "        return self._model(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        X = torch.from_numpy(X).float()\n",
    "        y_pred = self._model(X).detach().numpy()\n",
    "        return r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cfe1c89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4127, 100)\n",
      "(9993, 100)\n",
      "9993\n"
     ]
    }
   ],
   "source": [
    "word_embs = np.array(dataset.vocab.word_vectors)\n",
    "doc_embs = np.array(dataset.document_vectors)\n",
    "doc_answers = dataset.document_answers\n",
    "word_list = dataset.vocab.itos\n",
    "\n",
    "print(word_embs.shape)\n",
    "print(doc_embs.shape)\n",
    "print(len(doc_answers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f4b48c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pr, re = [[],[],[]], [[],[],[]]\n",
    "# ndcgs = defaultdict(list)\n",
    "\n",
    "# lr = 0.003\n",
    "# epochs = 10000\n",
    "# constraintHigh=1\n",
    "# constraintLow=0\n",
    "# # constraintHigh=float('inf')\n",
    "# # constraintLow=-float('inf')\n",
    "# loss_weight = [1, 0, 1]\n",
    "# L1, L2 = 0, 0\n",
    "# rand_type = 0\n",
    "\n",
    "# total_mul = 1\n",
    "\n",
    "# for uid, uemb in enumerate(tqdm(doc_embs[:50])):\n",
    "#     x = word_embs.T\n",
    "#     y = uemb\n",
    "#     total = len(doc_answers[uid])\n",
    "#     total = 1\n",
    "    \n",
    "#     torch_model = PyTorchLinearRegression(x.shape[1], lr, constraintHigh, constraintLow, int(total*total_mul), rand_type, L1, L2)\n",
    "#     torch_model.fit(x, y, epochs)\n",
    "    \n",
    "#     m1 = metric4(torch_model.w.detach().numpy(), doc_answers[uid], w_idx=None, topk=50, verbose=0)\n",
    "#     m2 = metric4(torch_model.w.detach().numpy(), doc_answers[uid], w_idx=None, topk=100, verbose=0)\n",
    "#     m3 = metric4(torch_model.w.detach().numpy(), doc_answers[uid], w_idx=None, topk=200, verbose=0)\n",
    "#     ndcg1 = metric_ndcg2(torch_model.w.detach().numpy(), tfidf_ans, uid, topk=50, verbose=0)\n",
    "#     ndcg2 = metric_ndcg2(torch_model.w.detach().numpy(), tfidf_ans, uid, topk=100, verbose=0)\n",
    "#     ndcg3 = metric_ndcg2(torch_model.w.detach().numpy(), tfidf_ans, uid, topk=200, verbose=0)\n",
    "#     ndcg4 = metric_ndcg2(torch_model.w.detach().numpy(), tfidf_ans, uid, topk=None, verbose=0)\n",
    "#     pr[0].append(m1[\"precision\"])\n",
    "#     re[0].append(m1[\"recall\"])\n",
    "#     pr[1].append(m2[\"precision\"])\n",
    "#     re[1].append(m2[\"recall\"])\n",
    "#     pr[2].append(m3[\"precision\"])\n",
    "#     re[2].append(m3[\"recall\"])\n",
    "    \n",
    "#     ndcgs[\"50\"].append(ndcg1)\n",
    "#     ndcgs[\"100\"].append(ndcg2)\n",
    "#     ndcgs[\"200\"].append(ndcg3)\n",
    "#     ndcgs[\"-1\"].append(ndcg4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52ece4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Precision:{np.mean(pr[0]):.4f} Recall:{np.mean(re[0]):.4f}\")\n",
    "# print(f\"Precision:{np.mean(pr[1]):.4f} Recall:{np.mean(re[1]):.4f}\")\n",
    "# print(f\"Precision:{np.mean(pr[2]):.4f} Recall:{np.mean(re[2]):.4f}\")\n",
    "# print(f\"NDCG 50:{np.mean(ndcgs['50']):.4f}\")\n",
    "# print(f\"NDCG 100:{np.mean(ndcgs['100']):.4f}\")\n",
    "# print(f\"NDCG 200:{np.mean(ndcgs['200']):.4f}\")\n",
    "# print(f\"NDCG all:{np.mean(ndcgs['-1']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3827045d",
   "metadata": {},
   "source": [
    "## Nearest Guessing, so bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0bd99ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embs_IDF = word_embs.copy()\n",
    "\n",
    "for word, IDF in dataset.vocab.IDF.items():\n",
    "    word_idx = dataset.vocab.stoi[word]\n",
    "    word_embs_IDF[word_idx] *= IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1db6d370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383f02d90e2f4fca80f25b4be9ff7e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pr, re = [[],[],[]], [[],[],[]]\n",
    "ndcgs = defaultdict(list)\n",
    "\n",
    "for uid, uemb in enumerate(tqdm(doc_embs[:100])):\n",
    "    y = uemb\n",
    "    word_embs_IDF\n",
    "    word_weight = np.dot(word_embs_IDF, y)\n",
    "    \n",
    "    m1 = metric4(word_weight, doc_answers[uid], w_idx=None, topk=50, verbose=0)\n",
    "    m2 = metric4(word_weight, doc_answers[uid], w_idx=None, topk=100, verbose=0)\n",
    "    m3 = metric4(word_weight, doc_answers[uid], w_idx=None, topk=200, verbose=0)\n",
    "    ndcg1 = metric_ndcg(word_weight, doc_answers[uid], topk=50, verbose=0)\n",
    "    ndcg2 = metric_ndcg(word_weight, doc_answers[uid], topk=100, verbose=0)\n",
    "    ndcg3 = metric_ndcg(word_weight, doc_answers[uid], topk=200, verbose=0)\n",
    "    ndcg4 = metric_ndcg(word_weight, doc_answers[uid], topk=None, verbose=0)\n",
    "    pr[0].append(m1[\"precision\"])\n",
    "    re[0].append(m1[\"recall\"])\n",
    "    pr[1].append(m2[\"precision\"])\n",
    "    re[1].append(m2[\"recall\"])\n",
    "    pr[2].append(m3[\"precision\"])\n",
    "    re[2].append(m3[\"recall\"])\n",
    "    \n",
    "    ndcgs[\"50\"].append(ndcg1)\n",
    "    ndcgs[\"100\"].append(ndcg2)\n",
    "    ndcgs[\"200\"].append(ndcg3)\n",
    "    ndcgs[\"-1\"].append(ndcg4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "71d4d93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.0380 Recall:0.1146\n",
      "Precision:0.0303 Recall:0.1675\n",
      "Precision:0.0250 Recall:0.2413\n",
      "NDCG 50:0.1190\n",
      "NDCG 100:0.1403\n",
      "NDCG 200:0.1673\n",
      "NDCG all:0.3492\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision:{np.mean(pr[0]):.4f} Recall:{np.mean(re[0]):.4f}\")\n",
    "print(f\"Precision:{np.mean(pr[1]):.4f} Recall:{np.mean(re[1]):.4f}\")\n",
    "print(f\"Precision:{np.mean(pr[2]):.4f} Recall:{np.mean(re[2]):.4f}\")\n",
    "print(f\"NDCG 50:{np.mean(ndcgs['50']):.4f}\")\n",
    "print(f\"NDCG 100:{np.mean(ndcgs['100']):.4f}\")\n",
    "print(f\"NDCG 200:{np.mean(ndcgs['200']):.4f}\")\n",
    "print(f\"NDCG all:{np.mean(ndcgs['-1']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "56a9e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## test dcg\n",
    "# from sklearn.metrics import ndcg_score, dcg_score\n",
    "# k=2\n",
    "\n",
    "# true_relevance = np.asarray([[1, 2, 3, 4]])\n",
    "# scores = np.asarray([[1, 2, 3, 2.5]])\n",
    "# print('dcg',dcg_score(true_relevance, scores,k=k))\n",
    "# print('ndcg',ndcg_score(true_relevance, scores,k=k))\n",
    "\n",
    "\n",
    "# w = 1 / (np.log(np.arange(true_relevance.shape[1])[:k] + 2) / np.log(2))\n",
    "# dcg = true_relevance[0][np.argsort(scores)[0][::-1][:k]].dot(w)\n",
    "# print(dcg)\n",
    "\n",
    "# idcg = np.sort(true_relevance[0])[::-1][:k].dot(w)\n",
    "# print(dcg/idcg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
