{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "792dad29",
   "metadata": {},
   "source": [
    "### dataset\n",
    "1. IMDB\n",
    "2. CNNNews\n",
    "3. [PubMed](https://github.com/LIAAD/KeywordExtractor-Datasets/blob/master/datasets/PubMed.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b007799a",
   "metadata": {},
   "source": [
    "### preprocess\n",
    "1. filter too frequent and less frequent words\n",
    "2. stemming\n",
    "3. document vector aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9967b",
   "metadata": {},
   "source": [
    "### evaluation\n",
    "1. F1\n",
    "2. NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9beff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/chrisliu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from itertools import cycle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Used to get the data\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "seed = 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2341254",
   "metadata": {},
   "source": [
    "## Preprocess config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dc01df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "config[\"dataset\"] = \"CNN\" # \"IMDB\" \"CNN\", \"PubMed\"\n",
    "config[\"n_document\"] = 100\n",
    "config[\"normalize_word_embedding\"] = False\n",
    "config[\"min_word_freq_threshold\"] = 20\n",
    "config[\"topk_word_freq_threshold\"] = 100\n",
    "config[\"document_vector_agg_weight\"] = 'IDF' # ['mean', 'IDF', 'uniform', 'gaussian', 'exponential', 'pmi']\n",
    "config[\"document_vector_weight_normalize\"] = False # weighted sum or mean, True for mean, False for sum \n",
    "config[\"select_topk_TFIDF\"] = None # ignore\n",
    "config[\"embedding_file\"] = \"../data/wordvectors.txt\"#\"../data/fasttext.en.100d.txt\"#\"../data/glove.6B.100d.txt\"\n",
    "# config[\"embedding_file\"] = \"../data/400d.txt\"\n",
    "config[\"topk\"] = [10, 30, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05b0b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_notebook():\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        if 'IPKernelApp' not in get_ipython().config:  # pragma: no cover\n",
    "            return False\n",
    "    except ImportError:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fc91521",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-dbb0a5f55303>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword2embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mword2embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_word2emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"embedding_file\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-dbb0a5f55303>\u001b[0m in \u001b[0;36mload_word2emb\u001b[0;34m(embedding_file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_word2emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mword2embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mword_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\".(\\d+)d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def load_word2emb(embedding_file):\n",
    "    word2embedding = dict()\n",
    "    word_dim = int(re.findall(r\".(\\d+)d\", embedding_file)[0])\n",
    "\n",
    "    with open(embedding_file, \"r\") as f:\n",
    "        for line in tqdm(f):\n",
    "            line = line.strip().split()\n",
    "            word = line[0]\n",
    "            embedding = list(map(float, line[1:]))\n",
    "            if len(embedding) != word_dim:\n",
    "                print('error', word)\n",
    "                continue\n",
    "            word2embedding[word] = np.array(embedding)\n",
    "\n",
    "    print(\"Number of words:%d\" % len(word2embedding))\n",
    "\n",
    "    return word2embedding\n",
    "\n",
    "word2embedding = load_word2emb(config[\"embedding_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bfc175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_wordemb(word2embedding):\n",
    "    # Every word emb should have norm 1\n",
    "    \n",
    "    word_emb = []\n",
    "    word_list = []\n",
    "    for word, emb in word2embedding.items():\n",
    "        word_list.append(word)\n",
    "        word_emb.append(emb)\n",
    "\n",
    "    word_emb = np.array(word_emb)\n",
    "\n",
    "    for i in range(len(word_emb)):\n",
    "        norm = np.linalg.norm(word_emb[i])\n",
    "        word_emb[i] = word_emb[i] / norm\n",
    "\n",
    "    for word, emb in tqdm(zip(word_list, word_emb)):\n",
    "        word2embedding[word] = emb\n",
    "    return word2embedding\n",
    "\n",
    "if config[\"normalize_word_embedding\"]:\n",
    "    normalize_wordemb(word2embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1233053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, word2embedding, config):\n",
    "        # The low frequency words will be assigned as <UNK> token\n",
    "        self.itos = {0: \"<UNK>\"}\n",
    "        self.stoi = {\"<UNK>\": 0}\n",
    "        \n",
    "        self.word2embedding = word2embedding\n",
    "        self.config = config\n",
    "\n",
    "        self.word_freq_in_corpus = defaultdict(int)\n",
    "        self.IDF = {}\n",
    "        self.ps = PorterStemmer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        self.word_dim = len(word2embedding['the'])\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def tokenizer_eng(self, text):\n",
    "        text = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n",
    "        text = text.strip().split()\n",
    "        \n",
    "        return [self.ps.stem(w) for w in text if w.lower() not in self.stop_words]\n",
    "    \n",
    "    def read_raw(self):        \n",
    "        if self.config[\"dataset\"] == 'IMDB':\n",
    "            data_file_path = '../data/IMDB.txt'\n",
    "        elif self.config[\"dataset\"] == 'CNN':\n",
    "            data_file_path = '../data/CNN.txt'\n",
    "        elif self.config[\"dataset\"] == 'PubMed':\n",
    "            data_file_path = '../data/PubMed.txt'\n",
    "        \n",
    "        # raw documents\n",
    "        self.raw_documents = []\n",
    "        with open(data_file_path,'r',encoding='utf-8') as f:\n",
    "            for line in tqdm(f, desc=\"Loading documents\"):\n",
    "                self.raw_documents.append(line.strip(\"\\n\"))\n",
    "                \n",
    "        return self.raw_documents\n",
    "    \n",
    "    def build_vocabulary(self):\n",
    "        sentence_list = self.raw_documents\n",
    "        \n",
    "        self.doc_freq = defaultdict(int) # # of document a word appear\n",
    "        self.document_num = len(sentence_list)\n",
    "        self.word_vectors = [[0]*self.word_dim] # unknown word emb\n",
    "        \n",
    "        for sentence in tqdm(sentence_list, desc=\"Preprocessing documents\"):\n",
    "            # for doc_freq\n",
    "            document_words = set()\n",
    "            \n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                # pass unknown word\n",
    "                if word not in self.word2embedding:\n",
    "                    continue\n",
    "                    \n",
    "                # calculate word freq\n",
    "                self.word_freq_in_corpus[word] += 1\n",
    "                document_words.add(word)\n",
    "                \n",
    "            for word in document_words:\n",
    "                self.doc_freq[word] += 1\n",
    "        \n",
    "        # calculate IDF\n",
    "        print('doc num', self.document_num)\n",
    "        for word, freq in self.doc_freq.items():\n",
    "            self.IDF[word] = math.log(self.document_num / (freq+1))\n",
    "        \n",
    "        # delete less freq words:\n",
    "        delete_words = []\n",
    "        for word, v in self.word_freq_in_corpus.items():\n",
    "            if v < self.config[\"min_word_freq_threshold\"]:\n",
    "                delete_words.append(word)     \n",
    "        for word in delete_words:\n",
    "            del self.IDF[word]    \n",
    "            del self.word_freq_in_corpus[word]    \n",
    "        \n",
    "        # delete too freq words\n",
    "        print('eliminate freq words')\n",
    "        IDF = [(word, freq) for word, freq in self.IDF.items()]\n",
    "        IDF.sort(key=lambda x: x[1])\n",
    "\n",
    "        for i in range(self.config[\"topk_word_freq_threshold\"]):\n",
    "            print(word)\n",
    "            word = IDF[i][0]\n",
    "            del self.IDF[word]\n",
    "            del self.word_freq_in_corpus[word]\n",
    "        \n",
    "        # construct word_vectors\n",
    "        idx = 1\n",
    "        for word in self.word_freq_in_corpus:\n",
    "            self.word_vectors.append(self.word2embedding[word])\n",
    "            self.stoi[word] = idx\n",
    "            self.itos[idx] = word\n",
    "            idx += 1\n",
    "            \n",
    "    def init_word_weight(self,sentence_list, agg):\n",
    "        if agg == 'mean':\n",
    "            self.word_weight = {word: 1 for word in self.IDF.keys()}\n",
    "        elif agg == 'IDF':\n",
    "            self.word_weight = self.IDF\n",
    "        elif agg == 'uniform':\n",
    "            self.word_weight = {word: np.random.uniform(low=0.0, high=1.0) for word in self.IDF.keys()}\n",
    "        elif agg == 'gaussian':\n",
    "            mu, sigma = 10, 1 # mean and standard deviation\n",
    "            self.word_weight = {word: np.random.normal(mu, sigma) for word in self.IDF.keys()}\n",
    "        elif agg == 'exponential':\n",
    "            self.word_weight = {word: np.random.exponential(scale=1.0) for word in self.IDF.keys()}\n",
    "        elif agg == 'pmi':\n",
    "            trigram_measures = BigramAssocMeasures()\n",
    "            self.word_weight = defaultdict(int)\n",
    "            corpus = []\n",
    "\n",
    "            for text in tqdm(sentence_list):\n",
    "                corpus.extend(text.split())\n",
    "\n",
    "            finder = BigramCollocationFinder.from_words(corpus)\n",
    "            for pmi_score in finder.score_ngrams(trigram_measures.pmi):\n",
    "                pair, score = pmi_score\n",
    "                self.word_weight[pair[0]] += score\n",
    "                self.word_weight[pair[1]] += score\n",
    "                \n",
    "    def calculate_document_vector(self):\n",
    "        # Return\n",
    "        # document_vectors: weighted sum of word emb\n",
    "        # document_answers_idx: doc to word index list\n",
    "        # document_answers_wsum: word weight summation, e.g. total TFIDF score of a doc\n",
    "        \n",
    "        document_vectors = [] \n",
    "        document_answers = []\n",
    "        document_answers_wsum = []\n",
    "        \n",
    "        sentence_list = self.raw_documents\n",
    "        agg = self.config[\"document_vector_agg_weight\"]\n",
    "        n_document = self.config[\"n_document\"]\n",
    "        select_topk_TFIDF = self.config[\"select_topk_TFIDF\"]\n",
    "        \n",
    "        self.init_word_weight(sentence_list, agg)\n",
    "        for sentence in tqdm(sentence_list[:min(n_document, len(sentence_list))], desc=\"calculate document vectors\"):\n",
    "            document_vector = np.zeros(len(self.word_vectors[0]))\n",
    "            select_words = []\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                # pass unknown word\n",
    "                if word not in self.stoi:\n",
    "                    continue\n",
    "                else:\n",
    "                    select_words.append(word)\n",
    "\n",
    "            # select topk TDIDF\n",
    "            if select_topk_TFIDF is not None:\n",
    "                doc_TFIDF = defaultdict(float)\n",
    "                for word in select_words:    \n",
    "                    doc_TFIDF[word] += self.IDF[word]\n",
    "\n",
    "                doc_TFIDF_l = [(word, TFIDF) for word, TFIDF in doc_TFIDF.items()]\n",
    "                doc_TFIDF_l.sort(key=lambda x:x[1], reverse=True)\n",
    "                \n",
    "                select_topk_words = set(list(map(lambda x:x[0], doc_TFIDF_l[:select_topk_TFIDF])))\n",
    "                select_words = [word for word in select_words if word in select_topk_words]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            total_weight = 0\n",
    "            # aggregate to doc vectors\n",
    "            for word in select_words:\n",
    "                document_vector += np.array(self.word2embedding[word]) * self.word_weight[word]\n",
    "                total_weight += self.word_weight[word]\n",
    "                \n",
    "            if len(select_words) == 0:\n",
    "                print('error', sentence)\n",
    "                continue\n",
    "            else:\n",
    "                if self.config[\"document_vector_weight_normalize\"]:\n",
    "                    document_vector /= total_weight\n",
    "                    total_weight = 1\n",
    "            \n",
    "            document_vectors.append(document_vector)\n",
    "            document_answers.append(select_words)\n",
    "            document_answers_wsum.append(total_weight)\n",
    "        \n",
    "        # get answers\n",
    "        document_answers_idx = []    \n",
    "        for ans in document_answers:\n",
    "            ans_idx = []\n",
    "            for token in ans:\n",
    "#                 if token in self.stoi:\n",
    "#                     ans_idx.append(self.stoi[token])     \n",
    "                ans_idx.append(self.stoi[token])                    \n",
    "            document_answers_idx.append(ans_idx)\n",
    "        \n",
    "        self.document_vectors = document_vectors\n",
    "        self.document_answers_idx = document_answers_idx\n",
    "        self.document_answers_wsum = document_answers_wsum\n",
    "        \n",
    "        return document_vectors, document_answers_idx, document_answers_wsum\n",
    "        \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]\n",
    "    \n",
    "    def check_docemb(self):\n",
    "        word_vectors = np.array(self.word_vectors)\n",
    "        pred = np.zeros(word_vectors.shape[1])\n",
    "        cnt = 0\n",
    "\n",
    "        for word_idx in self.document_answers_idx[0]:\n",
    "            pred += word_vectors[word_idx] * self.word_weight[self.itos[word_idx]]\n",
    "            cnt += self.word_weight[self.itos[word_idx]]\n",
    "        \n",
    "        if self.config[\"document_vector_weight_normalize\"]:\n",
    "            pred /= cnt\n",
    "        assert np.sum(self.document_vectors[0]) - np.sum(pred) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361a0b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(config, word2embedding):\n",
    "    # build vocabulary\n",
    "    vocab = Vocabulary(word2embedding, config)\n",
    "    vocab.read_raw()\n",
    "    vocab.build_vocabulary()\n",
    "    vocab_size = len(vocab)\n",
    "    # get doc emb\n",
    "    vocab.calculate_document_vector()\n",
    "    vocab.check_docemb()\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(config, word2embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e6d657",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finish building dataset!\")\n",
    "print(f\"Number of documents:{len(vocab.raw_documents)}\")\n",
    "print(f\"Number of words:{len(vocab)}\")\n",
    "\n",
    "l = list(map(len, vocab.document_answers_idx))\n",
    "print(\"Average length of document:\", np.mean(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c161ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = np.array(vocab.word_vectors)\n",
    "print(\"word_vectors:\", word_vectors.shape)\n",
    "\n",
    "document_vectors = np.array(vocab.document_vectors)\n",
    "print(\"document_vectors\", document_vectors.shape)\n",
    "\n",
    "document_answers_wsum = np.array(vocab.document_answers_wsum).reshape(-1, 1)\n",
    "print(\"document_answers_wsum\", document_answers_wsum.shape)\n",
    "\n",
    "# create weight_ans\n",
    "document_answers_idx = vocab.document_answers_idx\n",
    "\n",
    "# random shuffle\n",
    "# shuffle_idx = list(range(len(document_vectors)))\n",
    "# random.Random(seed).shuffle(shuffle_idx)\n",
    "\n",
    "# document_vectors = document_vectors[shuffle_idx]\n",
    "# document_answers_wsum = document_answers_wsum[shuffle_idx]\n",
    "# document_answers_idx = [document_answers_idx[idx] for idx in shuffle_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe65f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onthot_ans: word freq matrix\n",
    "# weight_ans: TFIDF matrix\n",
    "\n",
    "onehot_ans = np.zeros((len(document_answers_idx), word_vectors.shape[0]))\n",
    "weight_ans = np.zeros((len(document_answers_idx), word_vectors.shape[0]))\n",
    "print(weight_ans.shape)\n",
    "\n",
    "for i in tqdm(range(len(document_answers_idx))):\n",
    "    for word_idx in document_answers_idx[i]:\n",
    "        weight_ans[i, word_idx] += vocab.word_weight[vocab.itos[word_idx]]\n",
    "        onehot_ans[i, word_idx] += 1\n",
    "        \n",
    "    if config[\"document_vector_weight_normalize\"]:\n",
    "        weight_ans[i] /= np.sum(weight_ans[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd3dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "assert np.sum(document_vectors - np.dot(weight_ans, word_vectors) > 1e-10) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca2e49",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52979e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = []\n",
    "select_columns = ['model']\n",
    "for topk in config[\"topk\"]:\n",
    "    select_columns.append('percision@{}'.format(topk))\n",
    "for topk in config[\"topk\"]:\n",
    "    select_columns.append('recall@{}'.format(topk))\n",
    "for topk in config[\"topk\"]:\n",
    "    select_columns.append('F1@{}'.format(topk))\n",
    "for topk in config[\"topk\"]:\n",
    "    select_columns.append('ndcg@{}'.format(topk))\n",
    "select_columns.append('ndcg@all')\n",
    "select_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87d5379",
   "metadata": {},
   "source": [
    "## setting training size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9ddbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size_ratio = 1\n",
    "train_size = int(len(document_answers_idx) * train_size_ratio)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d31a21",
   "metadata": {},
   "source": [
    "## Top K freq word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c664900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251dbcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ans = document_answers_idx[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1efec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = [(word, freq) for word, freq in vocab.word_freq_in_corpus.items()]\n",
    "word_freq.sort(key=lambda x:x[1], reverse=True)\n",
    "word_freq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04793e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_word_evaluation(k=50):\n",
    "    topk_word = [word for (word, freq) in word_freq[:k]]\n",
    "\n",
    "    pr, re = [], []\n",
    "    for ans in tqdm(test_ans):\n",
    "        ans = set(ans)\n",
    "        ans = [vocab.itos[a] for a in ans]\n",
    "\n",
    "        hit = []\n",
    "        for word in ans:\n",
    "            if word in topk_word:\n",
    "                hit.append(word)\n",
    "\n",
    "        precision = len(hit) / k\n",
    "        recall = len(hit) / len(ans)\n",
    "        pr.append(precision)\n",
    "        re.append(recall)\n",
    "\n",
    "    pr = np.mean(pr)\n",
    "    re = np.mean(re)\n",
    "    f1 = 2 * pr * re / (pr + re) if (pr + re) != 0 else 0\n",
    "    print('top {} word'.format(k))\n",
    "    print('percision', np.mean(pr))\n",
    "    print('recall', np.mean(re))\n",
    "    print('F1', f1)\n",
    "    return f1\n",
    "\n",
    "\n",
    "for topk in config['topk']:\n",
    "    topk_results[\"F1@{}\".format(topk)] = topk_word_evaluation(k=topk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26308789",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def topk_word_evaluation_NDCG(k=50):\n",
    "    freq_word =[word for (word, freq) in word_freq]\n",
    "    freq_word_idx = [vocab.stoi[word] for word in freq_word if word in vocab.stoi]\n",
    "    \n",
    "    scores = np.zeros(len(vocab.word_vectors))\n",
    "    for rank, idx in enumerate(freq_word_idx):\n",
    "        scores[idx] = len(vocab.word_vectors) - rank\n",
    "    \n",
    "    NDCGs = []\n",
    "    \n",
    "    for ans in tqdm(test_ans):\n",
    "        weight_ans = np.zeros(len(vocab.word_vectors))\n",
    "        \n",
    "        for word_idx in ans:\n",
    "            if word_idx == 0:\n",
    "                continue\n",
    "            word = vocab.itos[word_idx]\n",
    "            weight_ans[word_idx] += vocab.IDF[word]\n",
    "\n",
    "        NDCG_score = ndcg_score(weight_ans.reshape(1,-1), scores.reshape(1,-1), k=k)\n",
    "        NDCGs.append(NDCG_score)\n",
    "\n",
    "    print('top {} NDCG:{}'.format(k, np.mean(NDCGs)))\n",
    "    \n",
    "    return np.mean(NDCGs)\n",
    "\n",
    "\n",
    "# for topk in config['topk']:\n",
    "#     topk_results[\"ndcg@{}\".format(topk)] = topk_word_evaluation_NDCG(k=topk)\n",
    "    \n",
    "# topk_results[\"ndcg@all\"] = topk_word_evaluation_NDCG(k=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1cbb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_results[\"model\"] = \"topk\"\n",
    "final_results.append(pd.Series(topk_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731c7c3c",
   "metadata": {},
   "source": [
    "## Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a091701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091f8b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(document_vectors.shape)\n",
    "print(weight_ans.shape)\n",
    "print(word_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75933b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sklearn(pred, ans):\n",
    "    results = {}\n",
    "        \n",
    "    one_hot_ans = np.arange(ans.shape[0])[ans > 0]\n",
    "    \n",
    "    for topk in config[\"topk\"]:\n",
    "        one_hot_pred = np.argsort(pred)[-topk:]\n",
    "        hit = np.intersect1d(one_hot_pred, one_hot_ans)\n",
    "        percision = len(hit) / topk\n",
    "        recall = len(hit) / len(one_hot_ans)\n",
    "        f1 = 2 * percision * recall / (percision + recall) if (percision + recall) > 0 else 0\n",
    "        \n",
    "        results['percision@{}'.format(topk)] = percision\n",
    "        results['recall@{}'.format(topk)] = recall\n",
    "        results['F1@{}'.format(topk)] = f1\n",
    "        \n",
    "    ans = ans.reshape(1, -1)\n",
    "    pred = pred.reshape(1, -1)\n",
    "    for topk in config[\"topk\"]:\n",
    "        results['ndcg@{}'.format(topk)] = ndcg_score(ans, pred, k=topk)\n",
    "\n",
    "    results['ndcg@all'] = (ndcg_score(ans, pred, k=None))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fcdf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sklearn(pred, ans):\n",
    "    results = {}\n",
    "        \n",
    "    # one_hot_ans = np.arange(ans.shape[0])[ans > 0]\n",
    "    \n",
    "    for topk in config[\"topk\"]:\n",
    "        one_hot_pred = np.argsort(pred)[-topk:]\n",
    "        one_hot_ans = np.argsort(ans)[-topk:]\n",
    "        hit = np.intersect1d(one_hot_pred, one_hot_ans)\n",
    "        percision = len(hit) / topk\n",
    "        recall = len(hit) / len(one_hot_ans)\n",
    "        f1 = 2 * percision * recall / (percision + recall) if (percision + recall) > 0 else 0\n",
    "        \n",
    "        results['percision@{}'.format(topk)] = percision\n",
    "        results['recall@{}'.format(topk)] = recall\n",
    "        results['F1@{}'.format(topk)] = f1\n",
    "        \n",
    "    ans = ans.reshape(1, -1)\n",
    "    pred = pred.reshape(1, -1)\n",
    "    for topk in config[\"topk\"]:\n",
    "        results['ndcg@{}'.format(topk)] = ndcg_score(ans, pred, k=topk)\n",
    "\n",
    "    results['ndcg@all'] = (ndcg_score(ans, pred, k=None))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbdc9ed",
   "metadata": {},
   "source": [
    "### linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e09dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for doc_id, doc_emb in enumerate(tqdm(document_vectors[:train_size])):\n",
    "    x = word_vectors.T\n",
    "    y = doc_emb\n",
    "    \n",
    "    ans = weight_ans[doc_id]\n",
    "    model = LinearRegression(fit_intercept=False).fit(x, y)\n",
    "    r2 = model.score(x, y)\n",
    "\n",
    "    res = evaluate_sklearn(model.coef_, ans)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18befbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results).mean()\n",
    "results['model'] = 'sk-linear-regression'\n",
    "final_results.append(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafc846b",
   "metadata": {},
   "source": [
    "### lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42cf137",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "sk_lasso_epoch = 1000\n",
    "\n",
    "for doc_id, doc_emb in enumerate(tqdm(document_vectors[:train_size])):\n",
    "    x = word_vectors.T\n",
    "    y = doc_emb\n",
    "    \n",
    "    ans = weight_ans[doc_id]\n",
    "    model = Lasso(positive=True, fit_intercept=False, alpha=0.1, max_iter=sk_lasso_epoch, tol=0).fit(x, y)\n",
    "    r2 = model.score(x, y)\n",
    "\n",
    "    res = evaluate_sklearn(model.coef_, ans)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07772202",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results).mean()\n",
    "results['model'] = 'sk-lasso'\n",
    "final_results.append(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f09963",
   "metadata": {},
   "source": [
    "## Basic Pursuit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3386dd",
   "metadata": {},
   "source": [
    "### OMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42d8855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuitCV\n",
    "\n",
    "results = []\n",
    "\n",
    "for doc_id, doc_emb in enumerate(tqdm(document_vectors[:train_size])):\n",
    "    x = word_vectors.T\n",
    "    y = doc_emb\n",
    "    \n",
    "    ans = weight_ans[doc_id]\n",
    "    n_nonzero_coefs = np.sum(weight_ans[doc_id] > 0) // 1\n",
    "    omp = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs, tol=None, normalize=True, fit_intercept=False)\n",
    "    omp.fit(x, y)\n",
    "\n",
    "    res = evaluate_sklearn(omp.coef_, ans)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d9d965",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results).mean()\n",
    "results['model'] = 'sk-omp'\n",
    "final_results.append(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e086a9",
   "metadata": {},
   "source": [
    "### BPDN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce7b552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylops\n",
    "\n",
    "results = []\n",
    "\n",
    "for doc_id, doc_emb in enumerate(tqdm(document_vectors[:train_size])):\n",
    "    d = doc_emb\n",
    "    \n",
    "    ans = weight_ans[doc_id]\n",
    "    xspgl1, pspgl1, info = \\\n",
    "        pylops.optimization.sparsity.SPGL1(word_vectors.T, d, None, tau=0, sigma=1e-3, iter_lim=200)\n",
    "\n",
    "    res = evaluate_sklearn(xspgl1, ans)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aea092",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results).mean()\n",
    "results['model'] = 'BPDN'\n",
    "final_results.append(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4fcb3a",
   "metadata": {},
   "source": [
    "## Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854f9586",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Lasso_Dataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 doc_vectors,\n",
    "                 doc_w_sum,\n",
    "                 weight_ans\n",
    "                 ):\n",
    "        self.doc_vectors = torch.FloatTensor(doc_vectors)\n",
    "        self.doc_w_sum = torch.FloatTensor(doc_w_sum)\n",
    "        self.weight_ans = weight_ans\n",
    "        assert len(doc_vectors) == len(doc_w_sum)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "                \n",
    "        return self.doc_vectors[idx], self.doc_w_sum[idx], idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa07e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(nn.Module):\n",
    "    \"\"\"\n",
    "    Input shape: (N, 3, 64, 64)\n",
    "    Output shape: (N, )\n",
    "    \"\"\"\n",
    "    def __init__(self, num_doc, num_words):\n",
    "        super(LR, self).__init__()\n",
    "        weight = torch.zeros(num_doc, num_words).to(device)\n",
    "        self.emb = torch.nn.Embedding.from_pretrained(weight, freeze=False)\n",
    "        \n",
    "    def forward(self, doc_ids, word_vectors):\n",
    "        return self.emb(doc_ids) @ word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c135bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_Custom_Lasso(model, train_loader):\n",
    "    results = {}\n",
    "    model.eval()\n",
    "    \n",
    "    scores = np.array(model.emb.cpu().weight.data)\n",
    "    model.emb.to(device)\n",
    "    true_relevance = train_loader.dataset.weight_ans\n",
    "\n",
    "    # F1\n",
    "    F1s = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for i in range(true_relevance.shape[0]):\n",
    "        one_hot_ans = np.arange(true_relevance.shape[1])[true_relevance[i] > 0]\n",
    "        pred = scores[i]\n",
    "        \n",
    "        F1_ = []\n",
    "        percision_ = []\n",
    "        recall_ = []\n",
    "        for topk in config[\"topk\"]:\n",
    "            one_hot_pred = np.argsort(pred)[-topk:]\n",
    "            \n",
    "            hit = np.intersect1d(one_hot_pred, one_hot_ans)\n",
    "            percision = len(hit) / topk\n",
    "            recall = len(hit) / len(one_hot_ans)\n",
    "            \n",
    "            F1 = 2 * percision * recall / (percision + recall) if (percision + recall) > 0 else 0\n",
    "            F1_.append(F1)\n",
    "            percision_.append(percision)\n",
    "            recall_.append(recall)\n",
    "            \n",
    "        F1s.append(F1_)\n",
    "        precisions.append(percision_)\n",
    "        recalls.append(recall_)\n",
    "        \n",
    "    F1s = np.mean(F1s, axis=0)\n",
    "    precisions = np.mean(precisions, axis=0)\n",
    "    recalls = np.mean(recalls, axis=0)\n",
    "    \n",
    "    for i, topk in enumerate(config[\"topk\"]):\n",
    "        results['F1@{}'.format(topk)] = F1s[i]\n",
    "        results['percision@{}'.format(topk)] = precisions[i]\n",
    "        results['recall@{}'.format(topk)] = recalls[i]\n",
    "\n",
    "    # NDCG\n",
    "    for topk in config[\"topk\"]:\n",
    "        results['ndcg@{}'.format(topk)] = ndcg_score(true_relevance, scores, k=topk)\n",
    "    results['ndcg@all'] = ndcg_score(true_relevance, scores, k=None)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac91f68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "print('document num', train_size)\n",
    "\n",
    "train_dataset = Custom_Lasso_Dataset(document_vectors[:train_size], document_answers_wsum[:train_size], weight_ans[:train_size])\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e533d31",
   "metadata": {},
   "source": [
    "## start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c01d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CLasso(select_idx):\n",
    "    batch_size = len(select_idx)\n",
    "    print('batch_size', batch_size)\n",
    "    train_dataset = Custom_Lasso_Dataset(document_vectors[select_idx], document_answers_wsum[select_idx], weight_ans[select_idx])\n",
    "    train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    model = LR(num_doc=batch_size, num_words=word_vectors.shape[0]).to(device)\n",
    "    model.train()\n",
    "\n",
    "    word_vectors_tensor = torch.FloatTensor(word_vectors).to(device)\n",
    "\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=nesterov)\n",
    "    criterion = nn.MSELoss(reduction='none')\n",
    "\n",
    "    results = []\n",
    "    step = 0\n",
    "    for epoch in tqdm(range(n_epoch)):    \n",
    "        loss_mse_his = []\n",
    "        loss_w_reg_his = []\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for data in train_loader:\n",
    "            doc_embs, doc_w_sum, doc_ids = data\n",
    "\n",
    "            doc_embs = doc_embs.to(device)\n",
    "            doc_w_sum = doc_w_sum.to(device)\n",
    "            doc_ids = doc_ids.to(device)\n",
    "\n",
    "            w_reg = doc_w_sum * w_sum_reg_mul\n",
    "            # w_reg = (torch.ones(doc_embs.size(0), 1) * w_sum_reg_mul).to(device)\n",
    "\n",
    "            # MSE loss\n",
    "            pred_doc_embs = model(doc_ids, word_vectors_tensor)     \n",
    "            loss_mse = criterion(pred_doc_embs, doc_embs)\n",
    "            loss_mse = torch.sum(torch.mean(loss_mse, dim=1))\n",
    "\n",
    "            pred_w_sum = torch.sum(model.emb(doc_ids), axis=1).view(-1, 1)\n",
    "            loss_w_reg = criterion(pred_w_sum, w_reg)\n",
    "            loss_w_reg = torch.sum(torch.mean(loss_w_reg, dim=1))\n",
    "\n",
    "            loss_l1 = torch.sum(torch.abs(model.emb(doc_ids)))\n",
    "            loss = loss_mse + loss_w_reg * w_sum_reg + loss_l1 * L1\n",
    "\n",
    "            # Model backwarding\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            loss_mse_his.append(loss_mse.item())\n",
    "            loss_w_reg_his.append(loss_w_reg.item())\n",
    "\n",
    "            for p in model.parameters():\n",
    "                p.data.clamp_(w_clip_value, float('inf'))\n",
    "#                 p.data -= L1\n",
    "#                 p.data[p.data < 0] = 0\n",
    "\n",
    "\n",
    "        if (verbose and epoch % valid_epoch == 0) or (not verbose and epoch == n_epoch-1):\n",
    "            res = {}\n",
    "            res['epoch'] = epoch\n",
    "            res['loss_mse'] = np.mean(loss_mse_his)\n",
    "            res['loss_w_reg'] = np.mean(loss_w_reg_his)\n",
    "\n",
    "            res_ndcg = evaluate_Custom_Lasso(model, train_loader)\n",
    "            res.update(res_ndcg)\n",
    "            results.append(res)\n",
    "\n",
    "            print()\n",
    "            for k, v in res.items():\n",
    "                print(k, v)\n",
    "                \n",
    "    return res, model\n",
    "\n",
    "# setting\n",
    "lr = 0.005\n",
    "momentum = 0.9999\n",
    "weight_decay = 0\n",
    "nesterov = False # True\n",
    "\n",
    "n_epoch = 15000\n",
    "\n",
    "w_sum_reg = 0#1e-2\n",
    "w_sum_reg_mul = 1\n",
    "w_clip_value = 0\n",
    "\n",
    "L1 = 1e-4\n",
    "\n",
    "verbose = False\n",
    "valid_epoch = 100\n",
    "\n",
    "results = []\n",
    "for i in range(len(document_vectors)//batch_size+1):\n",
    "    if i != len(document_vectors)//batch_size:\n",
    "        select_idx = np.arange(i*batch_size, (i+1)*batch_size)\n",
    "    elif len(document_vectors)%batch_size != 0:\n",
    "        select_idx = np.arange(i*batch_size, len(document_vectors))\n",
    "    else:\n",
    "        break\n",
    "    res, model = train_CLasso(select_idx)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49290d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "results_df = pd.DataFrame(results).set_index('epoch').mean()\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60936db",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['model'] = 'our-lasso'\n",
    "final_results.append(results_df[select_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd0e437",
   "metadata": {},
   "source": [
    "## Two stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa9d224",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_CLasso(select_idx):\n",
    "    \n",
    "    # setting\n",
    "    lr = 0.005\n",
    "    momentum = 0.9999\n",
    "    weight_decay = 0\n",
    "    nesterov = False # True\n",
    "\n",
    "    n_epoch = 10000\n",
    "\n",
    "    w_sum_reg = 0 #1e-2\n",
    "    w_sum_reg_mul = 1\n",
    "    w_clip_value = 0\n",
    "\n",
    "    L1 = 1e-4\n",
    "\n",
    "    batch_size = len(select_idx)\n",
    "    print('batch_size', batch_size)\n",
    "    train_dataset = Custom_Lasso_Dataset(document_vectors[select_idx], document_answers_wsum[select_idx], weight_ans[select_idx])\n",
    "    train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    model = LR(num_doc=batch_size, num_words=word_vectors.shape[0]).to(device)\n",
    "    model.train()\n",
    "\n",
    "    word_vectors_tensor = torch.FloatTensor(word_vectors).to(device)\n",
    "\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=nesterov)\n",
    "    criterion = nn.MSELoss(reduction='none')\n",
    "\n",
    "    results = []\n",
    "    step = 0\n",
    "    for epoch in tqdm(range(n_epoch)):    \n",
    "        loss_mse_his = []\n",
    "        loss_w_reg_his = []\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for data in train_loader:\n",
    "            doc_embs, doc_w_sum, doc_ids = data\n",
    "\n",
    "            doc_embs = doc_embs.to(device)\n",
    "            doc_w_sum = doc_w_sum.to(device)\n",
    "            doc_ids = doc_ids.to(device)\n",
    "\n",
    "            w_reg = doc_w_sum * w_sum_reg_mul\n",
    "            # w_reg = (torch.ones(doc_embs.size(0), 1) * w_sum_reg_mul).to(device)\n",
    "\n",
    "            # MSE loss\n",
    "            pred_doc_embs = model(doc_ids, word_vectors_tensor)     \n",
    "            loss_mse = criterion(pred_doc_embs, doc_embs)\n",
    "            loss_mse = torch.sum(torch.mean(loss_mse, dim=1))\n",
    "\n",
    "            pred_w_sum = torch.sum(model.emb(doc_ids), dim=1).view(-1, 1)\n",
    "            loss_w_reg = criterion(pred_w_sum, w_reg)\n",
    "            loss_w_reg = torch.sum(torch.mean(loss_w_reg, dim=1))\n",
    "\n",
    "            loss_l1 = torch.sum(torch.abs(model.emb(doc_ids)))\n",
    "            loss = loss_mse + loss_w_reg * w_sum_reg + loss_l1 * L1\n",
    "\n",
    "            # Model backwarding\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            loss_mse_his.append(loss_mse.item())\n",
    "            loss_w_reg_his.append(loss_w_reg.item())\n",
    "\n",
    "            for p in model.parameters():\n",
    "                p.data.clamp_(w_clip_value, float('inf'))\n",
    "#                 p.data -= L1\n",
    "#                 p.data[p.data < 0] = 0\n",
    "\n",
    "\n",
    "        if (verbose and epoch % valid_epoch == 0) or (not verbose and epoch == n_epoch-1):\n",
    "            res = {}\n",
    "            res['epoch'] = epoch\n",
    "            res['loss_mse'] = np.mean(loss_mse_his)\n",
    "            res['loss_w_reg'] = np.mean(loss_w_reg_his)\n",
    "\n",
    "            res_ndcg = evaluate_Custom_Lasso(model, train_loader)\n",
    "            res.update(res_ndcg)\n",
    "            results.append(res)\n",
    "\n",
    "            print()\n",
    "            for k, v in res.items():\n",
    "                print(k, v)\n",
    "    \n",
    "    \n",
    "    # setting\n",
    "    lr = 0.005\n",
    "    momentum = 0.999\n",
    "    weight_decay = 0\n",
    "    nesterov = False # True\n",
    "\n",
    "    n_epoch = 10000\n",
    "\n",
    "    w_sum_reg = 1e-2\n",
    "    w_sum_reg_mul = 1\n",
    "    w_clip_value = 0\n",
    "\n",
    "    L1 = 1e-4\n",
    "    \n",
    "    pred_wsum = torch.sum(model.emb.weight.data, dim=1).cpu().numpy()\n",
    "    print(pred_wsum)\n",
    "    print(document_answers_wsum[select_idx])\n",
    "    batch_size = len(select_idx)\n",
    "    print('batch_size', batch_size)\n",
    "    train_dataset = Custom_Lasso_Dataset(document_vectors[select_idx], pred_wsum, weight_ans[select_idx])\n",
    "    train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    model = LR(num_doc=batch_size, num_words=word_vectors.shape[0]).to(device)\n",
    "    model.train()\n",
    "\n",
    "    word_vectors_tensor = torch.FloatTensor(word_vectors).to(device)\n",
    "\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=nesterov)\n",
    "    criterion = nn.MSELoss(reduction='none')\n",
    "\n",
    "    results = []\n",
    "    step = 0\n",
    "    for epoch in tqdm(range(n_epoch)):    \n",
    "        loss_mse_his = []\n",
    "        loss_w_reg_his = []\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for data in train_loader:\n",
    "            doc_embs, doc_w_sum, doc_ids = data\n",
    "\n",
    "            doc_embs = doc_embs.to(device)\n",
    "            doc_w_sum = doc_w_sum.to(device)\n",
    "            doc_ids = doc_ids.to(device)\n",
    "\n",
    "            w_reg = doc_w_sum * w_sum_reg_mul\n",
    "            # w_reg = (torch.ones(doc_embs.size(0), 1) * w_sum_reg_mul).to(device)\n",
    "\n",
    "            # MSE loss\n",
    "            pred_doc_embs = model(doc_ids, word_vectors_tensor)     \n",
    "            loss_mse = criterion(pred_doc_embs, doc_embs)\n",
    "            loss_mse = torch.sum(torch.mean(loss_mse, dim=1))\n",
    "\n",
    "            pred_w_sum = torch.sum(model.emb(doc_ids), axis=1).view(-1, 1)\n",
    "            loss_w_reg = criterion(pred_w_sum, w_reg)\n",
    "            loss_w_reg = torch.sum(torch.mean(loss_w_reg, dim=1))\n",
    "\n",
    "            loss_l1 = torch.sum(torch.abs(model.emb(doc_ids)))\n",
    "            loss = loss_mse + loss_w_reg * w_sum_reg + loss_l1 * L1\n",
    "\n",
    "            # Model backwarding\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            loss_mse_his.append(loss_mse.item())\n",
    "            loss_w_reg_his.append(loss_w_reg.item())\n",
    "\n",
    "            for p in model.parameters():\n",
    "                p.data.clamp_(w_clip_value, float('inf'))\n",
    "#                 p.data -= L1\n",
    "#                 p.data[p.data < 0] = 0\n",
    "\n",
    "\n",
    "        if (verbose and epoch % valid_epoch == 0) or (not verbose and epoch == n_epoch-1):\n",
    "            res = {}\n",
    "            res['epoch'] = epoch\n",
    "            res['loss_mse'] = np.mean(loss_mse_his)\n",
    "            res['loss_w_reg'] = np.mean(loss_w_reg_his)\n",
    "\n",
    "            res_ndcg = evaluate_Custom_Lasso(model, train_loader)\n",
    "            res.update(res_ndcg)\n",
    "            results.append(res)\n",
    "\n",
    "            print()\n",
    "            for k, v in res.items():\n",
    "                print(k, v)\n",
    "    return res, model\n",
    "\n",
    "verbose = True\n",
    "valid_epoch = 1000\n",
    "\n",
    "results = []\n",
    "for i in range(len(document_vectors)//batch_size+1):\n",
    "    if i != len(document_vectors)//batch_size:\n",
    "        select_idx = np.arange(i*batch_size, (i+1)*batch_size)\n",
    "    elif len(document_vectors)%batch_size != 0:\n",
    "        select_idx = np.arange(i*batch_size, len(document_vectors))\n",
    "    else:\n",
    "        break\n",
    "    res, model = train_CLasso(select_idx)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2115b07e",
   "metadata": {},
   "source": [
    "## Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658c77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select doc_id and k\n",
    "doc_id = 36\n",
    "topk = 30\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31562cf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import colored\n",
    "from colored import stylize\n",
    "\n",
    "word_list = vocab.itos\n",
    "\n",
    "gt = [word_list[word_idx] for word_idx in np.argsort(weight_ans[doc_id])[::-1][:topk]]\n",
    "pred = [word_list[word_idx] for word_idx in np.argsort(model.emb.cpu().weight.data[doc_id].numpy())[::-1][:topk]]\n",
    "\n",
    "print('ground truth')\n",
    "for word in gt:\n",
    "    if word in pred:\n",
    "        print(stylize(word, colored.bg(\"yellow\")), end=' ')\n",
    "    else:\n",
    "        print(word, end=' ')\n",
    "\n",
    "print()\n",
    "print('\\nprediction')\n",
    "for word in pred:\n",
    "    if word in gt:\n",
    "        print(stylize(word, colored.bg(\"yellow\")), end=' ')\n",
    "    else:\n",
    "        print(word, end=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fa0add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw document\n",
    "print()\n",
    "ps = PorterStemmer()\n",
    "    \n",
    "for word in vocab.raw_documents[doc_id].split():\n",
    "    word_stem = ps.stem(word).lower()\n",
    "\n",
    "    if word_stem in gt:\n",
    "        if word_stem in pred:\n",
    "            print(stylize(word, colored.bg(\"yellow\")), end=' ')\n",
    "        else:\n",
    "            print(stylize(word, colored.bg(\"light_gray\")), end=' ')\n",
    "    else:\n",
    "        print(word, end=' ')\n",
    "# print(dataset.documents[doc_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc21f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "   \n",
    "scores = np.array(model.emb.weight.data)[doc_id].reshape(1, -1)\n",
    "true_relevance = train_loader.dataset.weight_ans[doc_id].reshape(1, -1)\n",
    "\n",
    "results['ndcg@50'] = (ndcg_score(true_relevance, scores, k=50))\n",
    "results['ndcg@100'] = (ndcg_score(true_relevance, scores, k=100))\n",
    "results['ndcg@200'] = (ndcg_score(true_relevance, scores, k=200))\n",
    "results['ndcg@all'] = (ndcg_score(true_relevance, scores, k=None))\n",
    "\n",
    "print('This document ndcg:')\n",
    "print('ground truth length:', np.sum(weight_ans[doc_id] > 0))\n",
    "print('NDCG top50', results['ndcg@50'])\n",
    "print('NDCG top100', results['ndcg@100'])\n",
    "print('NDCG top200', results['ndcg@200'])\n",
    "print('NDCG ALL', results['ndcg@all'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae49eac",
   "metadata": {},
   "source": [
    "## Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48834bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_notebook = in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb5ecc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "t = time.localtime()\n",
    "t = time.strftime(\"%Y-%m-%d_%H:%M:%S\", t)\n",
    "\n",
    "final_results_df = pd.DataFrame(final_results).reset_index(drop=True)\n",
    "\n",
    "experiment_dir = './records/dataset-{}-n_document-{}-wdist-{}-filtertopk-{}-dim-{}-time-{}'.format(\n",
    "                                        config['dataset'],\n",
    "                                        config['n_document'],\n",
    "                                        config[\"document_vector_agg_weight\"],\n",
    "                                        config[\"topk_word_freq_threshold\"],\n",
    "                                        config[\"embedding_file\"].split('.')[-2][:-1],\n",
    "                                        t)\n",
    "\n",
    "print('Saving to directory', experiment_dir)\n",
    "os.makedirs(experiment_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b704cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_df.to_csv(os.path.join(experiment_dir, 'result.csv'), index=False)\n",
    "\n",
    "import json\n",
    "with open(os.path.join(experiment_dir, 'config.json'), 'w') as json_file:\n",
    "    json.dump(config, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e6cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in final_results_df.set_index('model').columns:\n",
    "    plt.bar(final_results_df['model'],\n",
    "            final_results_df[feat], \n",
    "            width=0.5, \n",
    "            bottom=None, \n",
    "            align='center', \n",
    "            color=['lightsteelblue', \n",
    "                   'cornflowerblue', \n",
    "                   'royalblue', \n",
    "                   'navy'])\n",
    "    plt.title(feat)\n",
    "    plt.savefig(os.path.join(experiment_dir, '{}.png'.format(feat)))\n",
    "    plt.clf()\n",
    "    if is_notebook:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c679b721",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(final_results_df)\n",
    "final_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb3b503",
   "metadata": {},
   "source": [
    "## Basic pursuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01441938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(word_vectors.shape)\n",
    "# print(document_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcacfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spgl1\n",
    "\n",
    "# results = []\n",
    "# sigma = 1e-3\n",
    "\n",
    "# for doc_id, doc_emb in enumerate(tqdm(document_vectors[:train_size])):\n",
    "#     d = doc_emb\n",
    "#     ans = weight_ans[doc_id]\n",
    "#     coef, _, _, info = spgl1.spg_bpdn(A=word_vectors.T, b=d, sigma=sigma)\n",
    "\n",
    "#     res = evaluate_sklearn(coef, ans)\n",
    "#     results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0364e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = pd.DataFrame(results).mean()\n",
    "# results['model'] = 'SPG1'\n",
    "# final_results.append(results)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90f7fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spgl1\n",
    "\n",
    "# results = []\n",
    "\n",
    "# for doc_id, doc_emb in enumerate(tqdm(document_vectors[:train_size])):\n",
    "#     d = doc_emb\n",
    "#     ans = weight_ans[doc_id]\n",
    "#     coef, _, _, info = spgl1.spg_bp(A=word_vectors.T, b=d)\n",
    "\n",
    "#     res = evaluate_sklearn(coef, ans)\n",
    "#     results.append(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7821d96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = pd.DataFrame(results).mean()\n",
    "# results['model'] = 'SPG1'\n",
    "# final_results.append(results)\n",
    "# results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
