{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "792dad29",
   "metadata": {},
   "source": [
    "### dataset\n",
    "1. IMDB\n",
    "2. CNNNews\n",
    "3. [PubMed](https://github.com/LIAAD/KeywordExtractor-Datasets/blob/master/datasets/PubMed.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b007799a",
   "metadata": {},
   "source": [
    "### preprocess\n",
    "1. filter too frequent and less frequent words\n",
    "2. stemming\n",
    "3. document vector aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9967b",
   "metadata": {},
   "source": [
    "### evaluation\n",
    "1. F1\n",
    "2. NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9beff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/chrisliu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from itertools import cycle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Used to get the data\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "seed = 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2341254",
   "metadata": {},
   "source": [
    "## Preprocess config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dc01df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "config[\"dataset\"] = \"CNN\" # \"IMDB\" \"CNN\", \"PubMed\"\n",
    "config[\"n_document\"] = 5000\n",
    "config[\"normalize_word_embedding\"] = False\n",
    "config[\"min_word_freq_threshold\"] = 20\n",
    "config[\"topk_word_freq_threshold\"] = 100\n",
    "config[\"document_vector_agg_weight\"] = 'IDF' # ['mean', 'IDF', 'uniform', 'gaussian', 'exponential', 'pmi']\n",
    "config[\"document_vector_weight_normalize\"] = True # weighted sum or mean, True for mean, False for sum \n",
    "config[\"select_topk_TFIDF\"] = None # ignore\n",
    "config[\"embedding_file\"] = \"../data/glove.6B.200d.txt\" #glove.6B.100d.txt\n",
    "# config[\"embedding_file\"] = \"../data/400d.txt\"\n",
    "config[\"topk\"] = [10, 30, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05b0b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_notebook():\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        if 'IPKernelApp' not in get_ipython().config:  # pragma: no cover\n",
    "            return False\n",
    "    except ImportError:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fc91521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d0646d7c7142e9a49cb9af615d75c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:400000\n"
     ]
    }
   ],
   "source": [
    "def load_word2emb(embedding_file):\n",
    "    word2embedding = dict()\n",
    "    word_dim = int(re.findall(r\".(\\d+)d\", embedding_file)[0])\n",
    "\n",
    "    with open(embedding_file, \"r\") as f:\n",
    "        for line in tqdm(f):\n",
    "            line = line.strip().split()\n",
    "            word = line[0]\n",
    "            embedding = list(map(float, line[1:]))\n",
    "            word2embedding[word] = np.array(embedding)\n",
    "\n",
    "    print(\"Number of words:%d\" % len(word2embedding))\n",
    "\n",
    "    return word2embedding\n",
    "\n",
    "word2embedding = load_word2emb(config[\"embedding_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47bfc175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_wordemb(word2embedding):\n",
    "    # Every word emb should have norm 1\n",
    "    \n",
    "    word_emb = []\n",
    "    word_list = []\n",
    "    for word, emb in word2embedding.items():\n",
    "        word_list.append(word)\n",
    "        word_emb.append(emb)\n",
    "\n",
    "    word_emb = np.array(word_emb)\n",
    "\n",
    "    for i in range(len(word_emb)):\n",
    "        norm = np.linalg.norm(word_emb[i])\n",
    "        word_emb[i] = word_emb[i] / norm\n",
    "\n",
    "    for word, emb in tqdm(zip(word_list, word_emb)):\n",
    "        word2embedding[word] = emb\n",
    "    return word2embedding\n",
    "\n",
    "if config[\"normalize_word_embedding\"]:\n",
    "    normalize_wordemb(word2embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1233053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, word2embedding, config):\n",
    "        # The low frequency words will be assigned as <UNK> token\n",
    "        self.itos = {0: \"<UNK>\"}\n",
    "        self.stoi = {\"<UNK>\": 0}\n",
    "        \n",
    "        self.word2embedding = word2embedding\n",
    "        self.config = config\n",
    "\n",
    "        self.word_freq_in_corpus = defaultdict(int)\n",
    "        self.IDF = {}\n",
    "        self.ps = PorterStemmer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        self.word_dim = len(word2embedding['the'])\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def tokenizer_eng(self, text):\n",
    "        text = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n",
    "        text = text.strip().split()\n",
    "        \n",
    "        return [self.ps.stem(w) for w in text if w.lower() not in self.stop_words]\n",
    "    \n",
    "    def read_raw(self):        \n",
    "        if self.config[\"dataset\"] == 'IMDB':\n",
    "            data_file_path = '../data/IMDB.txt'\n",
    "        elif self.config[\"dataset\"] == 'CNN':\n",
    "            data_file_path = '../data/CNN.txt'\n",
    "        elif self.config[\"dataset\"] == 'PubMed':\n",
    "            data_file_path = '../data/PubMed.txt'\n",
    "        \n",
    "        # raw documents\n",
    "        self.raw_documents = []\n",
    "        with open(data_file_path,'r',encoding='utf-8') as f:\n",
    "            for line in tqdm(f, desc=\"Loading documents\"):\n",
    "                self.raw_documents.append(line.strip(\"\\n\"))\n",
    "                \n",
    "        return self.raw_documents\n",
    "    \n",
    "    def build_vocabulary(self):\n",
    "        sentence_list = self.raw_documents\n",
    "        \n",
    "        self.doc_freq = defaultdict(int) # # of document a word appear\n",
    "        self.document_num = len(sentence_list)\n",
    "        self.word_vectors = [[0]*self.word_dim] # unknown word emb\n",
    "        \n",
    "        for sentence in tqdm(sentence_list, desc=\"Preprocessing documents\"):\n",
    "            # for doc_freq\n",
    "            document_words = set()\n",
    "            \n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                # pass unknown word\n",
    "                if word not in self.word2embedding:\n",
    "                    continue\n",
    "                    \n",
    "                # calculate word freq\n",
    "                self.word_freq_in_corpus[word] += 1\n",
    "                document_words.add(word)\n",
    "                \n",
    "            for word in document_words:\n",
    "                self.doc_freq[word] += 1\n",
    "        \n",
    "        # calculate IDF\n",
    "        print('doc num', self.document_num)\n",
    "        for word, freq in self.doc_freq.items():\n",
    "            self.IDF[word] = math.log(self.document_num / (freq+1))\n",
    "        \n",
    "        # delete less freq words:\n",
    "        delete_words = []\n",
    "        for word, v in self.word_freq_in_corpus.items():\n",
    "            if v < self.config[\"min_word_freq_threshold\"]:\n",
    "                delete_words.append(word)     \n",
    "        for word in delete_words:\n",
    "            del self.IDF[word]    \n",
    "            del self.word_freq_in_corpus[word]    \n",
    "        \n",
    "        # delete too freq words\n",
    "        print('eliminate freq words')\n",
    "        IDF = [(word, freq) for word, freq in self.IDF.items()]\n",
    "        IDF.sort(key=lambda x: x[1])\n",
    "\n",
    "        for i in range(self.config[\"topk_word_freq_threshold\"]):\n",
    "            print(word)\n",
    "            word = IDF[i][0]\n",
    "            del self.IDF[word]\n",
    "            del self.word_freq_in_corpus[word]\n",
    "        \n",
    "        # construct word_vectors\n",
    "        idx = 1\n",
    "        for word in self.word_freq_in_corpus:\n",
    "            self.word_vectors.append(self.word2embedding[word])\n",
    "            self.stoi[word] = idx\n",
    "            self.itos[idx] = word\n",
    "            idx += 1\n",
    "            \n",
    "    def init_word_weight(self,sentence_list, agg):\n",
    "        if agg == 'mean':\n",
    "            self.word_weight = {word: 1 for word in self.IDF.keys()}\n",
    "        elif agg == 'IDF':\n",
    "            self.word_weight = self.IDF\n",
    "        elif agg == 'uniform':\n",
    "            self.word_weight = {word: np.random.uniform(low=0.0, high=1.0) for word in self.IDF.keys()}\n",
    "        elif agg == 'gaussian':\n",
    "            mu, sigma = 10, 1 # mean and standard deviation\n",
    "            self.word_weight = {word: np.random.normal(mu, sigma) for word in self.IDF.keys()}\n",
    "        elif agg == 'exponential':\n",
    "            self.word_weight = {word: np.random.exponential(scale=1.0) for word in self.IDF.keys()}\n",
    "        elif agg == 'pmi':\n",
    "            trigram_measures = BigramAssocMeasures()\n",
    "            self.word_weight = defaultdict(int)\n",
    "            corpus = []\n",
    "\n",
    "            for text in tqdm(sentence_list):\n",
    "                corpus.extend(text.split())\n",
    "\n",
    "            finder = BigramCollocationFinder.from_words(corpus)\n",
    "            for pmi_score in finder.score_ngrams(trigram_measures.pmi):\n",
    "                pair, score = pmi_score\n",
    "                self.word_weight[pair[0]] += score\n",
    "                self.word_weight[pair[1]] += score\n",
    "                \n",
    "    def calculate_document_vector(self):\n",
    "        # Return\n",
    "        # document_vectors: weighted sum of word emb\n",
    "        # document_answers_idx: doc to word index list\n",
    "        # document_answers_wsum: word weight summation, e.g. total TFIDF score of a doc\n",
    "        \n",
    "        document_vectors = [] \n",
    "        document_answers = []\n",
    "        document_answers_wsum = []\n",
    "        \n",
    "        sentence_list = self.raw_documents\n",
    "        agg = self.config[\"document_vector_agg_weight\"]\n",
    "        n_document = self.config[\"n_document\"]\n",
    "        select_topk_TFIDF = self.config[\"select_topk_TFIDF\"]\n",
    "        \n",
    "        self.init_word_weight(sentence_list, agg)\n",
    "        for sentence in tqdm(sentence_list[:min(n_document, len(sentence_list))], desc=\"calculate document vectors\"):\n",
    "            document_vector = np.zeros(len(self.word_vectors[0]))\n",
    "            select_words = []\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                # pass unknown word\n",
    "                if word not in self.stoi:\n",
    "                    continue\n",
    "                else:\n",
    "                    select_words.append(word)\n",
    "\n",
    "            # select topk TDIDF\n",
    "            if select_topk_TFIDF is not None:\n",
    "                doc_TFIDF = defaultdict(float)\n",
    "                for word in select_words:    \n",
    "                    doc_TFIDF[word] += self.IDF[word]\n",
    "\n",
    "                doc_TFIDF_l = [(word, TFIDF) for word, TFIDF in doc_TFIDF.items()]\n",
    "                doc_TFIDF_l.sort(key=lambda x:x[1], reverse=True)\n",
    "                \n",
    "                select_topk_words = set(list(map(lambda x:x[0], doc_TFIDF_l[:select_topk_TFIDF])))\n",
    "                select_words = [word for word in select_words if word in select_topk_words]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            total_weight = 0\n",
    "            # aggregate to doc vectors\n",
    "            for word in select_words:\n",
    "                document_vector += np.array(self.word2embedding[word]) * self.word_weight[word]\n",
    "                total_weight += self.word_weight[word]\n",
    "                \n",
    "            if len(select_words) == 0:\n",
    "                print('error', sentence)\n",
    "                continue\n",
    "            else:\n",
    "                if self.config[\"document_vector_weight_normalize\"]:\n",
    "                    document_vector /= total_weight\n",
    "                    total_weight = 1\n",
    "            \n",
    "            document_vectors.append(document_vector)\n",
    "            document_answers.append(select_words)\n",
    "            document_answers_wsum.append(total_weight)\n",
    "        \n",
    "        # get answers\n",
    "        document_answers_idx = []    \n",
    "        for ans in document_answers:\n",
    "            ans_idx = []\n",
    "            for token in ans:\n",
    "#                 if token in self.stoi:\n",
    "#                     ans_idx.append(self.stoi[token])     \n",
    "                ans_idx.append(self.stoi[token])                    \n",
    "            document_answers_idx.append(ans_idx)\n",
    "        \n",
    "        self.document_vectors = document_vectors\n",
    "        self.document_answers_idx = document_answers_idx\n",
    "        self.document_answers_wsum = document_answers_wsum\n",
    "        \n",
    "        return document_vectors, document_answers_idx, document_answers_wsum\n",
    "        \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]\n",
    "    \n",
    "    def check_docemb(self):\n",
    "        word_vectors = np.array(self.word_vectors)\n",
    "        pred = np.zeros(word_vectors.shape[1])\n",
    "        cnt = 0\n",
    "\n",
    "        for word_idx in self.document_answers_idx[0]:\n",
    "            pred += word_vectors[word_idx] * self.word_weight[self.itos[word_idx]]\n",
    "            cnt += self.word_weight[self.itos[word_idx]]\n",
    "        \n",
    "        if self.config[\"document_vector_weight_normalize\"]:\n",
    "            pred /= cnt\n",
    "        assert np.sum(self.document_vectors[0]) - np.sum(pred) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "361a0b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2ed3031e554e89937b72033e3523ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading documents: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d598c8aa5a431abf2b8ff5a382cde9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing documents:   0%|          | 0/19026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc num 19026\n",
      "eliminate freq words\n",
      "paroxysm\n",
      "subject\n",
      "line\n",
      "organ\n",
      "write\n",
      "univers\n",
      "one\n",
      "would\n",
      "use\n",
      "like\n",
      "get\n",
      "know\n",
      "dont\n",
      "think\n",
      "time\n",
      "make\n",
      "also\n",
      "say\n",
      "go\n",
      "im\n",
      "could\n",
      "want\n",
      "new\n",
      "work\n",
      "good\n",
      "well\n",
      "way\n",
      "need\n",
      "look\n",
      "even\n",
      "anyon\n",
      "thing\n",
      "see\n",
      "tri\n",
      "thank\n",
      "much\n",
      "year\n",
      "world\n",
      "system\n",
      "right\n",
      "problem\n",
      "may\n",
      "take\n",
      "mani\n",
      "two\n",
      "first\n",
      "seem\n",
      "question\n",
      "pleas\n",
      "1\n",
      "state\n",
      "us\n",
      "come\n",
      "2\n",
      "post\n",
      "help\n",
      "call\n",
      "usa\n",
      "point\n",
      "sinc\n",
      "find\n",
      "read\n",
      "still\n",
      "back\n",
      "mean\n",
      "ive\n",
      "give\n",
      "email\n",
      "sure\n",
      "differ\n",
      "might\n",
      "run\n",
      "cant\n",
      "reason\n",
      "last\n",
      "day\n",
      "interest\n",
      "case\n",
      "let\n",
      "person\n",
      "said\n",
      "never\n",
      "start\n",
      "doesnt\n",
      "tell\n",
      "better\n",
      "ask\n",
      "got\n",
      "without\n",
      "follow\n",
      "part\n",
      "lot\n",
      "3\n",
      "number\n",
      "put\n",
      "fact\n",
      "gener\n",
      "inform\n",
      "actual\n",
      "that\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ff10de6f4a417ba831c8ecf0e4625f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "calculate document vectors:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error |> \n",
      "error \n",
      "error \n",
      "error Mikael Fredriksson\n",
      "error \n",
      "error -------------------------------------------------\n",
      "error email: mikael_fredriksson@macexchange.se\n",
      "error \n",
      "error FIDO 2:203/211\n",
      "error  \n"
     ]
    }
   ],
   "source": [
    "def build_vocab(config, word2embedding):\n",
    "    # build vocabulary\n",
    "    vocab = Vocabulary(word2embedding, config)\n",
    "    vocab.read_raw()\n",
    "    vocab.build_vocabulary()\n",
    "    vocab_size = len(vocab)\n",
    "    # get doc emb\n",
    "    vocab.calculate_document_vector()\n",
    "    vocab.check_docemb()\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(config, word2embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7e6d657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish building dataset!\n",
      "Number of documents:19026\n",
      "Number of words:7602\n",
      "Average length of document: 86.76372745490983\n"
     ]
    }
   ],
   "source": [
    "print(\"Finish building dataset!\")\n",
    "print(f\"Number of documents:{len(vocab.raw_documents)}\")\n",
    "print(f\"Number of words:{len(vocab)}\")\n",
    "\n",
    "l = list(map(len, vocab.document_answers_idx))\n",
    "print(\"Average length of document:\", np.mean(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c161ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_vectors: (7602, 200)\n",
      "document_vectors (4990, 200)\n",
      "document_answers_wsum (4990, 1)\n"
     ]
    }
   ],
   "source": [
    "word_vectors = np.array(vocab.word_vectors)\n",
    "print(\"word_vectors:\", word_vectors.shape)\n",
    "\n",
    "document_vectors = np.array(vocab.document_vectors)\n",
    "print(\"document_vectors\", document_vectors.shape)\n",
    "\n",
    "document_answers_wsum = np.array(vocab.document_answers_wsum).reshape(-1, 1)\n",
    "print(\"document_answers_wsum\", document_answers_wsum.shape)\n",
    "\n",
    "# create weight_ans\n",
    "document_answers_idx = vocab.document_answers_idx\n",
    "\n",
    "# random shuffle\n",
    "# shuffle_idx = list(range(len(document_vectors)))\n",
    "# random.Random(seed).shuffle(shuffle_idx)\n",
    "\n",
    "# document_vectors = document_vectors[shuffle_idx]\n",
    "# document_answers_wsum = document_answers_wsum[shuffle_idx]\n",
    "# document_answers_idx = [document_answers_idx[idx] for idx in shuffle_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe65f37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4990, 7602)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79391fc791d4e16bc27883a2c23c9f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4990 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# onthot_ans: word freq matrix\n",
    "# weight_ans: TFIDF matrix\n",
    "\n",
    "onehot_ans = np.zeros((len(document_answers_idx), word_vectors.shape[0]))\n",
    "weight_ans = np.zeros((len(document_answers_idx), word_vectors.shape[0]))\n",
    "print(weight_ans.shape)\n",
    "\n",
    "for i in tqdm(range(len(document_answers_idx))):\n",
    "    for word_idx in document_answers_idx[i]:\n",
    "        weight_ans[i, word_idx] += vocab.word_weight[vocab.itos[word_idx]]\n",
    "        onehot_ans[i, word_idx] += 1\n",
    "        \n",
    "    if config[\"document_vector_weight_normalize\"]:\n",
    "        weight_ans[i] /= np.sum(weight_ans[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bd3dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "assert np.sum(document_vectors - np.dot(weight_ans, word_vectors) > 1e-10) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca2e49",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a52979e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model',\n",
       " 'percision@10',\n",
       " 'percision@30',\n",
       " 'percision@50',\n",
       " 'recall@10',\n",
       " 'recall@30',\n",
       " 'recall@50',\n",
       " 'F1@10',\n",
       " 'F1@30',\n",
       " 'F1@50',\n",
       " 'ndcg@10',\n",
       " 'ndcg@30',\n",
       " 'ndcg@50',\n",
       " 'ndcg@all']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results = []\n",
    "select_columns = ['model']\n",
    "for topk in config[\"topk\"]:\n",
    "    select_columns.append('percision@{}'.format(topk))\n",
    "for topk in config[\"topk\"]:\n",
    "    select_columns.append('recall@{}'.format(topk))\n",
    "for topk in config[\"topk\"]:\n",
    "    select_columns.append('F1@{}'.format(topk))\n",
    "for topk in config[\"topk\"]:\n",
    "    select_columns.append('ndcg@{}'.format(topk))\n",
    "select_columns.append('ndcg@all')\n",
    "select_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87d5379",
   "metadata": {},
   "source": [
    "## setting training size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f9ddbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4990"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size_ratio = 1\n",
    "train_size = int(len(document_answers_idx) * train_size_ratio)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d31a21",
   "metadata": {},
   "source": [
    "## Top K freq word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c664900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "251dbcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ans = document_answers_idx[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1efec49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('x', 6539),\n",
       " ('god', 5208),\n",
       " ('file', 4918),\n",
       " ('0', 4520),\n",
       " ('window', 4444),\n",
       " ('program', 4201),\n",
       " ('drive', 3633),\n",
       " ('4', 3528),\n",
       " ('game', 3474),\n",
       " ('govern', 3268)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = [(word, freq) for word, freq in vocab.word_freq_in_corpus.items()]\n",
    "word_freq.sort(key=lambda x:x[1], reverse=True)\n",
    "word_freq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a04793e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4c217744c549d58a4c4597aa0d0438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4990 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 word\n",
      "percision 0.07022044088176352\n",
      "recall 0.016468764447456485\n",
      "F1 0.026680228429513793\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad2e33af3aad44ac81daa028fa53ff5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4990 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 30 word\n",
      "percision 0.07760187040748162\n",
      "recall 0.04907192575899562\n",
      "F1 0.060124087832506286\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6f7c97a749482480e0ced9a185eaca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4990 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 50 word\n",
      "percision 0.07772344689378759\n",
      "recall 0.07982629578863966\n",
      "F1 0.07876083776236008\n"
     ]
    }
   ],
   "source": [
    "def topk_word_evaluation(k=50):\n",
    "    topk_word = [word for (word, freq) in word_freq[:k]]\n",
    "\n",
    "    pr, re = [], []\n",
    "    for ans in tqdm(test_ans):\n",
    "        ans = set(ans)\n",
    "        ans = [vocab.itos[a] for a in ans]\n",
    "\n",
    "        hit = []\n",
    "        for word in ans:\n",
    "            if word in topk_word:\n",
    "                hit.append(word)\n",
    "\n",
    "        precision = len(hit) / k\n",
    "        recall = len(hit) / len(ans)\n",
    "        pr.append(precision)\n",
    "        re.append(recall)\n",
    "\n",
    "    pr = np.mean(pr)\n",
    "    re = np.mean(re)\n",
    "    f1 = 2 * pr * re / (pr + re) if (pr + re) != 0 else 0\n",
    "    print('top {} word'.format(k))\n",
    "    print('percision', np.mean(pr))\n",
    "    print('recall', np.mean(re))\n",
    "    print('F1', f1)\n",
    "    return f1\n",
    "\n",
    "\n",
    "for topk in config['topk']:\n",
    "    topk_results[\"F1@{}\".format(topk)] = topk_word_evaluation(k=topk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26308789",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def topk_word_evaluation_NDCG(k=50):\n",
    "    freq_word =[word for (word, freq) in word_freq]\n",
    "    freq_word_idx = [vocab.stoi[word] for word in freq_word if word in vocab.stoi]\n",
    "    \n",
    "    scores = np.zeros(len(vocab.word_vectors))\n",
    "    for rank, idx in enumerate(freq_word_idx):\n",
    "        scores[idx] = len(vocab.word_vectors) - rank\n",
    "    \n",
    "    NDCGs = []\n",
    "    \n",
    "    for ans in tqdm(test_ans):\n",
    "        weight_ans = np.zeros(len(vocab.word_vectors))\n",
    "        \n",
    "        for word_idx in ans:\n",
    "            if word_idx == 0:\n",
    "                continue\n",
    "            word = vocab.itos[word_idx]\n",
    "            weight_ans[word_idx] += vocab.IDF[word]\n",
    "\n",
    "        NDCG_score = ndcg_score(weight_ans.reshape(1,-1), scores.reshape(1,-1), k=k)\n",
    "        NDCGs.append(NDCG_score)\n",
    "\n",
    "    print('top {} NDCG:{}'.format(k, np.mean(NDCGs)))\n",
    "    \n",
    "    return np.mean(NDCGs)\n",
    "\n",
    "\n",
    "# for topk in config['topk']:\n",
    "#     topk_results[\"ndcg@{}\".format(topk)] = topk_word_evaluation_NDCG(k=topk)\n",
    "    \n",
    "# topk_results[\"ndcg@all\"] = topk_word_evaluation_NDCG(k=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a1cbb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_results[\"model\"] = \"topk\"\n",
    "final_results.append(pd.Series(topk_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731c7c3c",
   "metadata": {},
   "source": [
    "## Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a091701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "091f8b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4990, 200)\n",
      "(4990, 7602)\n",
      "(7602, 200)\n"
     ]
    }
   ],
   "source": [
    "print(document_vectors.shape)\n",
    "print(weight_ans.shape)\n",
    "print(word_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75933b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sklearn(pred, ans):\n",
    "    results = {}\n",
    "        \n",
    "    one_hot_ans = np.arange(ans.shape[0])[ans > 0]\n",
    "    \n",
    "    for topk in config[\"topk\"]:\n",
    "        one_hot_pred = np.argsort(pred)[-topk:]\n",
    "        hit = np.intersect1d(one_hot_pred, one_hot_ans)\n",
    "        percision = len(hit) / topk\n",
    "        recall = len(hit) / len(one_hot_ans)\n",
    "        f1 = 2 * percision * recall / (percision + recall) if (percision + recall) > 0 else 0\n",
    "        \n",
    "        results['percision@{}'.format(topk)] = percision\n",
    "        results['recall@{}'.format(topk)] = recall\n",
    "        results['F1@{}'.format(topk)] = f1\n",
    "        \n",
    "    ans = ans.reshape(1, -1)\n",
    "    pred = pred.reshape(1, -1)\n",
    "    for topk in config[\"topk\"]:\n",
    "        results['ndcg@{}'.format(topk)] = ndcg_score(ans, pred, k=topk)\n",
    "\n",
    "    results['ndcg@all'] = (ndcg_score(ans, pred, k=None))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4fcb3a",
   "metadata": {},
   "source": [
    "## Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f4f53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "854f9586",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Lasso_Dataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 doc_vectors,\n",
    "                 doc_w_sum,\n",
    "                 weight_ans\n",
    "                 ):\n",
    "        self.doc_vectors = torch.FloatTensor(doc_vectors)\n",
    "        self.doc_w_sum = torch.FloatTensor(doc_w_sum)\n",
    "        self.weight_ans = weight_ans\n",
    "        assert len(doc_vectors) == len(doc_w_sum)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "                \n",
    "        return self.doc_vectors[idx], self.doc_w_sum[idx], idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0aa07e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(nn.Module):\n",
    "    \"\"\"\n",
    "    Input shape: (N, 3, 64, 64)\n",
    "    Output shape: (N, )\n",
    "    \"\"\"\n",
    "    def __init__(self, num_doc, num_words):\n",
    "        super(LR, self).__init__()\n",
    "        weight = torch.zeros(num_doc, num_words).to(device)\n",
    "#         weight = torch.rand(num_doc, num_words).to(device)\n",
    "        self.emb = torch.nn.Embedding.from_pretrained(weight, freeze=False)\n",
    "        \n",
    "    def forward(self, doc_ids, word_vectors):\n",
    "        return self.emb(doc_ids) @ word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c135bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_Custom_Lasso(model, train_loader):\n",
    "    results = {}\n",
    "    model.eval()\n",
    "    \n",
    "    scores = np.array(model.emb.cpu().weight.data)\n",
    "    model.emb.to(device)\n",
    "    true_relevance = train_loader.dataset.weight_ans\n",
    "\n",
    "    # F1\n",
    "    F1s = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for i in range(true_relevance.shape[0]):\n",
    "        one_hot_ans = np.arange(true_relevance.shape[1])[true_relevance[i] > 0]\n",
    "        pred = scores[i]\n",
    "        \n",
    "        F1_ = []\n",
    "        percision_ = []\n",
    "        recall_ = []\n",
    "        for topk in config[\"topk\"]:\n",
    "            topk = min(topk, sum(true_relevance[i] > 0))\n",
    "            one_hot_pred = np.argsort(pred)[-topk:]\n",
    "            one_hot_ans = np.argsort(true_relevance[i])[-topk:]\n",
    "            \n",
    "            hit = np.intersect1d(one_hot_pred, one_hot_ans)\n",
    "            percision = len(hit) / topk\n",
    "            recall = len(hit) / len(one_hot_ans)\n",
    "            \n",
    "            F1 = 2 * percision * recall / (percision + recall) if (percision + recall) > 0 else 0\n",
    "            F1_.append(F1)\n",
    "            percision_.append(percision)\n",
    "            recall_.append(recall)\n",
    "            \n",
    "        F1s.append(F1_)\n",
    "        precisions.append(percision_)\n",
    "        recalls.append(recall_)\n",
    "        \n",
    "    F1s = np.mean(F1s, axis=0)\n",
    "    precisions = np.mean(precisions, axis=0)\n",
    "    recalls = np.mean(recalls, axis=0)\n",
    "    \n",
    "    for i, topk in enumerate(config[\"topk\"]):\n",
    "        results['F1@{}'.format(topk)] = F1s[i]\n",
    "        results['percision@{}'.format(topk)] = precisions[i]\n",
    "        results['recall@{}'.format(topk)] = recalls[i]\n",
    "\n",
    "    # NDCG\n",
    "    for topk in config[\"topk\"]:\n",
    "        results['ndcg@{}'.format(topk)] = ndcg_score(true_relevance, scores, k=topk)\n",
    "    results['ndcg@all'] = ndcg_score(true_relevance, scores, k=None)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ac91f68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document num 100\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "train_size = 100#len(document_vectors)\n",
    "print('document num', train_size)\n",
    "\n",
    "train_dataset = Custom_Lasso_Dataset(document_vectors[:train_size], document_answers_wsum[:train_size], weight_ans[:train_size])\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e533d31",
   "metadata": {},
   "source": [
    "## start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0547688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457270fb8ccd4317b182da4ecf87d1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 1999\n",
      "loss_mse 0.2356768250465393\n",
      "loss_w_reg 9.036284446716309\n",
      "F1@10 0.813\n",
      "percision@10 0.813\n",
      "recall@10 0.813\n",
      "F1@30 0.7580000000000001\n",
      "percision@30 0.7580000000000001\n",
      "recall@30 0.7580000000000001\n",
      "F1@50 0.7186879362327315\n",
      "percision@50 0.7186879362327315\n",
      "recall@50 0.7186879362327315\n",
      "ndcg@10 0.9488813039647578\n",
      "ndcg@30 0.9105957938077393\n",
      "ndcg@50 0.8765122250706859\n",
      "ndcg@all 0.9313601037681367\n",
      "batch_size 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1a47cc4ddd408ea702d770a461ed4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-3f471f522ad5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_CLasso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselect_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-3f471f522ad5>\u001b[0m in \u001b[0;36mtrain_CLasso\u001b[0;34m(select_idx)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# Model backwarding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtual_env/py37/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtual_env/py37/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_CLasso(select_idx):\n",
    "    batch_size = len(select_idx)\n",
    "    print('batch_size', batch_size)\n",
    "    train_dataset = Custom_Lasso_Dataset(document_vectors[select_idx], document_answers_wsum[select_idx], weight_ans[select_idx])\n",
    "    train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    model = LR(num_doc=batch_size, num_words=word_vectors.shape[0]).to(device)\n",
    "    model.train()\n",
    "\n",
    "    word_vectors_tensor = torch.FloatTensor(word_vectors).to(device)\n",
    "\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=nesterov)\n",
    "    criterion = nn.MSELoss(reduction='none')\n",
    "\n",
    "    results = []\n",
    "    step = 0\n",
    "    for epoch in tqdm(range(n_epoch)):    \n",
    "        loss_mse_his = []\n",
    "        loss_w_reg_his = []\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for data in train_loader:\n",
    "            doc_embs, doc_w_sum, doc_ids = data\n",
    "\n",
    "            doc_embs = doc_embs.to(device)\n",
    "            doc_w_sum = doc_w_sum.to(device)\n",
    "            doc_ids = doc_ids.to(device)\n",
    "\n",
    "            w_reg = doc_w_sum * w_sum_reg_mul\n",
    "            # w_reg = (torch.ones(doc_embs.size(0), 1) * w_sum_reg_mul).to(device)\n",
    "\n",
    "            # MSE loss\n",
    "            pred_doc_embs = model(doc_ids, word_vectors_tensor)     \n",
    "            loss_mse = criterion(pred_doc_embs, doc_embs)\n",
    "            loss_mse = torch.sum(torch.mean(loss_mse, dim=1))\n",
    "\n",
    "            pred_w_sum = torch.sum(model.emb(doc_ids), axis=1).view(-1, 1)\n",
    "            loss_w_reg = criterion(pred_w_sum, w_reg)\n",
    "            loss_w_reg = torch.sum(torch.mean(loss_w_reg, dim=1))\n",
    "\n",
    "            loss_l1 = torch.sum(torch.abs(model.emb(doc_ids)))\n",
    "            loss = loss_mse + loss_w_reg * w_sum_reg + loss_l1 * L1\n",
    "\n",
    "            # Model backwarding\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            loss_mse_his.append(loss_mse.item())\n",
    "            loss_w_reg_his.append(loss_w_reg.item())\n",
    "\n",
    "            for p in model.parameters():\n",
    "                p.data.clamp_(w_clip_value, float('inf'))\n",
    "#                 p.data -= L1\n",
    "#                 p.data[p.data < 0] = 0\n",
    "\n",
    "\n",
    "        if (verbose and epoch % valid_epoch == 0) or (not verbose and epoch == n_epoch-1):\n",
    "            res = {}\n",
    "            res['epoch'] = epoch\n",
    "            res['loss_mse'] = np.mean(loss_mse_his)\n",
    "            res['loss_w_reg'] = np.mean(loss_w_reg_his)\n",
    "\n",
    "            res_ndcg = evaluate_Custom_Lasso(model, train_loader)\n",
    "            res.update(res_ndcg)\n",
    "            results.append(res)\n",
    "\n",
    "            print()\n",
    "            for k, v in res.items():\n",
    "                print(k, v)\n",
    "                \n",
    "    return res, model\n",
    "\n",
    "# setting\n",
    "lr = 0.02\n",
    "momentum = 0.999\n",
    "weight_decay = 0#1e-6\n",
    "nesterov = False # True\n",
    "\n",
    "n_epoch = 2000\n",
    "\n",
    "w_sum_reg = 1e-2\n",
    "w_sum_reg_mul = 1\n",
    "w_clip_value = 0\n",
    "\n",
    "L1 = 1e-5\n",
    "\n",
    "verbose = False\n",
    "valid_epoch = 100\n",
    "\n",
    "results = []\n",
    "for i in range(len(document_vectors)//batch_size+1):\n",
    "    if i != len(document_vectors)//batch_size:\n",
    "        select_idx = np.arange(i*batch_size, (i+1)*batch_size)\n",
    "    elif len(document_vectors)%batch_size != 0:\n",
    "        select_idx = np.arange(i*batch_size, len(document_vectors))\n",
    "    else:\n",
    "        break\n",
    "    res, model = train_CLasso(select_idx)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49290d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "results_df = pd.DataFrame(results).set_index('epoch').mean()\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8750bc30",
   "metadata": {},
   "source": [
    "### 200d\n",
    "1. -\n",
    "loss_mse        0.061461\n",
    "loss_w_reg      1.900502\n",
    "F1@10           0.825233\n",
    "percision@10    0.825233\n",
    "recall@10       0.825233\n",
    "F1@30           0.775852\n",
    "percision@30    0.775852\n",
    "recall@30       0.775852\n",
    "F1@50           0.739797\n",
    "percision@50    0.739797\n",
    "recall@50       0.739797\n",
    "ndcg@10         0.957472\n",
    "ndcg@30         0.915883\n",
    "ndcg@50         0.887274\n",
    "ndcg@all        0.938138\n",
    "2. weight sum, L1=1e-5, mom=0.999, lr=0.02, epoch=2000\n",
    "loss_mse        2.345032e-07\n",
    "loss_w_reg      3.833009e+00\n",
    "F1@10           2.686949e-01\n",
    "percision@10    6.027000e-01\n",
    "recall@10       1.955196e-01\n",
    "F1@30           2.735864e-01\n",
    "percision@30    3.311667e-01\n",
    "recall@30       2.891171e-01\n",
    "F1@50           2.493415e-01\n",
    "percision@50    2.426600e-01\n",
    "recall@50       3.350195e-01\n",
    "ndcg@10         6.806452e-01\n",
    "ndcg@30         5.929742e-01\n",
    "ndcg@50         5.840537e-01\n",
    "3. L1, weight sum=1e-2, mom=0.999, lr=0.02, epoch=2000\n",
    "loss_mse         0.908629\n",
    "loss_w_reg      11.272542\n",
    "F1@10            0.291310\n",
    "percision@10     0.637900\n",
    "recall@10        0.214145\n",
    "F1@30            0.284621\n",
    "percision@30     0.333467\n",
    "recall@30        0.308162\n",
    "F1@50            0.251672\n",
    "percision@50     0.239000\n",
    "recall@50        0.347421\n",
    "ndcg@10          0.731694\n",
    "ndcg@30          0.631273\n",
    "ndcg@50          0.617426\n",
    "4. clip, L1=1e-5, weight sum=1e-2, mom=0.999, lr=0.01, epoch=10000\n",
    "loss_mse        0.081876\n",
    "loss_w_reg      0.998885\n",
    "F1@10           0.267448\n",
    "percision@10    0.615500\n",
    "recall@10       0.192869\n",
    "F1@30           0.275906\n",
    "percision@30    0.337767\n",
    "recall@30       0.288057\n",
    "F1@50           0.250726\n",
    "percision@50    0.245280\n",
    "recall@50       0.333993\n",
    "ndcg@10         0.706146\n",
    "ndcg@30         0.613245\n",
    "ndcg@50         0.601412\n",
    "5. momentum, L1=1e-5, weight sum=1e-2, lr=0.005, epoch=20000\n",
    "loss_mse        0.000678\n",
    "loss_w_reg      0.017720\n",
    "F1@10           0.656829\n",
    "percision@10    0.656829\n",
    "recall@10       0.656829\n",
    "F1@30           0.542128\n",
    "percision@30    0.542128\n",
    "recall@30       0.542128\n",
    "F1@50           0.507397\n",
    "percision@50    0.507397\n",
    "recall@50       0.507397\n",
    "ndcg@10         0.882713\n",
    "ndcg@30         0.786544\n",
    "ndcg@50         0.766185\n",
    "ndcg@all        0.869014"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3763ac",
   "metadata": {},
   "source": [
    "###\n",
    "1. -\n",
    "loss_mse         0.807133\n",
    "loss_w_reg      10.091759\n",
    "F1@10            0.305983\n",
    "percision@10     0.660657\n",
    "recall@10        0.229369\n",
    "F1@30            0.295320\n",
    "percision@30     0.344128\n",
    "recall@30        0.325723\n",
    "F1@50            0.259885\n",
    "percision@50     0.246354\n",
    "recall@50        0.365655\n",
    "ndcg@10          0.740726\n",
    "ndcg@30          0.640511\n",
    "ndcg@50          0.626564\n",
    "2. weight sum\n",
    "loss_mse        2.345032e-07\n",
    "loss_w_reg      3.833009e+00\n",
    "F1@10           2.686949e-01\n",
    "percision@10    6.027000e-01\n",
    "recall@10       1.955196e-01\n",
    "F1@30           2.735864e-01\n",
    "percision@30    3.311667e-01\n",
    "recall@30       2.891171e-01\n",
    "F1@50           2.493415e-01\n",
    "percision@50    2.426600e-01\n",
    "recall@50       3.350195e-01\n",
    "ndcg@10         6.806452e-01\n",
    "ndcg@30         5.929742e-01\n",
    "ndcg@50         5.840537e-01\n",
    "3. L1\n",
    "loss_mse         0.908629\n",
    "loss_w_reg      11.272542\n",
    "F1@10            0.291310\n",
    "percision@10     0.637900\n",
    "recall@10        0.214145\n",
    "F1@30            0.284621\n",
    "percision@30     0.333467\n",
    "recall@30        0.308162\n",
    "F1@50            0.251672\n",
    "percision@50     0.239000\n",
    "recall@50        0.347421\n",
    "ndcg@10          0.731694\n",
    "ndcg@30          0.631273\n",
    "ndcg@50          0.617426\n",
    "4. clip\n",
    "loss_mse        0.081876\n",
    "loss_w_reg      0.998885\n",
    "F1@10           0.267448\n",
    "percision@10    0.615500\n",
    "recall@10       0.192869\n",
    "F1@30           0.275906\n",
    "percision@30    0.337767\n",
    "recall@30       0.288057\n",
    "F1@50           0.250726\n",
    "percision@50    0.245280\n",
    "recall@50       0.333993\n",
    "ndcg@10         0.706146\n",
    "ndcg@30         0.613245\n",
    "ndcg@50         0.601412\n",
    "5. momentum\n",
    "loss_mse        0.001852\n",
    "loss_w_reg      0.161574\n",
    "F1@10           0.217046\n",
    "percision@10    0.509400\n",
    "recall@10       0.156329\n",
    "F1@30           0.227579\n",
    "percision@30    0.283167\n",
    "recall@30       0.236398\n",
    "F1@50           0.212862\n",
    "percision@50    0.211420\n",
    "recall@50       0.280535\n",
    "ndcg@10         0.621891\n",
    "ndcg@30         0.543631\n",
    "ndcg@50         0.536357"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1dfba8",
   "metadata": {},
   "source": [
    "* w/o weight sum\n",
    "    * 5000 epoch 0.587, lr=0.005\n",
    "* w/o L1\n",
    "    * 2000 epoch 0.617, lr=0.01\n",
    "* w/o clip\n",
    "    * 5000 epoch 0.601, lr=0.005\n",
    "* w/o momentum\n",
    "    * 5000 epoch 0.536, lr=0.003\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2115b07e",
   "metadata": {},
   "source": [
    "## Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658c77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select doc_id and k\n",
    "doc_id = 36\n",
    "topk = 30\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31562cf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import colored\n",
    "from colored import stylize\n",
    "\n",
    "word_list = vocab.itos\n",
    "\n",
    "gt = [word_list[word_idx] for word_idx in np.argsort(weight_ans[400+doc_id])[::-1][:topk]]\n",
    "pred = [word_list[word_idx] for word_idx in np.argsort(model.emb.cpu().weight.data[doc_id].numpy())[::-1][:topk]]\n",
    "\n",
    "print('ground truth')\n",
    "for word in gt:\n",
    "    if word in pred:\n",
    "        print(stylize(word, colored.bg(\"yellow\")), end=' ')\n",
    "    else:\n",
    "        print(word, end=' ')\n",
    "\n",
    "print()\n",
    "print('\\nprediction')\n",
    "for word in pred:\n",
    "    if word in gt:\n",
    "        print(stylize(word, colored.bg(\"yellow\")), end=' ')\n",
    "    else:\n",
    "        print(word, end=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fa0add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw document\n",
    "print()\n",
    "ps = PorterStemmer()\n",
    "    \n",
    "for word in vocab.raw_documents[400+doc_id].split():\n",
    "    word_stem = ps.stem(word).lower()\n",
    "\n",
    "    if word_stem in gt:\n",
    "        if word_stem in pred:\n",
    "            print(stylize(word, colored.bg(\"yellow\")), end=' ')\n",
    "        else:\n",
    "            print(stylize(word, colored.bg(\"light_gray\")), end=' ')\n",
    "    else:\n",
    "        print(word, end=' ')\n",
    "# print(dataset.documents[doc_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc21f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "   \n",
    "scores = np.array(model.emb.weight.data)[doc_id].reshape(1, -1)\n",
    "true_relevance = train_loader.dataset.weight_ans[400+doc_id].reshape(1, -1)\n",
    "\n",
    "results['ndcg@50'] = (ndcg_score(true_relevance, scores, k=50))\n",
    "results['ndcg@100'] = (ndcg_score(true_relevance, scores, k=100))\n",
    "results['ndcg@200'] = (ndcg_score(true_relevance, scores, k=200))\n",
    "results['ndcg@all'] = (ndcg_score(true_relevance, scores, k=None))\n",
    "\n",
    "print('This document ndcg:')\n",
    "print('ground truth length:', np.sum(weight_ans[400+doc_id] > 0))\n",
    "print('NDCG top50', results['ndcg@50'])\n",
    "print('NDCG top100', results['ndcg@100'])\n",
    "print('NDCG top200', results['ndcg@200'])\n",
    "print('NDCG ALL', results['ndcg@all'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae49eac",
   "metadata": {},
   "source": [
    "## Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48834bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_notebook = in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb5ecc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "t = time.localtime()\n",
    "t = time.strftime(\"%Y-%m-%d_%H:%M:%S\", t)\n",
    "\n",
    "final_results_df = pd.DataFrame(final_results).reset_index(drop=True)\n",
    "\n",
    "experiment_dir = './records/dataset-{}-n_document-{}-wdist-{}-filtertopk-{}-dim-{}-time-{}'.format(\n",
    "                                        config['dataset'],\n",
    "                                        config['n_document'],\n",
    "                                        config[\"document_vector_agg_weight\"],\n",
    "                                        config[\"topk_word_freq_threshold\"],\n",
    "                                        config[\"embedding_file\"].split('.')[-2][:-1],\n",
    "                                        t)\n",
    "\n",
    "print('Saving to directory', experiment_dir)\n",
    "os.makedirs(experiment_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b704cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_df.to_csv(os.path.join(experiment_dir, 'result.csv'), index=False)\n",
    "\n",
    "import json\n",
    "with open(os.path.join(experiment_dir, 'config.json'), 'w') as json_file:\n",
    "    json.dump(config, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e6cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in final_results_df.set_index('model').columns:\n",
    "    plt.bar(final_results_df['model'],\n",
    "            final_results_df[feat], \n",
    "            width=0.5, \n",
    "            bottom=None, \n",
    "            align='center', \n",
    "            color=['lightsteelblue', \n",
    "                   'cornflowerblue', \n",
    "                   'royalblue', \n",
    "                   'navy'])\n",
    "    plt.title(feat)\n",
    "    plt.savefig(os.path.join(experiment_dir, '{}.png'.format(feat)))\n",
    "    plt.clf()\n",
    "    if is_notebook:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c679b721",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(final_results_df)\n",
    "final_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb3b503",
   "metadata": {},
   "source": [
    "## Basic pursuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01441938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(word_vectors.shape)\n",
    "# print(document_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcacfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spgl1\n",
    "\n",
    "# results = []\n",
    "# sigma = 1e-3\n",
    "\n",
    "# for doc_id, doc_emb in enumerate(tqdm(document_vectors[:train_size])):\n",
    "#     d = doc_emb\n",
    "#     ans = weight_ans[doc_id]\n",
    "#     coef, _, _, info = spgl1.spg_bpdn(A=word_vectors.T, b=d, sigma=sigma)\n",
    "\n",
    "#     res = evaluate_sklearn(coef, ans)\n",
    "#     results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0364e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = pd.DataFrame(results).mean()\n",
    "# results['model'] = 'SPG1'\n",
    "# final_results.append(results)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90f7fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spgl1\n",
    "\n",
    "# results = []\n",
    "\n",
    "# for doc_id, doc_emb in enumerate(tqdm(document_vectors[:train_size])):\n",
    "#     d = doc_emb\n",
    "#     ans = weight_ans[doc_id]\n",
    "#     coef, _, _, info = spgl1.spg_bp(A=word_vectors.T, b=d)\n",
    "\n",
    "#     res = evaluate_sklearn(coef, ans)\n",
    "#     results.append(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7821d96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = pd.DataFrame(results).mean()\n",
    "# results['model'] = 'SPG1'\n",
    "# final_results.append(results)\n",
    "# results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
