{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b33ac182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dhome/casimir0304/miniconda3/envs/ML/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import nltk\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from model.ide_topic_decoder import IDEDataset, IDETopicDecoder\n",
    "from utils.toolbox import same_seeds, show_settings, record_settings, get_preprocess_document_embs, get_preprocess_document_labels, get_word_embs\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.set_num_threads(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec65c827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Info ---------\n",
      "model: ZTM\n",
      "architecture: after\n",
      "activation: sigmoid\n",
      "dataset: tweet\n",
      "dataset_name: tweet\n",
      "vocabulary_size: 100\n",
      "encoder: bert\n",
      "target: tf-idf\n",
      "topic_num: 50\n",
      "seed: 123\n",
      "epochs: 10\n",
      "lr: 0.0001\n",
      "loss: listnet\n",
      "batch_size: 8\n",
      "weight_decay: 0\n",
      "ratio: 0.8\n",
      "topk: [10, 30, 50]\n",
      "save: False\n",
      "threshold: 0.7\n",
      "\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'model': 'ZTM',\n",
    "    'architecture': 'after',\n",
    "    'activation': 'sigmoid',\n",
    "    'dataset': 'tweet',\n",
    "    'dataset_name': 'tweet',\n",
    "    'vocabulary_size':100,\n",
    "    'encoder': 'bert',\n",
    "    'target': 'tf-idf',\n",
    "    'topic_num': 50,\n",
    "    'seed': 123,\n",
    "    'epochs': 10,\n",
    "    'lr': 1e-4,\n",
    "    'loss': 'listnet',\n",
    "    'batch_size': 8,\n",
    "    'weight_decay': 0,\n",
    "    'ratio': 0.8,\n",
    "    'topk': [10, 30, 50],\n",
    "    'save': False,\n",
    "    'threshold': 0.7,\n",
    "}\n",
    "\n",
    "show_settings(config)\n",
    "same_seeds(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2538e840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting preprocess documents: tweet\n",
      "min_df: 1 max_df: 1.0 vocabulary_size: 100 min_doc_word: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (/dhome/casimir0304/.cache/huggingface/datasets/tweet_eval/emotion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "unpreprocessed_corpus ,preprocessed_corpus = get_preprocess_document(**config)\n",
    "texts = [text.split() for text in preprocessed_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efcefe0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting preprocess documents embeddings\n",
      "Using cuda 3 for training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /dhome/casimir0304/.cache/torch/sentence_transformers/bert-base-uncased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /dhome/casimir0304/.cache/torch/sentence_transformers/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d609dc52e51452685131e04129c3e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generating document embedding\n",
    "doc_embs, doc_model, device = get_preprocess_document_embs(preprocessed_corpus, config['encoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6858bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6bade19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting preprocess documents labels\n"
     ]
    }
   ],
   "source": [
    "# Decode target & Vocabulary\n",
    "labels, vocabularys= get_preprocess_document_labels(preprocessed_corpus)\n",
    "id2token = {k: v for k, v in zip(range(0, len(vocabularys[config['target']])), vocabularys[config['target']])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2a97003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    }
   ],
   "source": [
    "print(len(vocabularys[config['target']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0abf1a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b617b1f40174001a4cfd267c0ce9dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:400000\n",
      "Getting [tensor] word embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/IDE/casimir0304/MSLAB_VectorDecomposition/casimir/../utils/toolbox.py:418: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  word_embs = torch.Tensor(word_embs)\n"
     ]
    }
   ],
   "source": [
    "# word embedding preparation\n",
    "word_embeddings = get_word_embs(vocabularys[config['target']], data_type='tensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1dec5ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import datetime\n",
    "import wordcloud\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(\"./\")\n",
    "from utils.loss import ListNet, MythNet\n",
    "from utils.eval import retrieval_normalized_dcg_all, retrieval_precision_all, semantic_precision_all, retrieval_precision_all_v2, semantic_precision_all_v2\n",
    "from utils.eval_topic import CoherenceNPMI, TopicDiversity, InvertedRBO\n",
    "from utils.toolbox import get_free_gpu, record_settings\n",
    "from model.inference_network import ContextualInferenceNetwork\n",
    "\n",
    "class IDEDataset(Dataset):\n",
    "    def __init__(self, corpus, emb, target):\n",
    "        \n",
    "        assert len(emb) == len(target)\n",
    "        self.corpus = corpus\n",
    "        self.emb = torch.FloatTensor(emb)\n",
    "        self.target = torch.FloatTensor(target)        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.corpus[idx], self.emb[idx], self.target[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emb)\n",
    "\n",
    "class DecoderNetwork(nn.Module):\n",
    "    def __init__(self, config, device, vocab_size, contextual_size, glove_word_embeddings, n_components, hidden_sizes=(100,100), activation='softplus', dropout=0.2, learn_priors=True):\n",
    "        super(DecoderNetwork, self).__init__()\n",
    "\n",
    "        assert activation in ['softplus', 'relu']\n",
    "\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.vocab_size = vocab_size\n",
    "        self.contextual_size = contextual_size\n",
    "        self.glove_word_embeddings = glove_word_embeddings\n",
    "        self.n_components = n_components\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.learn_priors = learn_priors\n",
    "        self.topic_word_matrix = None\n",
    "\n",
    "        # decoder architecture\n",
    "        self.batch_norm = nn.BatchNorm1d(vocab_size)\n",
    "        self.word_embedding =  nn.Parameter(torch.randn(vocab_size*4, vocab_size))\n",
    "        self.full_decoder = nn.Sequential(\n",
    "            nn.Linear(vocab_size, vocab_size*4),\n",
    "            nn.BatchNorm1d(vocab_size*4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(vocab_size*4, vocab_size),\n",
    "            nn.BatchNorm1d(vocab_size),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.half_decoder_tanh = nn.Sequential(\n",
    "            nn.Linear(vocab_size+contextual_size, vocab_size*4),\n",
    "            nn.BatchNorm1d(vocab_size*4),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.2),\n",
    "        )\n",
    "        self.half_decoder_sigmoid = nn.Sequential(\n",
    "            nn.Linear(vocab_size+contextual_size, vocab_size*4),\n",
    "            nn.BatchNorm1d(vocab_size*4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(p=0.2),\n",
    "        )\n",
    "        self.share_wieght_decoder = nn.Sequential(\n",
    "            nn.Linear(contextual_size, contextual_size*4),\n",
    "            nn.BatchNorm1d(contextual_size*4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(contextual_size*4, vocab_size),\n",
    "            nn.BatchNorm1d(vocab_size),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.glove_emb_decoder = nn.Sequential(\n",
    "            nn.Linear(vocab_size+contextual_size, vocab_size*4),\n",
    "            nn.BatchNorm1d(vocab_size*4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(vocab_size*4, glove_word_embeddings.shape[1]),\n",
    "            nn.BatchNorm1d(glove_word_embeddings.shape[1]),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        # topic model architecture\n",
    "        self.inf_net = ContextualInferenceNetwork(vocab_size, contextual_size, n_components, hidden_sizes, activation, label_size=0)\n",
    "        \n",
    "        topic_prior_mean = 0.0\n",
    "        self.prior_mean = torch.tensor([topic_prior_mean] * n_components).to(device)\n",
    "        if self.learn_priors:\n",
    "            self.prior_mean = nn.Parameter(self.prior_mean)\n",
    "\n",
    "        topic_prior_variance = 1. - (1. / self.n_components)\n",
    "        self.prior_variance = torch.tensor([topic_prior_variance] * n_components).to(device)\n",
    "        if self.learn_priors:\n",
    "            self.prior_variance = nn.Parameter(self.prior_variance)\n",
    "\n",
    "        self.beta = torch.Tensor(n_components, vocab_size).to(device)\n",
    "        self.beta = nn.Parameter(self.beta)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.beta)\n",
    "        \n",
    "        self.beta_batchnorm = nn.BatchNorm1d(vocab_size, affine=False)\n",
    "        \n",
    "        self.drop_theta = nn.Dropout(p=self.dropout)\n",
    "    \n",
    "    @staticmethod\n",
    "    def reparameterize(mu, logvar):\n",
    "        \"\"\"Reparameterize the theta distribution.\"\"\"\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def forward(self, emb, target, labels=None):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        posterior_mu, posterior_log_sigma = self.inf_net(target, emb, labels)\n",
    "        posterior_sigma = torch.exp(posterior_log_sigma)\n",
    "\n",
    "        # generate samples from theta\n",
    "        theta = F.softmax(self.reparameterize(posterior_mu, posterior_log_sigma), dim=1)\n",
    "        theta = self.drop_theta(theta)\n",
    "\n",
    "        # prodLDA\n",
    "        # in: batch_size x input_size x n_components\n",
    "        word_dist = F.softmax(self.beta_batchnorm(torch.matmul(theta, self.beta)), dim=1)\n",
    "        # word_dist: batch_size x input_size\n",
    "        self.topic_word_matrix = self.beta\n",
    "        \n",
    "        if self.config['activation'] == 'tanh':\n",
    "            emb_word_dist = torch.cat((word_dist, emb), dim=1)\n",
    "            decoded_word_dist = self.half_decoder_tanh(emb_word_dist)\n",
    "            recon_dist = torch.sigmoid(self.batch_norm((torch.matmul(decoded_word_dist, self.word_embedding))))\n",
    "        else:\n",
    "            emb_word_dist = torch.cat((word_dist, emb), dim=1)\n",
    "            decoded_word_dist = self.half_decoder_sigmoid(emb_word_dist)\n",
    "            recon_dist = torch.sigmoid(self.batch_norm((torch.matmul(decoded_word_dist, self.word_embedding))))\n",
    "\n",
    "        return self.prior_mean, self.prior_variance, posterior_mu, posterior_sigma, posterior_log_sigma, word_dist, recon_dist\n",
    "    \n",
    "    def get_theta(self, target, emb, labels=None):\n",
    "        with torch.no_grad():\n",
    "            posterior_mu, posterior_log_sigma = self.inf_net(target, emb, labels)\n",
    "            theta = F.softmax(self.reparameterize(posterior_mu, posterior_log_sigma), dim=1)\n",
    "\n",
    "            return theta\n",
    "\n",
    "class IDETopicDecoder:\n",
    "    def __init__(self, config, texts=None, vocab = None, idx2token=None, device=None, contextual_size=768, word_embeddings=None, \n",
    "                n_components=10, hidden_sizes=(100, 100), activation='softplus', dropout=0.2, learn_priors=True,\n",
    "                momentum=0.99, reduce_on_plateau=False, num_data_loader_workers=mp.cpu_count(), loss_weights=None):\n",
    "        self.config = config\n",
    "        self.texts = texts\n",
    "        self.vocab = vocab\n",
    "        self.idx2token = idx2token\n",
    "        self.device = device\n",
    "        self.contextual_size = contextual_size\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.n_components = n_components\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.learn_priors = learn_priors\n",
    "        self.reduce_on_plateau = reduce_on_plateau\n",
    "        self.momentum = momentum\n",
    "        self.num_data_loader_workers = num_data_loader_workers\n",
    "        self.training_doc_topic_distributions = None\n",
    "        self.distribution_cache = None\n",
    "        self.num_epochs = config['epochs']\n",
    "        if config['loss'] == 'mse':\n",
    "            self.loss_funct = torch.nn.MSELoss(reduction='mean')\n",
    "        else:\n",
    "             self.loss_funct = MythNet\n",
    "        if loss_weights:\n",
    "            self.weights = loss_weights\n",
    "        else:\n",
    "            self.weights = {\"beta\": 1}\n",
    "\n",
    "        self.model = DecoderNetwork(\n",
    "                    config, device, len(vocab), contextual_size, word_embeddings, n_components, hidden_sizes, activation,\n",
    "                    dropout, learn_priors)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(\n",
    "                        self.model.parameters(), lr=config['lr'], betas=(self.momentum, 0.99), weight_decay=config['weight_decay'])\n",
    "\n",
    "        if self.reduce_on_plateau:\n",
    "            self.scheduler = ReduceLROnPlateau(self.optimizer, patience=10)\n",
    "        \n",
    "        self.best_components = None\n",
    "\n",
    "    def loss(self, inputs, word_dists, recon_dists, prior_mean, prior_variance,\n",
    "              posterior_mean, posterior_variance, posterior_log_variance):\n",
    "        var_division = torch.sum(posterior_variance / prior_variance, dim=1)\n",
    "        diff_means = prior_mean - posterior_mean\n",
    "        diff_term = torch.sum((diff_means * diff_means) / prior_variance, dim=1)\n",
    "        logvar_det_division = prior_variance.log().sum() - posterior_log_variance.sum(dim=1)\n",
    "\n",
    "        KL = 0.5 * (var_division + diff_term - self.n_components + logvar_det_division)\n",
    "\n",
    "        RL = torch.sum(-inputs * torch.log(word_dists + 1e-10), dim=1)\n",
    "\n",
    "        DL = self.loss_funct(recon_dists, inputs)\n",
    "\n",
    "        return KL, RL, DL\n",
    "\n",
    "    def training(self, loader):\n",
    "        \"\"\"Train epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        samples_processed = 0\n",
    "\n",
    "        for batch, (corpus, emb, target) in enumerate(loader):\n",
    "            target = target.reshape(target.shape[0], -1)\n",
    "            emb, target = emb.to(device), target.to(device)\n",
    "\n",
    "            self.model.zero_grad()\n",
    "            prior_mean, prior_variance, posterior_mean, posterior_variance,\\\n",
    "            posterior_log_variance, word_dists, recon_dists = self.model(emb, target)\n",
    "\n",
    "            kl_loss, rl_loss, dl_loss = self.loss(\n",
    "                target, word_dists, recon_dists, prior_mean, prior_variance,\n",
    "                posterior_mean, posterior_variance, posterior_log_variance)\n",
    "            loss = self.weights[\"beta\"] * kl_loss + rl_loss + dl_loss\n",
    "            loss = loss.sum()\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            samples_processed += target.size()[0]\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= samples_processed\n",
    "\n",
    "        return samples_processed, train_loss\n",
    "    \n",
    "    def validation(self, loader):\n",
    "        \"\"\"Validation epoch.\"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        samples_processed = 0\n",
    "\n",
    "        results = defaultdict(list)\n",
    "        dists = defaultdict(list)\n",
    "\n",
    "        for batch, (corpus, emb, target) in enumerate(loader):\n",
    "            target = target.reshape(target.shape[0], -1)\n",
    "            emb, target = emb.to(device), target.to(device)\n",
    "\n",
    "            self.model.zero_grad()\n",
    "            prior_mean, prior_variance, posterior_mean, posterior_variance,\\\n",
    "            posterior_log_variance, word_dists, recon_dists = self.model(emb, target)\n",
    "            \n",
    "            kl_loss, rl_loss, dl_loss = self.loss(target, word_dists, recon_dists, prior_mean, prior_variance,\n",
    "                              posterior_mean, posterior_variance, posterior_log_variance)\n",
    "\n",
    "            loss = self.weights[\"beta\"] * kl_loss + rl_loss + dl_loss\n",
    "            loss = loss.sum()\n",
    "\n",
    "            samples_processed += target.size()[0]\n",
    "            val_loss += loss.item()\n",
    "\n",
    "             # Semantic Prcision for reconstruct\n",
    "            precision_scores, word_result = semantic_precision_all(recon_dists, target, self.word_embeddings, self.vocab, k=self.config['topk'], th = self.config['threshold'])\n",
    "            for k, v in precision_scores.items():\n",
    "                results['[Recon] Semantic Precision v1@{}'.format(k)].append(v)\n",
    "\n",
    "            precision_scores, word_result = semantic_precision_all_v2(recon_dists, target, self.word_embeddings, self.vocab, k=self.config['topk'], th = self.config['threshold'])\n",
    "            for k, v in precision_scores.items():\n",
    "                results['[Recon] Semantic Precision v2@{}'.format(k)].append(v)\n",
    "                \n",
    "            # Precision for reconstruct\n",
    "            precision_scores = retrieval_precision_all(recon_dists, target, k=self.config['topk'])\n",
    "            for k, v in precision_scores.items():\n",
    "                results['[Recon] Precision v1@{}'.format(k)].append(v)\n",
    "            \n",
    "            precision_scores = retrieval_precision_all_v2(recon_dists, target, k=self.config['topk'])\n",
    "            for k, v in precision_scores.items():\n",
    "                results['[Recon] Precision v2@{}'.format(k)].append(v)\n",
    "\n",
    "            # NDCG for reconstruct\n",
    "            ndcg_scores = retrieval_normalized_dcg_all(recon_dists, target, k=self.config['topk'])\n",
    "            for k, v in ndcg_scores.items():\n",
    "                results['[Recon] ndcg@{}'.format(k)].append(v)\n",
    "\n",
    "            # Semantic Prcision for word dist\n",
    "            precision_scores, word_result = semantic_precision_all(word_dists, target, self.word_embeddings, self.vocab, k=self.config['topk'], th = self.config['threshold'])\n",
    "            for k, v in precision_scores.items():\n",
    "                dists['[Word Dist] Semantic Precision v1@{}'.format(k)].append(v)\n",
    "\n",
    "            precision_scores, word_result = semantic_precision_all_v2(word_dists, target, self.word_embeddings, self.vocab, k=self.config['topk'], th = self.config['threshold'])\n",
    "            for k, v in precision_scores.items():\n",
    "                dists['[Word Dist] Semantic Precision v2@{}'.format(k)].append(v)\n",
    "                \n",
    "            # Precision for word dist\n",
    "            precision_scores = retrieval_precision_all(word_dists, target, k=self.config['topk'])\n",
    "            for k, v in precision_scores.items():\n",
    "                dists['[Word Dist] Precision v1@{}'.format(k)].append(v)\n",
    "\n",
    "            precision_scores = retrieval_precision_all_v2(word_dists, target, k=self.config['topk'])\n",
    "            for k, v in precision_scores.items():\n",
    "                dists['[Word Dist] Precision v2@{}'.format(k)].append(v)\n",
    "\n",
    "            # NDCG for word dist\n",
    "            ndcg_scores = retrieval_normalized_dcg_all(word_dists, target, k=self.config['topk'])\n",
    "            for k, v in ndcg_scores.items():\n",
    "                dists['[Word Dist] ndcg@{}'.format(k)].append(v)\n",
    "        \n",
    "        for k in results:\n",
    "            results[k] = np.mean(results[k])\n",
    "        \n",
    "        for k in dists:\n",
    "            dists[k] = np.mean(dists[k])\n",
    "\n",
    "        val_loss /= samples_processed\n",
    "\n",
    "        return samples_processed, val_loss, results, dists\n",
    "\n",
    "    def fit(self, training_set, validation_set, n_samples=20):\n",
    "        self.model.to(self.device)\n",
    "        train_loader = DataLoader(training_set, batch_size=self.config['batch_size'], shuffle=True, num_workers=self.num_data_loader_workers)\n",
    "        validation_loader = DataLoader(validation_set, batch_size=self.config['batch_size'], shuffle=True, num_workers=self.num_data_loader_workers)\n",
    "        \n",
    "        train_loss = 0\n",
    "        samples_processed = 0\n",
    "        \n",
    "        pbar = tqdm(self.num_epochs, position=0, leave=True)\n",
    "        record_settings(self.config)\n",
    "\n",
    "        for epoch in range(self.config['epochs']):\n",
    "            s = datetime.datetime.now()\n",
    "            sp, train_loss = self.training(train_loader)\n",
    "            samples_processed += sp\n",
    "            e = datetime.datetime.now()\n",
    "            pbar.update(1)\n",
    "\n",
    "            if  (epoch + 1) % 10 == 0:\n",
    "                s = datetime.datetime.now()\n",
    "                val_samples_processed, val_loss, val_res, dist_res = self.validation(validation_loader)\n",
    "                e = datetime.datetime.now()\n",
    "\n",
    "                pbar.set_description(\"Epoch: [{}/{}]\\t Seen Samples: [{}/{}]\\tTrain Loss: {}\\tValid Loss: {}\\tTime: {}\".format(\n",
    "                    epoch + 1, self.num_epochs, samples_processed,\n",
    "                    len(training_set) * self.num_epochs, train_loss, val_loss, e - s))\n",
    "                \n",
    "                npmi = CoherenceNPMI(texts=self.texts, topics=self.get_topic_lists(10))\n",
    "                diversity = InvertedRBO(topics=self.get_topic_lists(10))\n",
    "                # record = open('./'+self.config['experiment']+'_'+self.config['dataset']+'_'+self.config['model']+'_'+self.config['architecture']+'_'+self.config['activation']+'_'+self.config['encoder']+'_'+self.config['target']+'_loss_'+self.config['loss']+'_lr'+str(self.config['lr'])+'_batch'+str(self.config['batch_size'])+'_weightdecay'+str(self.config['weight_decay'])+'.txt', 'a')\n",
    "                print('---------------------------------------')\n",
    "                # record.write('-------------------------------------------------\\n')\n",
    "                print('EPOCH', epoch + 1)\n",
    "                # record.write('EPOCH '+ str(epoch + 1) + '\\n')\n",
    "                for key,val in val_res.items():\n",
    "                    print(f\"{key}:{val:.4f}\")\n",
    "                    # record.write(f\"{key}:{val:.4f}\\n\")\n",
    "                for key,val in dist_res.items():\n",
    "                    print(f\"{key}:{val:.4f}\")\n",
    "                    # record.write(f\"{key}:{val:.4f}\\n\")\n",
    "                print('NPMI: ', npmi.score())\n",
    "                print('IRBO: ', diversity.score())\n",
    "                # record.write('NPMI: '+ str(npmi.score()) + '\\n')\n",
    "                # record.write('IRBO: '+ str(diversity.score()) + '\\n')\n",
    "\n",
    "            self.best_components = self.model.beta\n",
    "            pbar.set_description(\"Epoch: [{}/{}]\\t Seen Samples: [{}/{}]\\tTrain Loss: {}\\tTime: {}\".format(\n",
    "                epoch + 1, self.num_epochs, samples_processed,\n",
    "                len(training_set) * self.num_epochs, train_loss, e - s))\n",
    "        pbar.close()\n",
    "    \n",
    "    def get_topic_lists(self, k=10):\n",
    "        \"\"\"\n",
    "        Retrieve the lists of topic words.\n",
    "\n",
    "        :param k: (int) number of words to return per topic, default 10.\n",
    "        \"\"\"\n",
    "        assert k <= len(self.vocab), \"k must be <= input size.\"\n",
    "        # TODO: collapse this method with the one that just returns the topics\n",
    "        component_dists = self.best_components\n",
    "        topics = []\n",
    "        for i in range(self.n_components):\n",
    "            _, idxs = torch.topk(component_dists[i], k)\n",
    "            component_words = [self.idx2token[idx]\n",
    "                               for idx in idxs.cpu().numpy()]\n",
    "            topics.append(component_words)\n",
    "        return topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4017e232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "dataset = IDEDataset(unpreprocessed_corpus, doc_embs, labels[config['target']])\n",
    "training_length = int(len(dataset) * config['ratio'])\n",
    "validation_length = len(dataset) - training_length\n",
    "training_set, validation_set = random_split(dataset, lengths=[training_length, validation_length],generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a062bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [5/10]\t Seen Samples: [2410/4820]\tTrain Loss: 20.43576637046466\tTime: 0:00:13.134904: : 5it [01:07, 13.31s/it] "
     ]
    }
   ],
   "source": [
    "model = IDETopicDecoder(config, texts=texts, vocab = vocabularys[config['target']], idx2token=id2token, device=device, contextual_size=doc_embs.shape[1], word_embeddings=word_embeddings)\n",
    "model.fit(training_set, validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf815cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "doc_idx = []\n",
    "print(len(validation_set))\n",
    "for idx in range(200):\n",
    "    doc_idx.append(random.randint(0, len(validation_set)))\n",
    "print(doc_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fca275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [5/20]: : 5it [00:25,  5.08s/it]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "# visualize documents\n",
    "check_nums = 10\n",
    "for idx in doc_idx:\n",
    "    # get recontruct result\n",
    "    recon_list, target_list, doc_list = model.get_reconstruct(validation_set)\n",
    "\n",
    "    # get ranking index\n",
    "    recon_rank_list = np.zeros((len(recon_list), len(tp.vocab)), dtype='float32')\n",
    "    target_rank_list = np.zeros((len(recon_list), len(tp.vocab)), dtype='float32')\n",
    "    for i in range(len(recon_list)):\n",
    "        recon_rank_list[i] = np.argsort(recon_list[i])[::-1]\n",
    "        target_rank_list[i] = np.argsort(target_list[i])[::-1]\n",
    "\n",
    "        # show info\n",
    "    doc_topics_distribution = model.get_doc_topic_distribution(validation_set)\n",
    "    doc_topics = model.get_topic_lists()[np.argmax(doc_topics_distribution[idx])]\n",
    "    print('Documents ', idx)\n",
    "    print(doc_list[idx])\n",
    "    print('---------------------------------------')\n",
    "    print('Topic of Document: ')\n",
    "    print(doc_topics)\n",
    "    print('---------------------------------------')\n",
    "    print('[Predict] Top 10 Words in Document: ')\n",
    "    for word_idx in range(10):\n",
    "        print(dataset.idx2token[recon_rank_list[idx][word_idx]])\n",
    "    print('---------------------------------------')\n",
    "    print('[Label] Top 10 Words in Document: ')\n",
    "    for idx in range(10):\n",
    "        print(dataset.idx2token[target_rank_list[idx][word_idx]])\n",
    "        print('---------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c87640c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_list, target_list, doc_list = model.get_reconstruct(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "420270c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3770, 2000)\n"
     ]
    }
   ],
   "source": [
    "print(target_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c61672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "recon_list = recon_list\n",
    "recon_rank_list = np.zeros((len(recon_list), len(tp.vocab)), dtype='float32')\n",
    "target_rank_list = np.zeros((len(recon_list), len(tp.vocab)), dtype='float32')\n",
    "for i in range(len(recon_list)):\n",
    "        recon_rank_list[i] = np.argsort(recon_list[i])[::-1]\n",
    "        target_rank_list[i] = np.argsort(target_list[i])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99d25b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 179. 1844. 1907. ...  491.  577.  979.]\n"
     ]
    }
   ],
   "source": [
    "doc_idx = 1698\n",
    "print(recon_rank_list[doc_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92484660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 950. 1379.   76. ... 1402.  709. 1320.]\n",
      " [ 107.  310.  793. ... 1815. 1387. 1539.]\n",
      " [1072. 1269. 1006. ...  980.  893.   46.]\n",
      " ...\n",
      " [ 761. 1626. 1996. ... 1014. 1373. 1739.]\n",
      " [ 670.  693.  865. ...  582.   46.  772.]\n",
      " [ 290.  928.  904. ... 1205.  476.  463.]]\n"
     ]
    }
   ],
   "source": [
    "print(recon_rank_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c65d854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: dennisk@cs.uoregon.edu (Dennis Kennedy)\n",
      "Subject: '72 Chevelle SS forsale\n",
      "Organization: University of Oregon\n",
      "Lines: 11\n",
      "Distribution: usa\n",
      "NNTP-Posting-Host: fp2-cc-25.uoregon.edu\n",
      "\n",
      "I don't want to sell this car, but I need money for college.\n",
      "1972 Chevelle Super Sport\n",
      "Rebuilt 402, four speed, 12 Bolt positrac\n",
      "Numbers match\n",
      "110,000 original miles\n",
      "no rust\n",
      "Looks and runs excellent\n",
      "$5995 or best offer.\n",
      "Call Dennis at (503)343-3759\n",
      "or email dennisk@cs.uoregon.edu\n"
     ]
    }
   ],
   "source": [
    "print(doc_list[doc_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e48338b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already\n",
      "tv\n",
      "video\n",
      "late\n",
      "card\n",
      "display\n",
      "3t\n",
      "1t\n",
      "asked\n",
      "games\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "    print(dataset.idx2token[recon_rank_list[doc_idx][idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4e5f982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs\n",
      "72\n",
      "miles\n",
      "excellent\n",
      "runs\n",
      "numbers\n",
      "offer\n",
      "sell\n",
      "four\n",
      "looks\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "    print(dataset.idx2token[target_rank_list[doc_idx][idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e42c4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['windows',\n",
       "  'drive',\n",
       "  'card',\n",
       "  'disk',\n",
       "  'help',\n",
       "  'mac',\n",
       "  'dos',\n",
       "  'mouse',\n",
       "  'problem',\n",
       "  'pc'],\n",
       " ['god',\n",
       "  'jesus',\n",
       "  'sin',\n",
       "  'rutgers',\n",
       "  'christ',\n",
       "  'faith',\n",
       "  'athos',\n",
       "  'truth',\n",
       "  'sandvik',\n",
       "  'church'],\n",
       " ['jpeg',\n",
       "  'edu',\n",
       "  'gif',\n",
       "  'image',\n",
       "  'quality',\n",
       "  'format',\n",
       "  'images',\n",
       "  'get',\n",
       "  'programs',\n",
       "  'color'],\n",
       " ['gov',\n",
       "  'access',\n",
       "  'hst',\n",
       "  'nasa',\n",
       "  'shuttle',\n",
       "  'digex',\n",
       "  'net',\n",
       "  'jpl',\n",
       "  'pat',\n",
       "  'mission'],\n",
       " ['bike', 'dog', 'ca', 'com', 'ride', 'riding', 'dod', 'bnr', 'car', 'bmw'],\n",
       " ['10', '46', 'van', '12', '25', 'nj', '11', '64', '28', '60'],\n",
       " ['ax', 'max', 'giz', 'bhj', 'writes', 'g9v', '75u', 'pl', 'b8f', '2tm'],\n",
       " ['one',\n",
       "  'people',\n",
       "  'would',\n",
       "  'like',\n",
       "  'see',\n",
       "  'even',\n",
       "  'time',\n",
       "  'lord',\n",
       "  'said',\n",
       "  'us'],\n",
       " ['article',\n",
       "  'writes',\n",
       "  'muslims',\n",
       "  'islam',\n",
       "  'turkey',\n",
       "  'edu',\n",
       "  'greek',\n",
       "  'muslim',\n",
       "  'turks',\n",
       "  'turkish'],\n",
       " ['window',\n",
       "  'problem',\n",
       "  'program',\n",
       "  'help',\n",
       "  'nl',\n",
       "  'thanks',\n",
       "  'error',\n",
       "  'create',\n",
       "  'table',\n",
       "  'screen'],\n",
       " ['cx', 'mv', 'ax', '0d', 'mc', 'hz', 'sp', '6um', '6ei', 'sc'],\n",
       " ['04', '02', '03', '06', '05', 'lost', 'edu', '01', '07', '00'],\n",
       " ['com',\n",
       "  'scsi',\n",
       "  'writes',\n",
       "  'bus',\n",
       "  'article',\n",
       "  'ide',\n",
       "  'car',\n",
       "  'isa',\n",
       "  'drive',\n",
       "  'oracle'],\n",
       " ['game',\n",
       "  'ca',\n",
       "  'espn',\n",
       "  'baseball',\n",
       "  'games',\n",
       "  'toronto',\n",
       "  'fan',\n",
       "  'team',\n",
       "  'hockey',\n",
       "  'show'],\n",
       " ['edu',\n",
       "  'image',\n",
       "  'data',\n",
       "  'analysis',\n",
       "  'processing',\n",
       "  'graphics',\n",
       "  'ftp',\n",
       "  '3d',\n",
       "  'gov',\n",
       "  'images'],\n",
       " ['car',\n",
       "  'dod',\n",
       "  'nasa',\n",
       "  'launch',\n",
       "  'space',\n",
       "  'oil',\n",
       "  'bike',\n",
       "  'cars',\n",
       "  'engine',\n",
       "  'driving'],\n",
       " ['com',\n",
       "  'windows',\n",
       "  'server',\n",
       "  'edu',\n",
       "  'os',\n",
       "  'dos',\n",
       "  'window',\n",
       "  'key',\n",
       "  'motif',\n",
       "  'subject'],\n",
       " ['atheism',\n",
       "  'atheists',\n",
       "  'atheist',\n",
       "  'alt',\n",
       "  'universe',\n",
       "  'one',\n",
       "  'people',\n",
       "  'existence',\n",
       "  'religious',\n",
       "  'god'],\n",
       " ['sale',\n",
       "  'offer',\n",
       "  'selling',\n",
       "  'condition',\n",
       "  'ex',\n",
       "  'shipping',\n",
       "  '1988',\n",
       "  'excellent',\n",
       "  'college',\n",
       "  'asking'],\n",
       " ['morality',\n",
       "  'objective',\n",
       "  'frank',\n",
       "  'moral',\n",
       "  'writes',\n",
       "  'values',\n",
       "  'islam',\n",
       "  'god',\n",
       "  'islamic',\n",
       "  'absolute'],\n",
       " ['people',\n",
       "  'one',\n",
       "  'us',\n",
       "  'azerbaijan',\n",
       "  'would',\n",
       "  'went',\n",
       "  'building',\n",
       "  'said',\n",
       "  'armenian',\n",
       "  'armenians'],\n",
       " ['thanks',\n",
       "  'advance',\n",
       "  'hi',\n",
       "  'address',\n",
       "  'looking',\n",
       "  'please',\n",
       "  'anyone',\n",
       "  'ch',\n",
       "  'site',\n",
       "  'ac'],\n",
       " ['writes',\n",
       "  'article',\n",
       "  'clutch',\n",
       "  'players',\n",
       "  'edu',\n",
       "  'year',\n",
       "  'morris',\n",
       "  'player',\n",
       "  'team',\n",
       "  'baseball'],\n",
       " ['ax',\n",
       "  'edu',\n",
       "  'article',\n",
       "  'writes',\n",
       "  'nntp',\n",
       "  'host',\n",
       "  'giz',\n",
       "  'max',\n",
       "  'posting',\n",
       "  'g9v'],\n",
       " ['article',\n",
       "  'writes',\n",
       "  'cramer',\n",
       "  'gay',\n",
       "  'optilink',\n",
       "  'clayton',\n",
       "  'sgi',\n",
       "  'men',\n",
       "  'livesey',\n",
       "  'homosexual'],\n",
       " ['edu',\n",
       "  'article',\n",
       "  'writes',\n",
       "  'insurance',\n",
       "  'drugs',\n",
       "  'guns',\n",
       "  'batf',\n",
       "  'health',\n",
       "  'fbi',\n",
       "  'would'],\n",
       " ['clipper',\n",
       "  'chip',\n",
       "  'escrow',\n",
       "  'encryption',\n",
       "  'keys',\n",
       "  'key',\n",
       "  'nsa',\n",
       "  'phone',\n",
       "  'secret',\n",
       "  'brad'],\n",
       " ['de',\n",
       "  'tu',\n",
       "  'windows',\n",
       "  'au',\n",
       "  'graphics',\n",
       "  'ac',\n",
       "  'files',\n",
       "  'germany',\n",
       "  'using',\n",
       "  'help'],\n",
       " ['article',\n",
       "  'edu',\n",
       "  'writes',\n",
       "  'nntp',\n",
       "  'host',\n",
       "  'posting',\n",
       "  'university',\n",
       "  'scsi',\n",
       "  'car',\n",
       "  'cwru'],\n",
       " ['et', 'body', 'tm', 'koresh', 'james', 'ex', 'food', 'tax', 'tek', 'normal'],\n",
       " ['stephanopoulos',\n",
       "  'mr',\n",
       "  'think',\n",
       "  'president',\n",
       "  'would',\n",
       "  'george',\n",
       "  'jobs',\n",
       "  'package',\n",
       "  'russia',\n",
       "  'know'],\n",
       " ['writes',\n",
       "  'article',\n",
       "  'edu',\n",
       "  'team',\n",
       "  'morris',\n",
       "  'year',\n",
       "  'roger',\n",
       "  'games',\n",
       "  'season',\n",
       "  'game'],\n",
       " ['sale',\n",
       "  'offer',\n",
       "  'condition',\n",
       "  'distribution',\n",
       "  'asking',\n",
       "  'excellent',\n",
       "  'sell',\n",
       "  'trade',\n",
       "  'included',\n",
       "  'price'],\n",
       " ['com',\n",
       "  'writes',\n",
       "  'article',\n",
       "  'br',\n",
       "  'isc',\n",
       "  'stratus',\n",
       "  'sw',\n",
       "  'nntp',\n",
       "  'distribution',\n",
       "  'rocket'],\n",
       " ['soviet',\n",
       "  'serdar',\n",
       "  'argic',\n",
       "  'escape',\n",
       "  'armenian',\n",
       "  'serve',\n",
       "  'mountain',\n",
       "  'turks',\n",
       "  'genocide',\n",
       "  'professor'],\n",
       " ['board',\n",
       "  'card',\n",
       "  'thanks',\n",
       "  'bus',\n",
       "  'drive',\n",
       "  'video',\n",
       "  'driver',\n",
       "  'anyone',\n",
       "  'port',\n",
       "  'pc'],\n",
       " ['baseball',\n",
       "  'virginia',\n",
       "  'buffalo',\n",
       "  'toronto',\n",
       "  'boston',\n",
       "  'night',\n",
       "  'cmu',\n",
       "  'reserve',\n",
       "  'espn',\n",
       "  'college'],\n",
       " ['pp', 'pts', 'period', 'goals', 'goal', 'st', '25', '11', 'play', 'pit'],\n",
       " ['scsi',\n",
       "  'drive',\n",
       "  'controller',\n",
       "  'drives',\n",
       "  'disk',\n",
       "  'ide',\n",
       "  'bios',\n",
       "  'tape',\n",
       "  'rom',\n",
       "  'card'],\n",
       " ['gun',\n",
       "  'people',\n",
       "  'guns',\n",
       "  'would',\n",
       "  'amendment',\n",
       "  'firearms',\n",
       "  'weapons',\n",
       "  'koresh',\n",
       "  'right',\n",
       "  'constitution'],\n",
       " ['sale',\n",
       "  'books',\n",
       "  '00',\n",
       "  'offer',\n",
       "  'please',\n",
       "  'price',\n",
       "  'cd',\n",
       "  'book',\n",
       "  'list',\n",
       "  'condition'],\n",
       " ['god',\n",
       "  'sin',\n",
       "  'church',\n",
       "  'mary',\n",
       "  'bible',\n",
       "  'christ',\n",
       "  'spirit',\n",
       "  'jesus',\n",
       "  'christian',\n",
       "  'paul'],\n",
       " ['com',\n",
       "  'writes',\n",
       "  'article',\n",
       "  'uk',\n",
       "  'convex',\n",
       "  'co',\n",
       "  'crypto',\n",
       "  'escrow',\n",
       "  'key',\n",
       "  'att'],\n",
       " ['privacy',\n",
       "  'cryptography',\n",
       "  'internet',\n",
       "  'mail',\n",
       "  'key',\n",
       "  'security',\n",
       "  'sci',\n",
       "  'secure',\n",
       "  'des',\n",
       "  'anonymous'],\n",
       " ['edu',\n",
       "  'article',\n",
       "  'writes',\n",
       "  'gordon',\n",
       "  'banks',\n",
       "  'keith',\n",
       "  'pitt',\n",
       "  'nntp',\n",
       "  'caltech',\n",
       "  'host'],\n",
       " ['thanks',\n",
       "  'site',\n",
       "  'address',\n",
       "  'sites',\n",
       "  'info',\n",
       "  'anyone',\n",
       "  'looking',\n",
       "  'et',\n",
       "  'ch',\n",
       "  'pre'],\n",
       " ['education',\n",
       "  'cancer',\n",
       "  'un',\n",
       "  'et',\n",
       "  'popular',\n",
       "  'released',\n",
       "  'pre',\n",
       "  'waiting',\n",
       "  'training',\n",
       "  'aids'],\n",
       " ['israel',\n",
       "  'israeli',\n",
       "  'arab',\n",
       "  'jewish',\n",
       "  'jews',\n",
       "  'arabs',\n",
       "  'adam',\n",
       "  'policy',\n",
       "  'attacks',\n",
       "  'peace'],\n",
       " ['edu',\n",
       "  'article',\n",
       "  'writes',\n",
       "  'nntp',\n",
       "  'atf',\n",
       "  'host',\n",
       "  'indiana',\n",
       "  'posting',\n",
       "  'ohio',\n",
       "  'university'],\n",
       " ['health',\n",
       "  'disease',\n",
       "  'medical',\n",
       "  'page',\n",
       "  'patients',\n",
       "  'volume',\n",
       "  'reported',\n",
       "  'age',\n",
       "  'risk',\n",
       "  'use']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_topic_lists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d5706b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [20/20]: : 20it [03:04,  9.25s/it]\n"
     ]
    }
   ],
   "source": [
    "doc_topics_distribution = model.get_doc_topic_distribution(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "256d0fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00264944, 0.00154069, 0.00157822, 0.00240728, 0.00557763,\n",
       "       0.00427778, 0.00276508, 0.00872498, 0.04012846, 0.00356483,\n",
       "       0.00288309, 0.00270965, 0.00298711, 0.00746678, 0.00177612,\n",
       "       0.00248846, 0.00091876, 0.00518265, 0.00199116, 0.01588894,\n",
       "       0.08754831, 0.00166128, 0.00302516, 0.00350954, 0.00638637,\n",
       "       0.00344035, 0.00659486, 0.00204372, 0.00246886, 0.01168864,\n",
       "       0.01562314, 0.00923532, 0.00320664, 0.00575135, 0.02873103,\n",
       "       0.00292833, 0.0047355 , 0.00574315, 0.00327179, 0.00780392,\n",
       "       0.00383813, 0.00213848, 0.00608836, 0.00335248, 0.00340084,\n",
       "       0.00218007, 0.00324257, 0.63300758, 0.00526384, 0.00458329])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topics_distribution[doc_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80cb1748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['israel', 'israeli', 'arab', 'jewish', 'jews', 'arabs', 'adam', 'policy', 'attacks', 'peace']\n"
     ]
    }
   ],
   "source": [
    "doc_topics = model.get_topic_lists()[np.argmax(doc_topics_distribution[doc_idx])]\n",
    "print(doc_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcf56ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        test.append(unpreprocessed_corpus[i+j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0803b9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (/dhome/casimir0304/.cache/huggingface/datasets/tweet_eval/emotion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
      "/data1/IDE/casimir0304/ide/utils/preprocessing.py:24: UserWarning: WhiteSpacePreprocessing is deprecated and will be removed in future versions.Use WhiteSpacePreprocessingStopwords.\n",
      "  warnings.warn(\"WhiteSpacePreprocessing is deprecated and will be removed in future versions.\"\n",
      "/dhome/casimir0304/miniconda3/envs/ML/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "raw_documents = load_document(config['dataset'])[\"documents\"]\n",
    "preprocessed_documents, unpreprocessed_corpus, texts = preprocess_document(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70e3da88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "raw_documents = load_document(config['dataset'])[\"documents\"]\n",
    "preprocessed_documents, unpreprocessed_corpus, texts = preprocess_document(raw_documents)\n",
    "vectorizer = TfidfVectorizer(token_pattern=r'(?u)\\b[\\w+|\\-]+\\b')\n",
    "decode_target = vectorizer.fit_transform(preprocessed_documents)\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "id2token = {k: v for k, v in zip(range(0, len(vocab)), vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f531a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token = {k: v for k, v in zip(range(0, len(vocab)), vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "478f6de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4614, 1719)\n"
     ]
    }
   ],
   "source": [
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31de50f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
