{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b33ac182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dhome/casimir0304/miniconda3/envs/ML/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import nltk\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from model.contextualized_topic_models.models.ctm import ZeroShotTM, CombinedTM\n",
    "from model.contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation, calculate_word_embeddings_tensor, load_word2emb\n",
    "from model.contextualized_topic_models.utils.preprocessing import WhiteSpacePreprocessing\n",
    "from utils.toolbox import same_seeds, show_settings, record_settings, get_preprocess_document, get_preprocess_document_embs, get_preprocess_document_labels, get_word_embs\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.set_num_threads(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec65c827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Info ---------\n",
      "model: ZTM\n",
      "dataset: tweet\n",
      "dataset_name: tweet\n",
      "vocabulary_size: 100\n",
      "encoder: roberta\n",
      "target: tf-idf\n",
      "topic_num: 50\n",
      "seed: 123\n",
      "epochs: 1\n",
      "ratio: 0.8\n",
      "topk: [10, 30, 50]\n",
      "save: False\n",
      "threshold: 0.7\n",
      "\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'model': 'ZTM',\n",
    "    'dataset': 'tweet',\n",
    "    'dataset_name': 'tweet',\n",
    "    'vocabulary_size':100,\n",
    "    'encoder': 'roberta',\n",
    "    'target': 'tf-idf',\n",
    "    'topic_num': 50,\n",
    "    'seed': 123,\n",
    "    'epochs': 1,\n",
    "    'ratio': 0.8,\n",
    "    'topk': [10, 30, 50],\n",
    "    'save': False,\n",
    "    'threshold': 0.7,\n",
    "}\n",
    "\n",
    "show_settings(config)\n",
    "same_seeds(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2538e840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting preprocess documents: tweet\n",
      "min_df: 1 max_df: 1.0 vocabulary_size: 100 min_doc_word: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (/dhome/casimir0304/.cache/huggingface/datasets/tweet_eval/emotion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
      "/dhome/casimir0304/miniconda3/envs/ML/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "unpreprocessed_corpus ,preprocessed_corpus = get_preprocess_document(**config)\n",
    "texts = [text.split() for text in preprocessed_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efcefe0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting preprocess documents embeddings\n",
      "Using cuda 0 for training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026eee8c8da843d5808023646d3d2363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generating document embedding\n",
    "doc_embs, doc_model = get_preprocess_document_embs(preprocessed_corpus, config['encoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6bade19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting preprocess documents labels\n"
     ]
    }
   ],
   "source": [
    "# Decode target & Vocabulary\n",
    "labels, vocabularys= get_preprocess_document_labels(preprocessed_corpus)\n",
    "id2token = {k: v for k, v in zip(range(0, len(vocabularys[config['target']])), vocabularys[config['target']])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4017e232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "tp = TopicModelDataPreparation(contextualized_model=doc_embs, target=config['target'])\n",
    "dataset = tp.fit(text_for_contextual=unpreprocessed_corpus, text_for_bow=preprocessed_corpus, text_for_doc2vec=texts, decode_target=labels[config['target']], vocab=vocabularys[config['target']], id2token=id2token)\n",
    "training_length = int(len(dataset) * config['ratio'])\n",
    "validation_length = len(dataset) - training_length\n",
    "training_set, validation_set = random_split(dataset, lengths=[training_length, validation_length],generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2a97003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "print(len(tp.vocab))\n",
    "print(len(vocabularys[config['target']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0abf1a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75adea22ebc143b889859e12ed53a651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:400000\n"
     ]
    }
   ],
   "source": [
    "# word embedding preparation\n",
    "word_embeddings = calculate_word_embeddings_tensor(load_word2emb(\"../data/glove.6B.300d.txt\"), tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9ac5ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define document embeddings dimension\n",
    "if config['encoder'] == 'doc2vec':\n",
    "    contextual_size = 200\n",
    "elif config['encoder'] == 'average':\n",
    "    contextual_size = 300\n",
    "else:\n",
    "    contextual_size = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a062bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda 2 for training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1/1]\t Seen Samples: [482/482]\tTrain Loss: 53.37462847064639\tTime: 0:00:04.788851: : 1it [00:11,  6.80s/it]                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "EPOCH 1\n",
      "[Recon] Semantic Precision@10:0.1832\n",
      "[Recon] Semantic Precision@30:0.0932\n",
      "[Recon] Semantic Precision@50:0.0605\n",
      "[Recon] Precision@10:0.1742\n",
      "[Recon] Precision@30:0.0897\n",
      "[Recon] Precision@50:0.0602\n",
      "[Recon] ndcg@10:0.4970\n",
      "[Recon] ndcg@30:0.5787\n",
      "[Recon] ndcg@50:0.6034\n",
      "[Recon] ndcg@all:0.6097\n",
      "[Word Dist] Semantic Precision@10:0.0720\n",
      "[Word Dist] Semantic Precision@30:0.0514\n",
      "[Word Dist] Semantic Precision@50:0.0482\n",
      "[Word Dist] Precision@10:0.0447\n",
      "[Word Dist] Precision@30:0.0368\n",
      "[Word Dist] Precision@50:0.0438\n",
      "[Word Dist] ndcg@10:0.0958\n",
      "[Word Dist] ndcg@30:0.1653\n",
      "[Word Dist] ndcg@50:0.2522\n",
      "[Word Dist] ndcg@all:0.3154\n",
      "NPMI:  -0.4655091663437667\n",
      "IRBO:  0.9054914960380933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1/1]\t Seen Samples: [482/482]\tTrain Loss: 53.37462847064639\tTime: 0:00:04.788851: : 1it [00:28, 28.75s/it]\n"
     ]
    }
   ],
   "source": [
    "if config['model'] == 'CombinedTM':\n",
    "    model = CombinedTM(bow_size=len(tp.vocab), contextual_size=contextual_size, n_components=config['topic_num'], num_epochs=config['epochs'], config=config, texts=texts, vocab = tp.vocab, word_embeddings=word_embeddings, idx2token=dataset.idx2token)\n",
    "elif config['model'] == 'mlp':\n",
    "    model = MLPDecoder(bow_size=len(tp.vocab), contextual_size=contextual_size, num_epochs=config['epochs'], config=config, texts=texts,vocab = tp.vocab, word_embeddings=word_embeddings, idx2token=dataset.idx2token)\n",
    "else:\n",
    "    model = ZeroShotTM(bow_size=len(tp.vocab), contextual_size=contextual_size, n_components=config['topic_num'], num_epochs=config['epochs'], config=config, texts=texts, vocab = tp.vocab, word_embeddings=word_embeddings, idx2token=dataset.idx2token)\n",
    "model.fit(training_set, validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf815cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "doc_idx = []\n",
    "print(len(validation_set))\n",
    "for idx in range(200):\n",
    "    doc_idx.append(random.randint(0, len(validation_set)))\n",
    "print(doc_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fca275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [5/20]: : 5it [00:25,  5.08s/it]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "# visualize documents\n",
    "check_nums = 10\n",
    "for idx in doc_idx:\n",
    "    # get recontruct result\n",
    "    recon_list, target_list, doc_list = model.get_reconstruct(validation_set)\n",
    "\n",
    "    # get ranking index\n",
    "    recon_rank_list = np.zeros((len(recon_list), len(tp.vocab)), dtype='float32')\n",
    "    target_rank_list = np.zeros((len(recon_list), len(tp.vocab)), dtype='float32')\n",
    "    for i in range(len(recon_list)):\n",
    "        recon_rank_list[i] = np.argsort(recon_list[i])[::-1]\n",
    "        target_rank_list[i] = np.argsort(target_list[i])[::-1]\n",
    "\n",
    "        # show info\n",
    "    doc_topics_distribution = model.get_doc_topic_distribution(validation_set)\n",
    "    doc_topics = model.get_topic_lists()[np.argmax(doc_topics_distribution[idx])]\n",
    "    print('Documents ', idx)\n",
    "    print(doc_list[idx])\n",
    "    print('---------------------------------------')\n",
    "    print('Topic of Document: ')\n",
    "    print(doc_topics)\n",
    "    print('---------------------------------------')\n",
    "    print('[Predict] Top 10 Words in Document: ')\n",
    "    for word_idx in range(10):\n",
    "        print(dataset.idx2token[recon_rank_list[idx][word_idx]])\n",
    "    print('---------------------------------------')\n",
    "    print('[Label] Top 10 Words in Document: ')\n",
    "    for idx in range(10):\n",
    "        print(dataset.idx2token[target_rank_list[idx][word_idx]])\n",
    "        print('---------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c87640c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_list, target_list, doc_list = model.get_reconstruct(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "420270c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3770, 2000)\n"
     ]
    }
   ],
   "source": [
    "print(target_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c61672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "recon_list = recon_list\n",
    "recon_rank_list = np.zeros((len(recon_list), len(tp.vocab)), dtype='float32')\n",
    "target_rank_list = np.zeros((len(recon_list), len(tp.vocab)), dtype='float32')\n",
    "for i in range(len(recon_list)):\n",
    "        recon_rank_list[i] = np.argsort(recon_list[i])[::-1]\n",
    "        target_rank_list[i] = np.argsort(target_list[i])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99d25b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 179. 1844. 1907. ...  491.  577.  979.]\n"
     ]
    }
   ],
   "source": [
    "doc_idx = 1698\n",
    "print(recon_rank_list[doc_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92484660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 950. 1379.   76. ... 1402.  709. 1320.]\n",
      " [ 107.  310.  793. ... 1815. 1387. 1539.]\n",
      " [1072. 1269. 1006. ...  980.  893.   46.]\n",
      " ...\n",
      " [ 761. 1626. 1996. ... 1014. 1373. 1739.]\n",
      " [ 670.  693.  865. ...  582.   46.  772.]\n",
      " [ 290.  928.  904. ... 1205.  476.  463.]]\n"
     ]
    }
   ],
   "source": [
    "print(recon_rank_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c65d854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: dennisk@cs.uoregon.edu (Dennis Kennedy)\n",
      "Subject: '72 Chevelle SS forsale\n",
      "Organization: University of Oregon\n",
      "Lines: 11\n",
      "Distribution: usa\n",
      "NNTP-Posting-Host: fp2-cc-25.uoregon.edu\n",
      "\n",
      "I don't want to sell this car, but I need money for college.\n",
      "1972 Chevelle Super Sport\n",
      "Rebuilt 402, four speed, 12 Bolt positrac\n",
      "Numbers match\n",
      "110,000 original miles\n",
      "no rust\n",
      "Looks and runs excellent\n",
      "$5995 or best offer.\n",
      "Call Dennis at (503)343-3759\n",
      "or email dennisk@cs.uoregon.edu\n"
     ]
    }
   ],
   "source": [
    "print(doc_list[doc_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e48338b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already\n",
      "tv\n",
      "video\n",
      "late\n",
      "card\n",
      "display\n",
      "3t\n",
      "1t\n",
      "asked\n",
      "games\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "    print(dataset.idx2token[recon_rank_list[doc_idx][idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4e5f982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs\n",
      "72\n",
      "miles\n",
      "excellent\n",
      "runs\n",
      "numbers\n",
      "offer\n",
      "sell\n",
      "four\n",
      "looks\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "    print(dataset.idx2token[target_rank_list[doc_idx][idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e42c4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['windows',\n",
       "  'drive',\n",
       "  'card',\n",
       "  'disk',\n",
       "  'help',\n",
       "  'mac',\n",
       "  'dos',\n",
       "  'mouse',\n",
       "  'problem',\n",
       "  'pc'],\n",
       " ['god',\n",
       "  'jesus',\n",
       "  'sin',\n",
       "  'rutgers',\n",
       "  'christ',\n",
       "  'faith',\n",
       "  'athos',\n",
       "  'truth',\n",
       "  'sandvik',\n",
       "  'church'],\n",
       " ['jpeg',\n",
       "  'edu',\n",
       "  'gif',\n",
       "  'image',\n",
       "  'quality',\n",
       "  'format',\n",
       "  'images',\n",
       "  'get',\n",
       "  'programs',\n",
       "  'color'],\n",
       " ['gov',\n",
       "  'access',\n",
       "  'hst',\n",
       "  'nasa',\n",
       "  'shuttle',\n",
       "  'digex',\n",
       "  'net',\n",
       "  'jpl',\n",
       "  'pat',\n",
       "  'mission'],\n",
       " ['bike', 'dog', 'ca', 'com', 'ride', 'riding', 'dod', 'bnr', 'car', 'bmw'],\n",
       " ['10', '46', 'van', '12', '25', 'nj', '11', '64', '28', '60'],\n",
       " ['ax', 'max', 'giz', 'bhj', 'writes', 'g9v', '75u', 'pl', 'b8f', '2tm'],\n",
       " ['one',\n",
       "  'people',\n",
       "  'would',\n",
       "  'like',\n",
       "  'see',\n",
       "  'even',\n",
       "  'time',\n",
       "  'lord',\n",
       "  'said',\n",
       "  'us'],\n",
       " ['article',\n",
       "  'writes',\n",
       "  'muslims',\n",
       "  'islam',\n",
       "  'turkey',\n",
       "  'edu',\n",
       "  'greek',\n",
       "  'muslim',\n",
       "  'turks',\n",
       "  'turkish'],\n",
       " ['window',\n",
       "  'problem',\n",
       "  'program',\n",
       "  'help',\n",
       "  'nl',\n",
       "  'thanks',\n",
       "  'error',\n",
       "  'create',\n",
       "  'table',\n",
       "  'screen'],\n",
       " ['cx', 'mv', 'ax', '0d', 'mc', 'hz', 'sp', '6um', '6ei', 'sc'],\n",
       " ['04', '02', '03', '06', '05', 'lost', 'edu', '01', '07', '00'],\n",
       " ['com',\n",
       "  'scsi',\n",
       "  'writes',\n",
       "  'bus',\n",
       "  'article',\n",
       "  'ide',\n",
       "  'car',\n",
       "  'isa',\n",
       "  'drive',\n",
       "  'oracle'],\n",
       " ['game',\n",
       "  'ca',\n",
       "  'espn',\n",
       "  'baseball',\n",
       "  'games',\n",
       "  'toronto',\n",
       "  'fan',\n",
       "  'team',\n",
       "  'hockey',\n",
       "  'show'],\n",
       " ['edu',\n",
       "  'image',\n",
       "  'data',\n",
       "  'analysis',\n",
       "  'processing',\n",
       "  'graphics',\n",
       "  'ftp',\n",
       "  '3d',\n",
       "  'gov',\n",
       "  'images'],\n",
       " ['car',\n",
       "  'dod',\n",
       "  'nasa',\n",
       "  'launch',\n",
       "  'space',\n",
       "  'oil',\n",
       "  'bike',\n",
       "  'cars',\n",
       "  'engine',\n",
       "  'driving'],\n",
       " ['com',\n",
       "  'windows',\n",
       "  'server',\n",
       "  'edu',\n",
       "  'os',\n",
       "  'dos',\n",
       "  'window',\n",
       "  'key',\n",
       "  'motif',\n",
       "  'subject'],\n",
       " ['atheism',\n",
       "  'atheists',\n",
       "  'atheist',\n",
       "  'alt',\n",
       "  'universe',\n",
       "  'one',\n",
       "  'people',\n",
       "  'existence',\n",
       "  'religious',\n",
       "  'god'],\n",
       " ['sale',\n",
       "  'offer',\n",
       "  'selling',\n",
       "  'condition',\n",
       "  'ex',\n",
       "  'shipping',\n",
       "  '1988',\n",
       "  'excellent',\n",
       "  'college',\n",
       "  'asking'],\n",
       " ['morality',\n",
       "  'objective',\n",
       "  'frank',\n",
       "  'moral',\n",
       "  'writes',\n",
       "  'values',\n",
       "  'islam',\n",
       "  'god',\n",
       "  'islamic',\n",
       "  'absolute'],\n",
       " ['people',\n",
       "  'one',\n",
       "  'us',\n",
       "  'azerbaijan',\n",
       "  'would',\n",
       "  'went',\n",
       "  'building',\n",
       "  'said',\n",
       "  'armenian',\n",
       "  'armenians'],\n",
       " ['thanks',\n",
       "  'advance',\n",
       "  'hi',\n",
       "  'address',\n",
       "  'looking',\n",
       "  'please',\n",
       "  'anyone',\n",
       "  'ch',\n",
       "  'site',\n",
       "  'ac'],\n",
       " ['writes',\n",
       "  'article',\n",
       "  'clutch',\n",
       "  'players',\n",
       "  'edu',\n",
       "  'year',\n",
       "  'morris',\n",
       "  'player',\n",
       "  'team',\n",
       "  'baseball'],\n",
       " ['ax',\n",
       "  'edu',\n",
       "  'article',\n",
       "  'writes',\n",
       "  'nntp',\n",
       "  'host',\n",
       "  'giz',\n",
       "  'max',\n",
       "  'posting',\n",
       "  'g9v'],\n",
       " ['article',\n",
       "  'writes',\n",
       "  'cramer',\n",
       "  'gay',\n",
       "  'optilink',\n",
       "  'clayton',\n",
       "  'sgi',\n",
       "  'men',\n",
       "  'livesey',\n",
       "  'homosexual'],\n",
       " ['edu',\n",
       "  'article',\n",
       "  'writes',\n",
       "  'insurance',\n",
       "  'drugs',\n",
       "  'guns',\n",
       "  'batf',\n",
       "  'health',\n",
       "  'fbi',\n",
       "  'would'],\n",
       " ['clipper',\n",
       "  'chip',\n",
       "  'escrow',\n",
       "  'encryption',\n",
       "  'keys',\n",
       "  'key',\n",
       "  'nsa',\n",
       "  'phone',\n",
       "  'secret',\n",
       "  'brad'],\n",
       " ['de',\n",
       "  'tu',\n",
       "  'windows',\n",
       "  'au',\n",
       "  'graphics',\n",
       "  'ac',\n",
       "  'files',\n",
       "  'germany',\n",
       "  'using',\n",
       "  'help'],\n",
       " ['article',\n",
       "  'edu',\n",
       "  'writes',\n",
       "  'nntp',\n",
       "  'host',\n",
       "  'posting',\n",
       "  'university',\n",
       "  'scsi',\n",
       "  'car',\n",
       "  'cwru'],\n",
       " ['et', 'body', 'tm', 'koresh', 'james', 'ex', 'food', 'tax', 'tek', 'normal'],\n",
       " ['stephanopoulos',\n",
       "  'mr',\n",
       "  'think',\n",
       "  'president',\n",
       "  'would',\n",
       "  'george',\n",
       "  'jobs',\n",
       "  'package',\n",
       "  'russia',\n",
       "  'know'],\n",
       " ['writes',\n",
       "  'article',\n",
       "  'edu',\n",
       "  'team',\n",
       "  'morris',\n",
       "  'year',\n",
       "  'roger',\n",
       "  'games',\n",
       "  'season',\n",
       "  'game'],\n",
       " ['sale',\n",
       "  'offer',\n",
       "  'condition',\n",
       "  'distribution',\n",
       "  'asking',\n",
       "  'excellent',\n",
       "  'sell',\n",
       "  'trade',\n",
       "  'included',\n",
       "  'price'],\n",
       " ['com',\n",
       "  'writes',\n",
       "  'article',\n",
       "  'br',\n",
       "  'isc',\n",
       "  'stratus',\n",
       "  'sw',\n",
       "  'nntp',\n",
       "  'distribution',\n",
       "  'rocket'],\n",
       " ['soviet',\n",
       "  'serdar',\n",
       "  'argic',\n",
       "  'escape',\n",
       "  'armenian',\n",
       "  'serve',\n",
       "  'mountain',\n",
       "  'turks',\n",
       "  'genocide',\n",
       "  'professor'],\n",
       " ['board',\n",
       "  'card',\n",
       "  'thanks',\n",
       "  'bus',\n",
       "  'drive',\n",
       "  'video',\n",
       "  'driver',\n",
       "  'anyone',\n",
       "  'port',\n",
       "  'pc'],\n",
       " ['baseball',\n",
       "  'virginia',\n",
       "  'buffalo',\n",
       "  'toronto',\n",
       "  'boston',\n",
       "  'night',\n",
       "  'cmu',\n",
       "  'reserve',\n",
       "  'espn',\n",
       "  'college'],\n",
       " ['pp', 'pts', 'period', 'goals', 'goal', 'st', '25', '11', 'play', 'pit'],\n",
       " ['scsi',\n",
       "  'drive',\n",
       "  'controller',\n",
       "  'drives',\n",
       "  'disk',\n",
       "  'ide',\n",
       "  'bios',\n",
       "  'tape',\n",
       "  'rom',\n",
       "  'card'],\n",
       " ['gun',\n",
       "  'people',\n",
       "  'guns',\n",
       "  'would',\n",
       "  'amendment',\n",
       "  'firearms',\n",
       "  'weapons',\n",
       "  'koresh',\n",
       "  'right',\n",
       "  'constitution'],\n",
       " ['sale',\n",
       "  'books',\n",
       "  '00',\n",
       "  'offer',\n",
       "  'please',\n",
       "  'price',\n",
       "  'cd',\n",
       "  'book',\n",
       "  'list',\n",
       "  'condition'],\n",
       " ['god',\n",
       "  'sin',\n",
       "  'church',\n",
       "  'mary',\n",
       "  'bible',\n",
       "  'christ',\n",
       "  'spirit',\n",
       "  'jesus',\n",
       "  'christian',\n",
       "  'paul'],\n",
       " ['com',\n",
       "  'writes',\n",
       "  'article',\n",
       "  'uk',\n",
       "  'convex',\n",
       "  'co',\n",
       "  'crypto',\n",
       "  'escrow',\n",
       "  'key',\n",
       "  'att'],\n",
       " ['privacy',\n",
       "  'cryptography',\n",
       "  'internet',\n",
       "  'mail',\n",
       "  'key',\n",
       "  'security',\n",
       "  'sci',\n",
       "  'secure',\n",
       "  'des',\n",
       "  'anonymous'],\n",
       " ['edu',\n",
       "  'article',\n",
       "  'writes',\n",
       "  'gordon',\n",
       "  'banks',\n",
       "  'keith',\n",
       "  'pitt',\n",
       "  'nntp',\n",
       "  'caltech',\n",
       "  'host'],\n",
       " ['thanks',\n",
       "  'site',\n",
       "  'address',\n",
       "  'sites',\n",
       "  'info',\n",
       "  'anyone',\n",
       "  'looking',\n",
       "  'et',\n",
       "  'ch',\n",
       "  'pre'],\n",
       " ['education',\n",
       "  'cancer',\n",
       "  'un',\n",
       "  'et',\n",
       "  'popular',\n",
       "  'released',\n",
       "  'pre',\n",
       "  'waiting',\n",
       "  'training',\n",
       "  'aids'],\n",
       " ['israel',\n",
       "  'israeli',\n",
       "  'arab',\n",
       "  'jewish',\n",
       "  'jews',\n",
       "  'arabs',\n",
       "  'adam',\n",
       "  'policy',\n",
       "  'attacks',\n",
       "  'peace'],\n",
       " ['edu',\n",
       "  'article',\n",
       "  'writes',\n",
       "  'nntp',\n",
       "  'atf',\n",
       "  'host',\n",
       "  'indiana',\n",
       "  'posting',\n",
       "  'ohio',\n",
       "  'university'],\n",
       " ['health',\n",
       "  'disease',\n",
       "  'medical',\n",
       "  'page',\n",
       "  'patients',\n",
       "  'volume',\n",
       "  'reported',\n",
       "  'age',\n",
       "  'risk',\n",
       "  'use']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_topic_lists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d5706b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [20/20]: : 20it [03:04,  9.25s/it]\n"
     ]
    }
   ],
   "source": [
    "doc_topics_distribution = model.get_doc_topic_distribution(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "256d0fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00264944, 0.00154069, 0.00157822, 0.00240728, 0.00557763,\n",
       "       0.00427778, 0.00276508, 0.00872498, 0.04012846, 0.00356483,\n",
       "       0.00288309, 0.00270965, 0.00298711, 0.00746678, 0.00177612,\n",
       "       0.00248846, 0.00091876, 0.00518265, 0.00199116, 0.01588894,\n",
       "       0.08754831, 0.00166128, 0.00302516, 0.00350954, 0.00638637,\n",
       "       0.00344035, 0.00659486, 0.00204372, 0.00246886, 0.01168864,\n",
       "       0.01562314, 0.00923532, 0.00320664, 0.00575135, 0.02873103,\n",
       "       0.00292833, 0.0047355 , 0.00574315, 0.00327179, 0.00780392,\n",
       "       0.00383813, 0.00213848, 0.00608836, 0.00335248, 0.00340084,\n",
       "       0.00218007, 0.00324257, 0.63300758, 0.00526384, 0.00458329])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topics_distribution[doc_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80cb1748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['israel', 'israeli', 'arab', 'jewish', 'jews', 'arabs', 'adam', 'policy', 'attacks', 'peace']\n"
     ]
    }
   ],
   "source": [
    "doc_topics = model.get_topic_lists()[np.argmax(doc_topics_distribution[doc_idx])]\n",
    "print(doc_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcf56ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        test.append(unpreprocessed_corpus[i+j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0803b9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (/dhome/casimir0304/.cache/huggingface/datasets/tweet_eval/emotion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
      "/data1/IDE/casimir0304/ide/utils/preprocessing.py:24: UserWarning: WhiteSpacePreprocessing is deprecated and will be removed in future versions.Use WhiteSpacePreprocessingStopwords.\n",
      "  warnings.warn(\"WhiteSpacePreprocessing is deprecated and will be removed in future versions.\"\n",
      "/dhome/casimir0304/miniconda3/envs/ML/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "raw_documents = load_document(config['dataset'])[\"documents\"]\n",
    "preprocessed_documents, unpreprocessed_corpus, texts = preprocess_document(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70e3da88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "raw_documents = load_document(config['dataset'])[\"documents\"]\n",
    "preprocessed_documents, unpreprocessed_corpus, texts = preprocess_document(raw_documents)\n",
    "vectorizer = TfidfVectorizer(token_pattern=r'(?u)\\b[\\w+|\\-]+\\b')\n",
    "decode_target = vectorizer.fit_transform(preprocessed_documents)\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "id2token = {k: v for k, v in zip(range(0, len(vocab)), vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f531a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token = {k: v for k, v in zip(range(0, len(vocab)), vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "478f6de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4614, 1719)\n"
     ]
    }
   ],
   "source": [
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31de50f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
