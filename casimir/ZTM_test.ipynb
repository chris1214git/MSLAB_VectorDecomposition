{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b33ac182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dhome/casimir0304/miniconda3/envs/ML/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import nltk\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from model.ide_topic_decoder import IDEDataset, IDETopicDecoder\n",
    "#from utils.toolbox import same_seeds, show_settings, record_settings, get_preprocess_document_embs, get_preprocess_document_labels, get_word_embs\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.set_num_threads(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "592243ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from math import log\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from scipy import sparse\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from utils.preprocessing import WhiteSpacePreprocessing, WhiteSpacePreprocessingStopwords, WhiteSpacePreprocessing_v2\n",
    "from utils.data_loader import load_document, load_word2emb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def preprocess_document(raw_documents):\n",
    "    sp = WhiteSpacePreprocessingStopwords(raw_documents, stopwords_list=['english'], vocabulary_size=10000, min_words=15)\n",
    "    preprocessed_documents, unpreprocessed_corpus, vocab, _ = sp.preprocess()\n",
    "    delete_non_eng_documents = delete_non_eng(preprocessed_documents)\n",
    "    noun_documents = pos(delete_non_eng_documents)\n",
    "    delete_documents = []\n",
    "    for idx in range(len(noun_documents)):\n",
    "        if len(noun_documents[idx]) == 0:\n",
    "            delete_documents.append(idx)\n",
    "    delete_documents = sorted(delete_documents, reverse=True)\n",
    "    for idx in delete_documents:\n",
    "        del unpreprocessed_corpus[idx]\n",
    "    noun_documents = list(filter(None, noun_documents))\n",
    "    texts = [text.split() for text in noun_documents]\n",
    "    return noun_documents, unpreprocessed_corpus, texts, vocab\n",
    "\n",
    "def generate_document_embedding(model, documents):\n",
    "    if model == 'roberta':\n",
    "        model = SentenceTransformer(\"paraphrase-distilroberta-base-v1\", device=get_free_gpu())\n",
    "    else:\n",
    "        model = SentenceTransformer(\"all-mpnet-base-v2\", device=get_free_gpu())\n",
    "\n",
    "    return np.array(model.encode(documents, show_progress_bar=True, batch_size=200))\n",
    "\n",
    "def tokenizer_eng(text):\n",
    "        text = re.sub(r'[^A-Za-z ]+', '', text)\n",
    "        text = text.strip().split()\n",
    "        return text\n",
    "\n",
    "def delete_non_eng(documents):\n",
    "    preprocessed_documents = []\n",
    "    for text in documents:\n",
    "        selected_word = []\n",
    "        for word in tokenizer_eng(text):\n",
    "            selected_word.append(word)\n",
    "        preprocessed_documents.append(\" \".join(selected_word))\n",
    "    return preprocessed_documents\n",
    "\n",
    "\n",
    "def pos(documents):\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    is_verb = lambda pos: pos[:2] == 'VB'\n",
    "\n",
    "    preprocessed_documents = []\n",
    "    for text in documents:\n",
    "        tokenized = nltk.word_tokenize(text)\n",
    "        noun_word = []\n",
    "        for (word, pos) in nltk.pos_tag(tokenized):\n",
    "            if is_noun(pos) or is_verb(pos):\n",
    "                noun_word.append(word)\n",
    "        preprocessed_documents.append(\" \".join(noun_word))\n",
    "    return preprocessed_documents\n",
    "\n",
    "def calculate_word_embeddings_tensor(word2embedding, vocab, idx2token):\n",
    "    word_embeddings = torch.zeros(len(vocab), len(word2embedding['a']))\n",
    "    for k in idx2token:\n",
    "        if idx2token[k] not in word2embedding:\n",
    "            # print('not found word embedding', idx2token[k])\n",
    "            continue\n",
    "        word_embeddings[k] = torch.tensor(word2embedding[idx2token[k]])\n",
    "\n",
    "    return word_embeddings\n",
    "\n",
    "def get_free_gpu():\n",
    "    os.system('nvidia-smi -q -d Memory |grep -A4 GPU|grep Free > tmp')\n",
    "    memory_available = [int(x.split()[2])\n",
    "                        for x in open('tmp', 'r').readlines()]\n",
    "    os.system('rm -f tmp')\n",
    "    print('Using cuda {} for training...'.format(int(np.argmax(memory_available))))\n",
    "    torch.cuda.device(int(np.argmax(memory_available)))\n",
    "    return \"cuda:{}\".format(int(np.argmax(memory_available)))\n",
    "\n",
    "\n",
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def show_settings(config):\n",
    "    print('-------- Info ---------')\n",
    "    settings = \"\"\n",
    "    for key in list(config.keys()):\n",
    "        settings += \"{}: {}\\n\".format(key, config.get(key))\n",
    "    print(settings)\n",
    "    print('-----------------------')\n",
    "\n",
    "def record_settings(config):\n",
    "    record = open('./'+config['dataset']+'_'+config['model']+'_'+config['encoder']+'_'+config['target']+'.txt', 'a')\n",
    "    record.write('-------- Info ---------\\n')\n",
    "    settings = \"\"\n",
    "    for key in list(config.keys()):\n",
    "        settings += \"{}: {}\\n\".format(key, config.get(key))\n",
    "    record.write(settings)\n",
    "    record.write('-----------------------\\n')\n",
    "\n",
    "def split_data(dataset, config):\n",
    "    train_length = int(len(dataset)*0.6)\n",
    "    valid_length = int(len(dataset)*0.2)\n",
    "    test_length = len(dataset) - train_length - valid_length\n",
    "\n",
    "    train_dataset, valid_dataset, test_dataset = random_split(\n",
    "        dataset, lengths=[train_length, valid_length, test_length],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=config[\"batch_size\"],\n",
    "        shuffle=True, pin_memory=True,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, batch_size=config[\"batch_size\"], shuffle=False, pin_memory=True)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "def doc_filter(raw_document, vocab):\n",
    "    PATTERN = r\"(?u)\\b\\w\\w+\\b\"\n",
    "    doc = re.findall(PATTERN, raw_document.lower())\n",
    "    return [x for x in doc if x in vocab]\n",
    "\n",
    "def generate_graph(doc_list, word2index, index2word):\n",
    "    window_size = 10\n",
    "    windows = []\n",
    "\n",
    "    # Traverse Each Document & Move window on each of them\n",
    "    for doc in doc_list:\n",
    "        length = len(doc)\n",
    "        if length <= window_size:\n",
    "            windows.append(doc)\n",
    "        else:\n",
    "            for i in range(length-window_size+1):\n",
    "                window = doc[i: i+window_size]\n",
    "                windows.append(window)\n",
    "    \n",
    "    word_freq = {}\n",
    "    word_pair_count = {}\n",
    "    for window in tqdm(windows, desc='Calculate word pair: '):\n",
    "        appeared = set()\n",
    "        for i in range(len(window)):\n",
    "            if window[i] not in appeared:\n",
    "                if window[i] in word_freq:\n",
    "                    word_freq[window[i]] += 1\n",
    "                else:\n",
    "                    word_freq[window[i]] = 1\n",
    "                appeared.add(window[i])\n",
    "            if i != 0:\n",
    "                for j in range(0, i):\n",
    "                    word_i = window[i]\n",
    "                    word_i_id = word2index[word_i]\n",
    "                    word_j = window[j]\n",
    "                    word_j_id = word2index[word_j]\n",
    "                    if word_i_id == word_j_id:\n",
    "                        continue\n",
    "                    word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n",
    "                    if word_pair_str in word_pair_count:\n",
    "                        word_pair_count[word_pair_str] += 1\n",
    "                    else:\n",
    "                        word_pair_count[word_pair_str] = 1\n",
    "                    word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n",
    "                    if word_pair_str in word_pair_count:\n",
    "                        word_pair_count[word_pair_str] += 1\n",
    "                    else:\n",
    "                        word_pair_count[word_pair_str] = 1\n",
    "    \n",
    "    row = []\n",
    "    col = []\n",
    "    edge = []\n",
    "    weight = []\n",
    "    # pmi as weights\n",
    "\n",
    "    num_window = len(windows)\n",
    "    # count_mean = np.array(list(word_pair_count.values())).mean()\n",
    "    for key in tqdm(word_pair_count, desc='Construct Edge: '):\n",
    "        temp = key.split(',')\n",
    "        i = int(temp[0])\n",
    "        j = int(temp[1])\n",
    "        count = word_pair_count[key]\n",
    "        word_freq_i = word_freq[index2word[i]]\n",
    "        word_freq_j = word_freq[index2word[j]]\n",
    "        pmi = log((1.0 * count / num_window) /\n",
    "                (1.0 * word_freq_i * word_freq_j/(num_window * num_window)))\n",
    "        if pmi <= 0:\n",
    "            continue\n",
    "        row.append(i)\n",
    "        col.append(j)\n",
    "        if count >= 15:\n",
    "            edge.append([i, j])\n",
    "            edge.append([j, i])\n",
    "        weight.append(pmi)\n",
    "\n",
    "    print('# of Node: {}\\n# of Edge: {}'.format(len(word2index), len(edge)))\n",
    "\n",
    "    return edge\n",
    "\n",
    "def get_preprocess_document(dataset, min_df=1, max_df=1.0, vocabulary_size=None, min_doc_word=15, use_pos=True, **kwargs):\n",
    "    '''\n",
    "    Returns preprocessed_docs & unpreprocessed_docs of the dataset\n",
    "\n",
    "            Parameters:\n",
    "                    dataset (str): For data_loader\n",
    "                    min_df, max_df, vocabulary_size: For CountVectorizer in CTM preprocess\n",
    "                    min_doc_word: Minimum doc length\n",
    "            Returns:\n",
    "                    unpreprocessed_docs (list):\n",
    "                    preprocessed_docs (list):\n",
    "    '''\n",
    "    print('Getting preprocess documents:', dataset)\n",
    "    print(f'min_df: {min_df} max_df: {max_df} vocabulary_size: {vocabulary_size} min_doc_word: {min_doc_word}')\n",
    "    raw_documents = load_document(dataset)[\"documents\"]\n",
    "    # CTM preprocess\n",
    "    sp = WhiteSpacePreprocessing_v2(raw_documents, stopwords_language='english',\\\n",
    "                                    min_df=min_df, max_df=max_df, vocabulary_size=vocabulary_size)\n",
    "\n",
    "    preprocessed_docs, unpreprocessed_docs, vocabulary, _ = sp.preprocess()\n",
    "    # filter special character\n",
    "    preprocessed_docs = delete_non_eng(preprocessed_docs)\n",
    "    # select nouns & verbs\n",
    "    if use_pos:\n",
    "        preprocessed_docs = pos(preprocessed_docs)\n",
    "    # delete short articles\n",
    "    delete_docs_idx = []\n",
    "    for idx in range(len(preprocessed_docs)):\n",
    "        # length > min_doc_word\n",
    "        if len(preprocessed_docs[idx]) == 0 or len(preprocessed_docs[idx]) < min_doc_word:\n",
    "            delete_docs_idx.append(idx)\n",
    "    delete_docs_idx = sorted(delete_docs_idx, reverse=True)\n",
    "    for idx in delete_docs_idx:\n",
    "        del preprocessed_docs[idx]\n",
    "        del unpreprocessed_docs[idx]\n",
    "    \n",
    "    return unpreprocessed_docs ,preprocessed_docs\n",
    "\n",
    "def get_preprocess_document_labels(preprocessed_docs, preprocess_config='../chris/parameters/preprocess_config.json'):\n",
    "    '''\n",
    "    Returns labels for document decoder\n",
    "\n",
    "            Parameters:\n",
    "                    preprocessed_docs (list): \n",
    "            Returns:\n",
    "                    labels (dict): bow, tf-idf\n",
    "                    vocabulary (dict): bow, tf-idf\n",
    "    '''\n",
    "    print('Getting preprocess documents labels')\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    # covert sparse matrix to numpy array\n",
    "    tf_idf_vector = vectorizer.fit_transform(preprocessed_docs).toarray()\n",
    "    bow_vector = tf_idf_vector.copy()\n",
    "    bow_vector[bow_vector > 0] = 1\n",
    "    bow_vector[bow_vector < 0] = 0\n",
    "    vocabulary = vectorizer.get_feature_names()\n",
    "\n",
    "    labels = {}\n",
    "    labels['tf-idf'] = tf_idf_vector\n",
    "    labels['bow'] = bow_vector\n",
    "    \n",
    "    vocabularys = {}\n",
    "    vocabularys['tf-idf'] = vocabulary\n",
    "    vocabularys['bow'] = vocabulary\n",
    "\n",
    "    return labels, vocabularys\n",
    "\n",
    "def get_preprocess_document_labels_v2(preprocessed_docs, preprocess_config, preprocess_config_dir, ngram=1):\n",
    "    '''\n",
    "    Returns labels for document decoder\n",
    "\n",
    "            Parameters:\n",
    "                    preprocessed_docs (list): \n",
    "            Returns:\n",
    "                    labels (dict): bow, tf-idf\n",
    "                    vocabulary (dict): bow, tf-idf\n",
    "    '''\n",
    "    print('Getting preprocess documents labels')\n",
    "    print('Finding precompute_keyword by preprocess_config', preprocess_config)\n",
    "\n",
    "    config_dir = os.path.join('../data/precompute_keyword', preprocess_config_dir, \\\n",
    "                              '{}_ngram_{}'.format(preprocess_config['dataset'], ngram))\n",
    "    # check preprocess config the same when loading precompute labels\n",
    "    with open(os.path.join(config_dir, 'preprocess_config.json'), 'r') as f:\n",
    "        preprocess_config2 = json.load(f)\n",
    "        assert preprocess_config == preprocess_config2\n",
    "\n",
    "    tf_idf_vector = sparse.load_npz(os.path.join(config_dir, 'TFIDF.npz'))\n",
    "    bow_vector = sparse.load_npz(os.path.join(config_dir, 'BOW.npz'))\n",
    "    try:\n",
    "        keybert_vector = sparse.load_npz(os.path.join(config_dir, 'KeyBERT.npz'))\n",
    "        yake_vector = sparse.load_npz(os.path.join(config_dir, 'YAKE.npz'))\n",
    "    except:\n",
    "        print('no precompute keyword')\n",
    "        keybert_vector = None\n",
    "        yake_vector = None\n",
    "\n",
    "    vocabulary = np.load(os.path.join(config_dir, 'vocabulary.npy'))\n",
    "\n",
    "    labels = {}\n",
    "    labels['tf-idf'] = tf_idf_vector\n",
    "    labels['bow'] = bow_vector\n",
    "    labels['keybert'] = keybert_vector\n",
    "    labels['yake'] = yake_vector\n",
    "    \n",
    "    return labels, vocabulary\n",
    "    \n",
    "def merge_targets(targets, targets2, vocabularys, vocabularys2):\n",
    "    # ref: https://numpy.org/doc/stable/reference/generated/numpy.put_along_axis.html#numpy.put_along_axis\n",
    "    vocabularys_all = list(vocabularys)\n",
    "    vocabularys2_map_idx = []\n",
    "    \n",
    "    for s in vocabularys2:\n",
    "        if s in vocabularys_all:\n",
    "            idx = vocabularys_all.index(s)\n",
    "            vocabularys2_map_idx.append(idx)\n",
    "        else:\n",
    "            vocabularys_all.append(s)\n",
    "            vocabularys2_map_idx.append(len(vocabularys_all)-1)\n",
    "            \n",
    "    print('vocabularys len', len(vocabularys))\n",
    "    print('vocabularys2 len', len(vocabularys2))\n",
    "    print('merge len', len(vocabularys_all))\n",
    "    mr = (len(vocabularys_all) - len(vocabularys)) / len(vocabularys2)\n",
    "    print('vocabularys2 missing ratio', mr)\n",
    "    \n",
    "    new_targets = np.zeros((targets.shape[0], len(vocabularys_all)))\n",
    "    new_targets2 = np.zeros((targets2.shape[0], len(vocabularys_all)))\n",
    "    \n",
    "    idx1 = np.arange(targets.shape[1])\n",
    "    idx1 = np.repeat(idx1.reshape(1, -1), targets.shape[0], axis=0)\n",
    "    np.put_along_axis(new_targets, idx1, targets, axis=1)\n",
    "    \n",
    "    assert len(vocabularys2_map_idx) == len(vocabularys2)\n",
    "    idx2 = np.array(vocabularys2_map_idx)\n",
    "    idx2 = np.repeat(idx2.reshape(1, -1), targets2.shape[0], axis=0)\n",
    "    np.put_along_axis(new_targets2, idx2, targets2, axis=1)\n",
    "    \n",
    "    assert new_targets.shape[1] == new_targets2.shape[1] == len(vocabularys_all)\n",
    "    \n",
    "    return new_targets, new_targets2, np.array(vocabularys_all)\n",
    "\n",
    "def get_preprocess_document_embs(preprocessed_docs, model_name):\n",
    "    '''\n",
    "    Returns embeddings(input) for document decoder\n",
    "\n",
    "            Parameters:\n",
    "                    preprocessed_docs (list): \n",
    "                    model_name (str):\n",
    "            Returns:\n",
    "                    doc_embs (array): \n",
    "                    model (class): \n",
    "    '''\n",
    "    print('Getting preprocess documents embeddings')\n",
    "    device = get_free_gpu()\n",
    "    if model_name == 'bert':\n",
    "        model = SentenceTransformer(\"bert-base-uncased\", device=device)\n",
    "        doc_embs = np.array(model.encode(preprocessed_docs, show_progress_bar=True, batch_size=50))\n",
    "    elif model_name == 'mpnet':\n",
    "        model = SentenceTransformer(\"all-mpnet-base-v2\", device=device)\n",
    "        doc_embs = np.array(model.encode(preprocessed_docs, show_progress_bar=True, batch_size=50))\n",
    "    elif model_name == 'average':\n",
    "        model = SentenceTransformer(\"average_word_embeddings_glove.840B.300d\", device=device)\n",
    "        doc_embs = np.array(model.encode(preprocessed_docs, show_progress_bar=True, batch_size=50))\n",
    "    elif model_name == 'doc2vec':\n",
    "        doc_embs = []\n",
    "        preprocessed_docs_split = [doc.split() for doc in preprocessed_docs]\n",
    "        documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(preprocessed_docs_split)]\n",
    "        model = Doc2Vec(documents, vector_size=200, workers=4)\n",
    "        for idx in range(len(preprocessed_docs_split)):\n",
    "            doc_embs.append(model.infer_vector(preprocessed_docs_split[idx]))\n",
    "        doc_embs = np.array(doc_embs)\n",
    "\n",
    "    return doc_embs, model, device  \n",
    "\n",
    "def get_word_embs(vocabularys, id2token=None, word_emb_file='../data/glove.6B.300d.txt', data_type='ndarray'):\n",
    "    '''\n",
    "    Returns word_embs array for semantic precision\n",
    "\n",
    "            Parameters:\n",
    "                    vocabularys (list): \n",
    "                    word_emb_file (str): \n",
    "            Returns:\n",
    "                    word_embs (array): \n",
    "    '''\n",
    "    \n",
    "    word2emb = load_word2emb(word_emb_file)\n",
    "    dim = len(list(word2emb.values())[0])\n",
    "    word_embs = []\n",
    "    for word in vocabularys:\n",
    "        if word not in word2emb:\n",
    "            emb = np.zeros(dim)\n",
    "        else:\n",
    "            emb = word2emb[word]\n",
    "        word_embs.append(emb) \n",
    "\n",
    "    if data_type == 'tensor':\n",
    "        print('Getting [tensor] word embeddings')\n",
    "        word_embs = torch.Tensor(word_embs)\n",
    "    else:\n",
    "        print('Getting [ndarray] word embeddings')\n",
    "        word_embs = np.array(word_embs)\n",
    "    return word_embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec65c827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'model': 'ZTM',\n",
    "    'architecture': 'after',\n",
    "    'activation': 'sigmoid',\n",
    "    'dataset': 'tweet',\n",
    "    'dataset_name': 'tweet',\n",
    "    'vocabulary_size':100,\n",
    "    'encoder': 'bert',\n",
    "    'target': 'tf-idf',\n",
    "    'topic_num': 50,\n",
    "    'seed': 123,\n",
    "    'epochs': 10,\n",
    "    'lr': 1e-4,\n",
    "    'loss': 'listnet',\n",
    "    'batch_size': 8,\n",
    "    'weight_decay': 0,\n",
    "    'ratio': 0.8,\n",
    "    'topk': [10, 30, 50],\n",
    "    'save': False,\n",
    "    'threshold': 0.7,\n",
    "}\n",
    "\n",
    "#show_settings(config)\n",
    "same_seeds(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2538e840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting preprocess documents: tweet\n",
      "min_df: 1 max_df: 1.0 vocabulary_size: 100 min_doc_word: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (/dhome/casimir0304/.cache/huggingface/datasets/tweet_eval/emotion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
      "/dhome/casimir0304/miniconda3/envs/ML/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "unpreprocessed_corpus ,preprocessed_corpus = get_preprocess_document(**config)\n",
    "texts = [text.split() for text in preprocessed_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efcefe0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting preprocess documents embeddings\n",
      "Using cuda 2 for training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /dhome/casimir0304/.cache/torch/sentence_transformers/bert-base-uncased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /dhome/casimir0304/.cache/torch/sentence_transformers/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221ffc8b7f2944b39fb1dc6e1961b34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generating document embedding\n",
    "doc_embs, doc_model, device = get_preprocess_document_embs(preprocessed_corpus, config['encoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6858bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6bade19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting preprocess documents labels\n"
     ]
    }
   ],
   "source": [
    "# Decode target & Vocabulary\n",
    "labels, vocabularys= get_preprocess_document_labels(preprocessed_corpus)\n",
    "id2token = {k: v for k, v in zip(range(0, len(vocabularys[config['target']])), vocabularys[config['target']])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2a97003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    }
   ],
   "source": [
    "print(len(vocabularys[config['target']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0abf1a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d94e74903f4b1d81d555f02b06a71f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:400000\n",
      "Getting [tensor] word embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1482048/2379153461.py:418: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  word_embs = torch.Tensor(word_embs)\n"
     ]
    }
   ],
   "source": [
    "# word embedding preparation\n",
    "word_embeddings = get_word_embs(vocabularys[config['target']], data_type='tensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dec5ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import datetime\n",
    "import wordcloud\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(\"./\")\n",
    "from utils.loss import ListNet, MythNet\n",
    "from utils.eval import retrieval_normalized_dcg_all, retrieval_precision_all, semantic_precision_all, retrieval_precision_all_v2, semantic_precision_all_v2\n",
    "from utils.eval_topic import CoherenceNPMI, TopicDiversity, InvertedRBO\n",
    "from utils.toolbox import get_free_gpu, record_settings\n",
    "from model.inference_network import ContextualInferenceNetwork\n",
    "\n",
    "class IDEDataset(Dataset):\n",
    "    def __init__(self, corpus, emb, target):\n",
    "        \n",
    "        assert len(emb) == len(target)\n",
    "        self.corpus = corpus\n",
    "        self.emb = torch.FloatTensor(emb)\n",
    "        self.target = torch.FloatTensor(target)        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.corpus[idx], self.emb[idx], self.target[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emb)\n",
    "\n",
    "class DecoderNetwork(nn.Module):\n",
    "    def __init__(self, config, device, vocab_size, contextual_size, glove_word_embeddings, n_components, hidden_sizes=(100,100), activation='softplus', dropout=0.2, learn_priors=True):\n",
    "        super(DecoderNetwork, self).__init__()\n",
    "\n",
    "        assert activation in ['softplus', 'relu']\n",
    "\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.vocab_size = vocab_size\n",
    "        self.contextual_size = contextual_size\n",
    "        self.glove_word_embeddings = glove_word_embeddings\n",
    "        self.n_components = n_components\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.learn_priors = learn_priors\n",
    "        self.topic_word_matrix = None\n",
    "\n",
    "        # decoder architecture\n",
    "        self.batch_norm = nn.BatchNorm1d(vocab_size)\n",
    "        self.word_embedding =  nn.Parameter(torch.randn(vocab_size*4, vocab_size))\n",
    "        self.full_decoder = nn.Sequential(\n",
    "            nn.Linear(vocab_size, vocab_size*4),\n",
    "            nn.BatchNorm1d(vocab_size*4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(vocab_size*4, vocab_size),\n",
    "            nn.BatchNorm1d(vocab_size),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.half_decoder_tanh = nn.Sequential(\n",
    "            nn.Linear(vocab_size+contextual_size, vocab_size*4),\n",
    "            nn.BatchNorm1d(vocab_size*4),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.2),\n",
    "        )\n",
    "        self.half_decoder_sigmoid = nn.Sequential(\n",
    "            nn.Linear(vocab_size+contextual_size, vocab_size*4),\n",
    "            nn.BatchNorm1d(vocab_size*4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(p=0.2),\n",
    "        )\n",
    "        self.share_wieght_decoder = nn.Sequential(\n",
    "            nn.Linear(contextual_size, contextual_size*4),\n",
    "            nn.BatchNorm1d(contextual_size*4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(contextual_size*4, vocab_size),\n",
    "            nn.BatchNorm1d(vocab_size),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.glove_emb_decoder = nn.Sequential(\n",
    "            nn.Linear(vocab_size+contextual_size, vocab_size*4),\n",
    "            nn.BatchNorm1d(vocab_size*4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(vocab_size*4, glove_word_embeddings.shape[1]),\n",
    "            nn.BatchNorm1d(glove_word_embeddings.shape[1]),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        # topic model architecture\n",
    "        self.inf_net = ContextualInferenceNetwork(vocab_size, contextual_size, n_components, hidden_sizes, activation, label_size=0)\n",
    "        \n",
    "        topic_prior_mean = 0.0\n",
    "        self.prior_mean = torch.tensor([topic_prior_mean] * n_components).to(device)\n",
    "        if self.learn_priors:\n",
    "            self.prior_mean = nn.Parameter(self.prior_mean)\n",
    "\n",
    "        topic_prior_variance = 1. - (1. / self.n_components)\n",
    "        self.prior_variance = torch.tensor([topic_prior_variance] * n_components).to(device)\n",
    "        if self.learn_priors:\n",
    "            self.prior_variance = nn.Parameter(self.prior_variance)\n",
    "\n",
    "        self.beta = torch.Tensor(n_components, vocab_size).to(device)\n",
    "        self.beta = nn.Parameter(self.beta)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.beta)\n",
    "        \n",
    "        self.beta_batchnorm = nn.BatchNorm1d(vocab_size, affine=False)\n",
    "        \n",
    "        self.drop_theta = nn.Dropout(p=self.dropout)\n",
    "    \n",
    "    @staticmethod\n",
    "    def reparameterize(mu, logvar):\n",
    "        \"\"\"Reparameterize the theta distribution.\"\"\"\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def forward(self, emb, target, labels=None):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        posterior_mu, posterior_log_sigma = self.inf_net(target, emb, labels)\n",
    "        posterior_sigma = torch.exp(posterior_log_sigma)\n",
    "\n",
    "        # generate samples from theta\n",
    "        theta = F.softmax(self.reparameterize(posterior_mu, posterior_log_sigma), dim=1)\n",
    "        theta = self.drop_theta(theta)\n",
    "\n",
    "        # prodLDA\n",
    "        # in: batch_size x input_size x n_components\n",
    "        word_dist = F.softmax(self.beta_batchnorm(torch.matmul(theta, self.beta)), dim=1)\n",
    "        # word_dist: batch_size x input_size\n",
    "        self.topic_word_matrix = self.beta\n",
    "        \n",
    "        if self.config['activation'] == 'tanh':\n",
    "            emb_word_dist = torch.cat((word_dist, emb), dim=1)\n",
    "            decoded_word_dist = self.half_decoder_tanh(emb_word_dist)\n",
    "            recon_dist = torch.sigmoid(self.batch_norm((torch.matmul(decoded_word_dist, self.word_embedding))))\n",
    "        else:\n",
    "            emb_word_dist = torch.cat((word_dist, emb), dim=1)\n",
    "            decoded_word_dist = self.half_decoder_sigmoid(emb_word_dist)\n",
    "            recon_dist = torch.sigmoid(self.batch_norm((torch.matmul(decoded_word_dist, self.word_embedding))))\n",
    "\n",
    "        return self.prior_mean, self.prior_variance, posterior_mu, posterior_sigma, posterior_log_sigma, word_dist, recon_dist\n",
    "    \n",
    "    def get_theta(self, target, emb, labels=None):\n",
    "        with torch.no_grad():\n",
    "            posterior_mu, posterior_log_sigma = self.inf_net(target, emb, labels)\n",
    "            theta = F.softmax(self.reparameterize(posterior_mu, posterior_log_sigma), dim=1)\n",
    "\n",
    "            return theta\n",
    "\n",
    "class IDETopicDecoder:\n",
    "    def __init__(self, config, texts=None, vocab = None, idx2token=None, device=None, contextual_size=768, word_embeddings=None, \n",
    "                n_components=10, hidden_sizes=(100, 100), activation='softplus', dropout=0.2, learn_priors=True,\n",
    "                momentum=0.99, reduce_on_plateau=False, num_data_loader_workers=mp.cpu_count(), loss_weights=None):\n",
    "        self.config = config\n",
    "        self.texts = texts\n",
    "        self.vocab = vocab\n",
    "        self.idx2token = idx2token\n",
    "        self.device = device\n",
    "        self.contextual_size = contextual_size\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.n_components = n_components\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.learn_priors = learn_priors\n",
    "        self.reduce_on_plateau = reduce_on_plateau\n",
    "        self.momentum = momentum\n",
    "        self.num_data_loader_workers = num_data_loader_workers\n",
    "        self.training_doc_topic_distributions = None\n",
    "        self.distribution_cache = None\n",
    "        self.num_epochs = config['epochs']\n",
    "        if config['loss'] == 'mse':\n",
    "            self.loss_funct = torch.nn.MSELoss(reduction='mean')\n",
    "        else:\n",
    "             self.loss_funct = MythNet\n",
    "        if loss_weights:\n",
    "            self.weights = loss_weights\n",
    "        else:\n",
    "            self.weights = {\"beta\": 1}\n",
    "\n",
    "        self.model = DecoderNetwork(\n",
    "                    config, device, len(vocab), contextual_size, word_embeddings, n_components, hidden_sizes, activation,\n",
    "                    dropout, learn_priors)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(\n",
    "                        self.model.parameters(), lr=config['lr'], betas=(self.momentum, 0.99), weight_decay=config['weight_decay'])\n",
    "\n",
    "        if self.reduce_on_plateau:\n",
    "            self.scheduler = ReduceLROnPlateau(self.optimizer, patience=10)\n",
    "        \n",
    "        self.best_components = None\n",
    "\n",
    "    def loss(self, inputs, word_dists, recon_dists, prior_mean, prior_variance,\n",
    "              posterior_mean, posterior_variance, posterior_log_variance):\n",
    "        var_division = torch.sum(posterior_variance / prior_variance, dim=1)\n",
    "        diff_means = prior_mean - posterior_mean\n",
    "        diff_term = torch.sum((diff_means * diff_means) / prior_variance, dim=1)\n",
    "        logvar_det_division = prior_variance.log().sum() - posterior_log_variance.sum(dim=1)\n",
    "\n",
    "        KL = 0.5 * (var_division + diff_term - self.n_components + logvar_det_division)\n",
    "\n",
    "        RL = torch.sum(-inputs * torch.log(word_dists + 1e-10), dim=1)\n",
    "\n",
    "        DL = self.loss_funct(recon_dists, inputs)\n",
    "\n",
    "        return KL, RL, DL\n",
    "\n",
    "    def training(self, loader):\n",
    "        \"\"\"Train epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        samples_processed = 0\n",
    "\n",
    "        for batch, (corpus, emb, target) in enumerate(loader):\n",
    "            target = target.reshape(target.shape[0], -1)\n",
    "            emb, target = emb.to(self.device), target.to(self.device)\n",
    "\n",
    "            self.model.zero_grad()\n",
    "            prior_mean, prior_variance, posterior_mean, posterior_variance,\\\n",
    "            posterior_log_variance, word_dists, recon_dists = self.model(emb, target)\n",
    "\n",
    "            kl_loss, rl_loss, dl_loss = self.loss(\n",
    "                target, word_dists, recon_dists, prior_mean, prior_variance,\n",
    "                posterior_mean, posterior_variance, posterior_log_variance)\n",
    "            loss = self.weights[\"beta\"] * kl_loss + rl_loss + dl_loss\n",
    "            loss = loss.sum()\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            samples_processed += target.size()[0]\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= samples_processed\n",
    "\n",
    "        return samples_processed, train_loss\n",
    "    \n",
    "    def validation(self, loader):\n",
    "        \"\"\"Validation epoch.\"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        samples_processed = 0\n",
    "\n",
    "        results = defaultdict(list)\n",
    "        dists = defaultdict(list)\n",
    "\n",
    "        for batch, (corpus, emb, target) in enumerate(loader):\n",
    "            target = target.reshape(target.shape[0], -1)\n",
    "            emb, target = emb.to(self.device), target.to(self.device)\n",
    "\n",
    "            self.model.zero_grad()\n",
    "            prior_mean, prior_variance, posterior_mean, posterior_variance,\\\n",
    "            posterior_log_variance, word_dists, recon_dists = self.model(emb, target)\n",
    "            \n",
    "            kl_loss, rl_loss, dl_loss = self.loss(target, word_dists, recon_dists, prior_mean, prior_variance,\n",
    "                              posterior_mean, posterior_variance, posterior_log_variance)\n",
    "\n",
    "            loss = self.weights[\"beta\"] * kl_loss + rl_loss + dl_loss\n",
    "            loss = loss.sum()\n",
    "\n",
    "            samples_processed += target.size()[0]\n",
    "            val_loss += loss.item()\n",
    "\n",
    "             # Semantic Prcision for reconstruct\n",
    "            precision_scores, word_result = semantic_precision_all(recon_dists, target, self.word_embeddings, self.vocab, k=self.config['topk'], th = self.config['threshold'])\n",
    "            for k, v in precision_scores.items():\n",
    "                results['[Recon] Semantic Precision v1@{}'.format(k)].append(v)\n",
    "\n",
    "            precision_scores, word_result = semantic_precision_all_v2(recon_dists, target, self.word_embeddings, self.vocab, k=self.config['topk'], th = self.config['threshold'])\n",
    "            for k, v in precision_scores.items():\n",
    "                results['[Recon] Semantic Precision v2@{}'.format(k)].append(v)\n",
    "                \n",
    "            # Precision for reconstruct\n",
    "            precision_scores = retrieval_precision_all(recon_dists, target, k=self.config['topk'])\n",
    "            for k, v in precision_scores.items():\n",
    "                results['[Recon] Precision v1@{}'.format(k)].append(v)\n",
    "            \n",
    "            precision_scores = retrieval_precision_all_v2(recon_dists, target, k=self.config['topk'])\n",
    "            for k, v in precision_scores.items():\n",
    "                results['[Recon] Precision v2@{}'.format(k)].append(v)\n",
    "\n",
    "            # NDCG for reconstruct\n",
    "            ndcg_scores = retrieval_normalized_dcg_all(recon_dists, target, k=self.config['topk'])\n",
    "            for k, v in ndcg_scores.items():\n",
    "                results['[Recon] ndcg@{}'.format(k)].append(v)\n",
    "\n",
    "            # Semantic Prcision for word dist\n",
    "            precision_scores, word_result = semantic_precision_all(word_dists, target, self.word_embeddings, self.vocab, k=self.config['topk'], th = self.config['threshold'])\n",
    "            for k, v in precision_scores.items():\n",
    "                dists['[Word Dist] Semantic Precision v1@{}'.format(k)].append(v)\n",
    "\n",
    "            precision_scores, word_result = semantic_precision_all_v2(word_dists, target, self.word_embeddings, self.vocab, k=self.config['topk'], th = self.config['threshold'])\n",
    "            for k, v in precision_scores.items():\n",
    "                dists['[Word Dist] Semantic Precision v2@{}'.format(k)].append(v)\n",
    "                \n",
    "            # Precision for word dist\n",
    "            precision_scores = retrieval_precision_all(word_dists, target, k=self.config['topk'])\n",
    "            for k, v in precision_scores.items():\n",
    "                dists['[Word Dist] Precision v1@{}'.format(k)].append(v)\n",
    "\n",
    "            precision_scores = retrieval_precision_all_v2(word_dists, target, k=self.config['topk'])\n",
    "            for k, v in precision_scores.items():\n",
    "                dists['[Word Dist] Precision v2@{}'.format(k)].append(v)\n",
    "\n",
    "            # NDCG for word dist\n",
    "            ndcg_scores = retrieval_normalized_dcg_all(word_dists, target, k=self.config['topk'])\n",
    "            for k, v in ndcg_scores.items():\n",
    "                dists['[Word Dist] ndcg@{}'.format(k)].append(v)\n",
    "        \n",
    "        for k in results:\n",
    "            results[k] = np.mean(results[k])\n",
    "        \n",
    "        for k in dists:\n",
    "            dists[k] = np.mean(dists[k])\n",
    "\n",
    "        val_loss /= samples_processed\n",
    "\n",
    "        return samples_processed, val_loss, results, dists\n",
    "\n",
    "    def fit(self, training_set, validation_set, n_samples=20):\n",
    "        self.model.to(self.device)\n",
    "        train_loader = DataLoader(training_set, batch_size=self.config['batch_size'], shuffle=True, num_workers=self.num_data_loader_workers)\n",
    "        validation_loader = DataLoader(validation_set, batch_size=self.config['batch_size'], shuffle=True, num_workers=self.num_data_loader_workers)\n",
    "        \n",
    "        train_loss = 0\n",
    "        samples_processed = 0\n",
    "        \n",
    "        pbar = tqdm(self.num_epochs, position=0, leave=True)\n",
    "        record_settings(self.config)\n",
    "\n",
    "        for epoch in range(self.config['epochs']):\n",
    "            s = datetime.datetime.now()\n",
    "            sp, train_loss = self.training(train_loader)\n",
    "            samples_processed += sp\n",
    "            e = datetime.datetime.now()\n",
    "            pbar.update(1)\n",
    "\n",
    "            if  (epoch + 1) % 10 == 0:\n",
    "                s = datetime.datetime.now()\n",
    "                val_samples_processed, val_loss, val_res, dist_res = self.validation(validation_loader)\n",
    "                e = datetime.datetime.now()\n",
    "\n",
    "                pbar.set_description(\"Epoch: [{}/{}]\\t Seen Samples: [{}/{}]\\tTrain Loss: {}\\tValid Loss: {}\\tTime: {}\".format(\n",
    "                    epoch + 1, self.num_epochs, samples_processed,\n",
    "                    len(training_set) * self.num_epochs, train_loss, val_loss, e - s))\n",
    "                \n",
    "                npmi = CoherenceNPMI(texts=self.texts, topics=self.get_topic_lists(10))\n",
    "                diversity = InvertedRBO(topics=self.get_topic_lists(10))\n",
    "                #record = open('./'+self.config['experiment']+'_'+self.config['dataset']+'_'+self.config['model']+'_'+self.config['architecture']+'_'+self.config['activation']+'_'+self.config['encoder']+'_'+self.config['target']+'_loss_'+self.config['loss']+'_lr'+str(self.config['lr'])+'_batch'+str(self.config['batch_size'])+'_weightdecay'+str(self.config['weight_decay'])+'.txt', 'a')\n",
    "                print('---------------------------------------')\n",
    "                #record.write('-------------------------------------------------\\n')\n",
    "                print('EPOCH', epoch + 1)\n",
    "                #record.write('EPOCH '+ str(epoch + 1) + '\\n')\n",
    "                for key,val in val_res.items():\n",
    "                    print(f\"{key}:{val:.4f}\")\n",
    "                    #record.write(f\"{key}:{val:.4f}\\n\")\n",
    "                for key,val in dist_res.items():\n",
    "                    print(f\"{key}:{val:.4f}\")\n",
    "                    #record.write(f\"{key}:{val:.4f}\\n\")\n",
    "                print('NPMI: ', npmi.score())\n",
    "                print('IRBO: ', diversity.score())\n",
    "                #record.write('NPMI: '+ str(npmi.score()) + '\\n')\n",
    "                #record.write('IRBO: '+ str(diversity.score()) + '\\n')\n",
    "\n",
    "            self.best_components = self.model.beta\n",
    "            pbar.set_description(\"Epoch: [{}/{}]\\t Seen Samples: [{}/{}]\\tTrain Loss: {}\\tTime: {}\".format(\n",
    "                epoch + 1, self.num_epochs, samples_processed,\n",
    "                len(training_set) * self.num_epochs, train_loss, e - s))\n",
    "        pbar.close()\n",
    "    \n",
    "    def get_topic_lists(self, k=10):\n",
    "        \"\"\"\n",
    "        Retrieve the lists of topic words.\n",
    "\n",
    "        :param k: (int) number of words to return per topic, default 10.\n",
    "        \"\"\"\n",
    "        assert k <= len(self.vocab), \"k must be <= input size.\"\n",
    "        # TODO: collapse this method with the one that just returns the topics\n",
    "        component_dists = self.best_components\n",
    "        topics = []\n",
    "        for i in range(self.n_components):\n",
    "            _, idxs = torch.topk(component_dists[i], k)\n",
    "            component_words = [self.idx2token[idx]\n",
    "                               for idx in idxs.cpu().numpy()]\n",
    "            topics.append(component_words)\n",
    "        return topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4017e232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "dataset = IDEDataset(unpreprocessed_corpus, doc_embs, labels[config['target']])\n",
    "training_length = int(len(dataset) * config['ratio'])\n",
    "validation_length = len(dataset) - training_length\n",
    "training_set, validation_set = random_split(dataset, lengths=[training_length, validation_length],generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a062bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [10/10]\t Seen Samples: [4820/4820]\tTrain Loss: 20.03272578132598\tValid Loss: 11.53534818286738\tTime: 0:00:17.283672: : 10it [02:06, 10.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "EPOCH 10\n",
      "[Recon] Semantic Precision v1@10:0.2773\n",
      "[Recon] Semantic Precision v1@30:0.1039\n",
      "[Recon] Semantic Precision v1@50:0.0630\n",
      "[Recon] Semantic Precision v2@10:0.2773\n",
      "[Recon] Semantic Precision v2@30:0.1039\n",
      "[Recon] Semantic Precision v2@50:0.0630\n",
      "[Recon] Precision v1@10:0.2719\n",
      "[Recon] Precision v1@30:0.1036\n",
      "[Recon] Precision v1@50:0.0630\n",
      "[Recon] Precision v2@10:0.2719\n",
      "[Recon] Precision v2@30:0.1036\n",
      "[Recon] Precision v2@50:0.0630\n",
      "[Recon] ndcg@10:0.8406\n",
      "[Recon] ndcg@30:0.8748\n",
      "[Recon] ndcg@50:0.8773\n",
      "[Recon] ndcg@all:0.8773\n",
      "[Word Dist] Semantic Precision v1@10:0.0789\n",
      "[Word Dist] Semantic Precision v1@30:0.0594\n",
      "[Word Dist] Semantic Precision v1@50:0.0495\n",
      "[Word Dist] Semantic Precision v2@10:0.0789\n",
      "[Word Dist] Semantic Precision v2@30:0.0594\n",
      "[Word Dist] Semantic Precision v2@50:0.0495\n",
      "[Word Dist] Precision v1@10:0.0461\n",
      "[Word Dist] Precision v1@30:0.0438\n",
      "[Word Dist] Precision v1@50:0.0427\n",
      "[Word Dist] Precision v2@10:0.0461\n",
      "[Word Dist] Precision v2@30:0.0438\n",
      "[Word Dist] Precision v2@50:0.0427\n",
      "[Word Dist] ndcg@10:0.0991\n",
      "[Word Dist] ndcg@30:0.1869\n",
      "[Word Dist] ndcg@50:0.2520\n",
      "[Word Dist] ndcg@all:0.3231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [10/10]\t Seen Samples: [4820/4820]\tTrain Loss: 20.03272578132598\tTime: 0:00:17.283672: : 10it [02:16, 13.68s/it]                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPMI:  -0.47155388081625854\n",
      "IRBO:  0.8990302476769841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = IDETopicDecoder(config, texts=texts, vocab = vocabularys[config['target']], idx2token=id2token, device=device, contextual_size=doc_embs.shape[1], word_embeddings=word_embeddings)\n",
    "model.fit(training_set, validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf815cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "doc_idx = []\n",
    "print(len(validation_set))\n",
    "for idx in range(200):\n",
    "    doc_idx.append(random.randint(0, len(validation_set)))\n",
    "print(doc_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fca275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [5/20]: : 5it [00:25,  5.08s/it]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "# visualize documents\n",
    "check_nums = 10\n",
    "for idx in doc_idx:\n",
    "    # get recontruct result\n",
    "    recon_list, target_list, doc_list = model.get_reconstruct(validation_set)\n",
    "\n",
    "    # get ranking index\n",
    "    recon_rank_list = np.zeros((len(recon_list), len(tp.vocab)), dtype='float32')\n",
    "    target_rank_list = np.zeros((len(recon_list), len(tp.vocab)), dtype='float32')\n",
    "    for i in range(len(recon_list)):\n",
    "        recon_rank_list[i] = np.argsort(recon_list[i])[::-1]\n",
    "        target_rank_list[i] = np.argsort(target_list[i])[::-1]\n",
    "\n",
    "        # show info\n",
    "    doc_topics_distribution = model.get_doc_topic_distribution(validation_set)\n",
    "    doc_topics = model.get_topic_lists()[np.argmax(doc_topics_distribution[idx])]\n",
    "    print('Documents ', idx)\n",
    "    print(doc_list[idx])\n",
    "    print('---------------------------------------')\n",
    "    print('Topic of Document: ')\n",
    "    print(doc_topics)\n",
    "    print('---------------------------------------')\n",
    "    print('[Predict] Top 10 Words in Document: ')\n",
    "    for word_idx in range(10):\n",
    "        print(dataset.idx2token[recon_rank_list[idx][word_idx]])\n",
    "    print('---------------------------------------')\n",
    "    print('[Label] Top 10 Words in Document: ')\n",
    "    for idx in range(10):\n",
    "        print(dataset.idx2token[target_rank_list[idx][word_idx]])\n",
    "        print('---------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c87640c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_list, target_list, doc_list = model.get_reconstruct(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "420270c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3770, 2000)\n"
     ]
    }
   ],
   "source": [
    "print(target_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c61672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "recon_list = recon_list\n",
    "recon_rank_list = np.zeros((len(recon_list), len(tp.vocab)), dtype='float32')\n",
    "target_rank_list = np.zeros((len(recon_list), len(tp.vocab)), dtype='float32')\n",
    "for i in range(len(recon_list)):\n",
    "        recon_rank_list[i] = np.argsort(recon_list[i])[::-1]\n",
    "        target_rank_list[i] = np.argsort(target_list[i])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99d25b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 179. 1844. 1907. ...  491.  577.  979.]\n"
     ]
    }
   ],
   "source": [
    "doc_idx = 1698\n",
    "print(recon_rank_list[doc_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92484660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 950. 1379.   76. ... 1402.  709. 1320.]\n",
      " [ 107.  310.  793. ... 1815. 1387. 1539.]\n",
      " [1072. 1269. 1006. ...  980.  893.   46.]\n",
      " ...\n",
      " [ 761. 1626. 1996. ... 1014. 1373. 1739.]\n",
      " [ 670.  693.  865. ...  582.   46.  772.]\n",
      " [ 290.  928.  904. ... 1205.  476.  463.]]\n"
     ]
    }
   ],
   "source": [
    "print(recon_rank_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c65d854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: dennisk@cs.uoregon.edu (Dennis Kennedy)\n",
      "Subject: '72 Chevelle SS forsale\n",
      "Organization: University of Oregon\n",
      "Lines: 11\n",
      "Distribution: usa\n",
      "NNTP-Posting-Host: fp2-cc-25.uoregon.edu\n",
      "\n",
      "I don't want to sell this car, but I need money for college.\n",
      "1972 Chevelle Super Sport\n",
      "Rebuilt 402, four speed, 12 Bolt positrac\n",
      "Numbers match\n",
      "110,000 original miles\n",
      "no rust\n",
      "Looks and runs excellent\n",
      "$5995 or best offer.\n",
      "Call Dennis at (503)343-3759\n",
      "or email dennisk@cs.uoregon.edu\n"
     ]
    }
   ],
   "source": [
    "print(doc_list[doc_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e48338b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already\n",
      "tv\n",
      "video\n",
      "late\n",
      "card\n",
      "display\n",
      "3t\n",
      "1t\n",
      "asked\n",
      "games\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "    print(dataset.idx2token[recon_rank_list[doc_idx][idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4e5f982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs\n",
      "72\n",
      "miles\n",
      "excellent\n",
      "runs\n",
      "numbers\n",
      "offer\n",
      "sell\n",
      "four\n",
      "looks\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "    print(dataset.idx2token[target_rank_list[doc_idx][idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e42c4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['windows',\n",
       "  'drive',\n",
       "  'card',\n",
       "  'disk',\n",
       "  'help',\n",
       "  'mac',\n",
       "  'dos',\n",
       "  'mouse',\n",
       "  'problem',\n",
       "  'pc'],\n",
       " ['god',\n",
       "  'jesus',\n",
       "  'sin',\n",
       "  'rutgers',\n",
       "  'christ',\n",
       "  'faith',\n",
       "  'athos',\n",
       "  'truth',\n",
       "  'sandvik',\n",
       "  'church'],\n",
       " ['jpeg',\n",
       "  'edu',\n",
       "  'gif',\n",
       "  'image',\n",
       "  'quality',\n",
       "  'format',\n",
       "  'images',\n",
       "  'get',\n",
       "  'programs',\n",
       "  'color'],\n",
       " ['gov',\n",
       "  'access',\n",
       "  'hst',\n",
       "  'nasa',\n",
       "  'shuttle',\n",
       "  'digex',\n",
       "  'net',\n",
       "  'jpl',\n",
       "  'pat',\n",
       "  'mission'],\n",
       " ['bike', 'dog', 'ca', 'com', 'ride', 'riding', 'dod', 'bnr', 'car', 'bmw'],\n",
       " ['10', '46', 'van', '12', '25', 'nj', '11', '64', '28', '60'],\n",
       " ['ax', 'max', 'giz', 'bhj', 'writes', 'g9v', '75u', 'pl', 'b8f', '2tm'],\n",
       " ['one',\n",
       "  'people',\n",
       "  'would',\n",
       "  'like',\n",
       "  'see',\n",
       "  'even',\n",
       "  'time',\n",
       "  'lord',\n",
       "  'said',\n",
       "  'us'],\n",
       " ['article',\n",
       "  'writes',\n",
       "  'muslims',\n",
       "  'islam',\n",
       "  'turkey',\n",
       "  'edu',\n",
       "  'greek',\n",
       "  'muslim',\n",
       "  'turks',\n",
       "  'turkish'],\n",
       " ['window',\n",
       "  'problem',\n",
       "  'program',\n",
       "  'help',\n",
       "  'nl',\n",
       "  'thanks',\n",
       "  'error',\n",
       "  'create',\n",
       "  'table',\n",
       "  'screen'],\n",
       " ['cx', 'mv', 'ax', '0d', 'mc', 'hz', 'sp', '6um', '6ei', 'sc'],\n",
       " ['04', '02', '03', '06', '05', 'lost', 'edu', '01', '07', '00'],\n",
       " ['com',\n",
       "  'scsi',\n",
       "  'writes',\n",
       "  'bus',\n",
       "  'article',\n",
       "  'ide',\n",
       "  'car',\n",
       "  'isa',\n",
       "  'drive',\n",
       "  'oracle'],\n",
       " ['game',\n",
       "  'ca',\n",
       "  'espn',\n",
       "  'baseball',\n",
       "  'games',\n",
       "  'toronto',\n",
       "  'fan',\n",
       "  'team',\n",
       "  'hockey',\n",
       "  'show'],\n",
       " ['edu',\n",
       "  'image',\n",
       "  'data',\n",
       "  'analysis',\n",
       "  'processing',\n",
       "  'graphics',\n",
       "  'ftp',\n",
       "  '3d',\n",
       "  'gov',\n",
       "  'images'],\n",
       " ['car',\n",
       "  'dod',\n",
       "  'nasa',\n",
       "  'launch',\n",
       "  'space',\n",
       "  'oil',\n",
       "  'bike',\n",
       "  'cars',\n",
       "  'engine',\n",
       "  'driving'],\n",
       " ['com',\n",
       "  'windows',\n",
       "  'server',\n",
       "  'edu',\n",
       "  'os',\n",
       "  'dos',\n",
       "  'window',\n",
       "  'key',\n",
       "  'motif',\n",
       "  'subject'],\n",
       " ['atheism',\n",
       "  'atheists',\n",
       "  'atheist',\n",
       "  'alt',\n",
       "  'universe',\n",
       "  'one',\n",
       "  'people',\n",
       "  'existence',\n",
       "  'religious',\n",
       "  'god'],\n",
       " ['sale',\n",
       "  'offer',\n",
       "  'selling',\n",
       "  'condition',\n",
       "  'ex',\n",
       "  'shipping',\n",
       "  '1988',\n",
       "  'excellent',\n",
       "  'college',\n",
       "  'asking'],\n",
       " ['morality',\n",
       "  'objective',\n",
       "  'frank',\n",
       "  'moral',\n",
       "  'writes',\n",
       "  'values',\n",
       "  'islam',\n",
       "  'god',\n",
       "  'islamic',\n",
       "  'absolute'],\n",
       " ['people',\n",
       "  'one',\n",
       "  'us',\n",
       "  'azerbaijan',\n",
       "  'would',\n",
       "  'went',\n",
       "  'building',\n",
       "  'said',\n",
       "  'armenian',\n",
       "  'armenians'],\n",
       " ['thanks',\n",
       "  'advance',\n",
       "  'hi',\n",
       "  'address',\n",
       "  'looking',\n",
       "  'please',\n",
       "  'anyone',\n",
       "  'ch',\n",
       "  'site',\n",
       "  'ac'],\n",
       " ['writes',\n",
       "  'article',\n",
       "  'clutch',\n",
       "  'players',\n",
       "  'edu',\n",
       "  'year',\n",
       "  'morris',\n",
       "  'player',\n",
       "  'team',\n",
       "  'baseball'],\n",
       " ['ax',\n",
       "  'edu',\n",
       "  'article',\n",
       "  'writes',\n",
       "  'nntp',\n",
       "  'host',\n",
       "  'giz',\n",
       "  'max',\n",
       "  'posting',\n",
       "  'g9v'],\n",
       " ['article',\n",
       "  'writes',\n",
       "  'cramer',\n",
       "  'gay',\n",
       "  'optilink',\n",
       "  'clayton',\n",
       "  'sgi',\n",
       "  'men',\n",
       "  'livesey',\n",
       "  'homosexual'],\n",
       " ['edu',\n",
       "  'article',\n",
       "  'writes',\n",
       "  'insurance',\n",
       "  'drugs',\n",
       "  'guns',\n",
       "  'batf',\n",
       "  'health',\n",
       "  'fbi',\n",
       "  'would'],\n",
       " ['clipper',\n",
       "  'chip',\n",
       "  'escrow',\n",
       "  'encryption',\n",
       "  'keys',\n",
       "  'key',\n",
       "  'nsa',\n",
       "  'phone',\n",
       "  'secret',\n",
       "  'brad'],\n",
       " ['de',\n",
       "  'tu',\n",
       "  'windows',\n",
       "  'au',\n",
       "  'graphics',\n",
       "  'ac',\n",
       "  'files',\n",
       "  'germany',\n",
       "  'using',\n",
       "  'help'],\n",
       " ['article',\n",
       "  'edu',\n",
       "  'writes',\n",
       "  'nntp',\n",
       "  'host',\n",
       "  'posting',\n",
       "  'university',\n",
       "  'scsi',\n",
       "  'car',\n",
       "  'cwru'],\n",
       " ['et', 'body', 'tm', 'koresh', 'james', 'ex', 'food', 'tax', 'tek', 'normal'],\n",
       " ['stephanopoulos',\n",
       "  'mr',\n",
       "  'think',\n",
       "  'president',\n",
       "  'would',\n",
       "  'george',\n",
       "  'jobs',\n",
       "  'package',\n",
       "  'russia',\n",
       "  'know'],\n",
       " ['writes',\n",
       "  'article',\n",
       "  'edu',\n",
       "  'team',\n",
       "  'morris',\n",
       "  'year',\n",
       "  'roger',\n",
       "  'games',\n",
       "  'season',\n",
       "  'game'],\n",
       " ['sale',\n",
       "  'offer',\n",
       "  'condition',\n",
       "  'distribution',\n",
       "  'asking',\n",
       "  'excellent',\n",
       "  'sell',\n",
       "  'trade',\n",
       "  'included',\n",
       "  'price'],\n",
       " ['com',\n",
       "  'writes',\n",
       "  'article',\n",
       "  'br',\n",
       "  'isc',\n",
       "  'stratus',\n",
       "  'sw',\n",
       "  'nntp',\n",
       "  'distribution',\n",
       "  'rocket'],\n",
       " ['soviet',\n",
       "  'serdar',\n",
       "  'argic',\n",
       "  'escape',\n",
       "  'armenian',\n",
       "  'serve',\n",
       "  'mountain',\n",
       "  'turks',\n",
       "  'genocide',\n",
       "  'professor'],\n",
       " ['board',\n",
       "  'card',\n",
       "  'thanks',\n",
       "  'bus',\n",
       "  'drive',\n",
       "  'video',\n",
       "  'driver',\n",
       "  'anyone',\n",
       "  'port',\n",
       "  'pc'],\n",
       " ['baseball',\n",
       "  'virginia',\n",
       "  'buffalo',\n",
       "  'toronto',\n",
       "  'boston',\n",
       "  'night',\n",
       "  'cmu',\n",
       "  'reserve',\n",
       "  'espn',\n",
       "  'college'],\n",
       " ['pp', 'pts', 'period', 'goals', 'goal', 'st', '25', '11', 'play', 'pit'],\n",
       " ['scsi',\n",
       "  'drive',\n",
       "  'controller',\n",
       "  'drives',\n",
       "  'disk',\n",
       "  'ide',\n",
       "  'bios',\n",
       "  'tape',\n",
       "  'rom',\n",
       "  'card'],\n",
       " ['gun',\n",
       "  'people',\n",
       "  'guns',\n",
       "  'would',\n",
       "  'amendment',\n",
       "  'firearms',\n",
       "  'weapons',\n",
       "  'koresh',\n",
       "  'right',\n",
       "  'constitution'],\n",
       " ['sale',\n",
       "  'books',\n",
       "  '00',\n",
       "  'offer',\n",
       "  'please',\n",
       "  'price',\n",
       "  'cd',\n",
       "  'book',\n",
       "  'list',\n",
       "  'condition'],\n",
       " ['god',\n",
       "  'sin',\n",
       "  'church',\n",
       "  'mary',\n",
       "  'bible',\n",
       "  'christ',\n",
       "  'spirit',\n",
       "  'jesus',\n",
       "  'christian',\n",
       "  'paul'],\n",
       " ['com',\n",
       "  'writes',\n",
       "  'article',\n",
       "  'uk',\n",
       "  'convex',\n",
       "  'co',\n",
       "  'crypto',\n",
       "  'escrow',\n",
       "  'key',\n",
       "  'att'],\n",
       " ['privacy',\n",
       "  'cryptography',\n",
       "  'internet',\n",
       "  'mail',\n",
       "  'key',\n",
       "  'security',\n",
       "  'sci',\n",
       "  'secure',\n",
       "  'des',\n",
       "  'anonymous'],\n",
       " ['edu',\n",
       "  'article',\n",
       "  'writes',\n",
       "  'gordon',\n",
       "  'banks',\n",
       "  'keith',\n",
       "  'pitt',\n",
       "  'nntp',\n",
       "  'caltech',\n",
       "  'host'],\n",
       " ['thanks',\n",
       "  'site',\n",
       "  'address',\n",
       "  'sites',\n",
       "  'info',\n",
       "  'anyone',\n",
       "  'looking',\n",
       "  'et',\n",
       "  'ch',\n",
       "  'pre'],\n",
       " ['education',\n",
       "  'cancer',\n",
       "  'un',\n",
       "  'et',\n",
       "  'popular',\n",
       "  'released',\n",
       "  'pre',\n",
       "  'waiting',\n",
       "  'training',\n",
       "  'aids'],\n",
       " ['israel',\n",
       "  'israeli',\n",
       "  'arab',\n",
       "  'jewish',\n",
       "  'jews',\n",
       "  'arabs',\n",
       "  'adam',\n",
       "  'policy',\n",
       "  'attacks',\n",
       "  'peace'],\n",
       " ['edu',\n",
       "  'article',\n",
       "  'writes',\n",
       "  'nntp',\n",
       "  'atf',\n",
       "  'host',\n",
       "  'indiana',\n",
       "  'posting',\n",
       "  'ohio',\n",
       "  'university'],\n",
       " ['health',\n",
       "  'disease',\n",
       "  'medical',\n",
       "  'page',\n",
       "  'patients',\n",
       "  'volume',\n",
       "  'reported',\n",
       "  'age',\n",
       "  'risk',\n",
       "  'use']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_topic_lists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d5706b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [20/20]: : 20it [03:04,  9.25s/it]\n"
     ]
    }
   ],
   "source": [
    "doc_topics_distribution = model.get_doc_topic_distribution(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "256d0fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00264944, 0.00154069, 0.00157822, 0.00240728, 0.00557763,\n",
       "       0.00427778, 0.00276508, 0.00872498, 0.04012846, 0.00356483,\n",
       "       0.00288309, 0.00270965, 0.00298711, 0.00746678, 0.00177612,\n",
       "       0.00248846, 0.00091876, 0.00518265, 0.00199116, 0.01588894,\n",
       "       0.08754831, 0.00166128, 0.00302516, 0.00350954, 0.00638637,\n",
       "       0.00344035, 0.00659486, 0.00204372, 0.00246886, 0.01168864,\n",
       "       0.01562314, 0.00923532, 0.00320664, 0.00575135, 0.02873103,\n",
       "       0.00292833, 0.0047355 , 0.00574315, 0.00327179, 0.00780392,\n",
       "       0.00383813, 0.00213848, 0.00608836, 0.00335248, 0.00340084,\n",
       "       0.00218007, 0.00324257, 0.63300758, 0.00526384, 0.00458329])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topics_distribution[doc_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80cb1748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['israel', 'israeli', 'arab', 'jewish', 'jews', 'arabs', 'adam', 'policy', 'attacks', 'peace']\n"
     ]
    }
   ],
   "source": [
    "doc_topics = model.get_topic_lists()[np.argmax(doc_topics_distribution[doc_idx])]\n",
    "print(doc_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcf56ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        test.append(unpreprocessed_corpus[i+j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0803b9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (/dhome/casimir0304/.cache/huggingface/datasets/tweet_eval/emotion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
      "/data1/IDE/casimir0304/ide/utils/preprocessing.py:24: UserWarning: WhiteSpacePreprocessing is deprecated and will be removed in future versions.Use WhiteSpacePreprocessingStopwords.\n",
      "  warnings.warn(\"WhiteSpacePreprocessing is deprecated and will be removed in future versions.\"\n",
      "/dhome/casimir0304/miniconda3/envs/ML/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "raw_documents = load_document(config['dataset'])[\"documents\"]\n",
    "preprocessed_documents, unpreprocessed_corpus, texts = preprocess_document(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70e3da88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "raw_documents = load_document(config['dataset'])[\"documents\"]\n",
    "preprocessed_documents, unpreprocessed_corpus, texts = preprocess_document(raw_documents)\n",
    "vectorizer = TfidfVectorizer(token_pattern=r'(?u)\\b[\\w+|\\-]+\\b')\n",
    "decode_target = vectorizer.fit_transform(preprocessed_documents)\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "id2token = {k: v for k, v in zip(range(0, len(vocab)), vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f531a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token = {k: v for k, v in zip(range(0, len(vocab)), vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "478f6de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4614, 1719)\n"
     ]
    }
   ],
   "source": [
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31de50f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
