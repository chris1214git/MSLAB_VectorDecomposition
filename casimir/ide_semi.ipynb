{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd92d923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from load_pretrain_label import load_preprocess_document_labels\n",
    "#from model.ide_ae_decoder import IDEDataset, IDEAEDecoder\n",
    "from utils.toolbox import same_seeds, show_settings, record_settings, get_preprocess_document, get_preprocess_document_embs, get_preprocess_document_labels, get_word_embs, merge_targets\n",
    "from utils.eval import retrieval_normalized_dcg_all, retrieval_precision_all, semantic_precision_all, retrieval_precision_all_v2, semantic_precision_all_v2\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.set_num_threads(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f8ece2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(config):    \n",
    "    # Data preprocessing\n",
    "    unpreprocessed_corpus ,preprocessed_corpus = get_preprocess_document(**config)\n",
    "    texts = [text.split() for text in preprocessed_corpus]\n",
    "    print('[INFO] Load corpus done.')\n",
    "\n",
    "    # Generating document embedding\n",
    "    while True:\n",
    "        try:\n",
    "            doc_embs, doc_model, device = get_preprocess_document_embs(preprocessed_corpus, config['encoder'])\n",
    "            break\n",
    "        except:\n",
    "            print('[Error] CUDA Memory Insufficient, retry after 15 secondes.')\n",
    "            time.sleep(15)\n",
    "    print('[INFO] Generate embedding done.')\n",
    "    \n",
    "    # Generate Decode target & Vocabulary\n",
    "    if config['target'] == 'keybert' or config['target'] == 'yake':\n",
    "        labels, vocabularys= load_preprocess_document_labels(config)\n",
    "        label = labels[config['target']].toarray()\n",
    "    else:\n",
    "        labels, vocabularys= get_preprocess_document_labels(preprocessed_corpus)\n",
    "        label = labels[config['target']]\n",
    "        vocabularys = vocabularys[config['target']]\n",
    "    print('[INFO] Load label done.')\n",
    "    \n",
    "    # generate idx to token\n",
    "    id2token = {k: v for k, v in zip(range(0, len(vocabularys)), vocabularys)}\n",
    "    print('[INFO] Generate id2token done.')\n",
    "    \n",
    "    idx = np.arange(len(unpreprocessed_corpus))\n",
    "    np.random.shuffle(idx)\n",
    "    train_length = int(len(unpreprocessed_corpus) * 0.8)\n",
    "    train_idx = idx[:train_length]\n",
    "    valid_idx = idx[train_length:]\n",
    "\n",
    "    train_unpreprocessed_corpus = list(np.array(unpreprocessed_corpus)[train_idx])\n",
    "    valid_unpreprocessed_corpus = list(np.array(unpreprocessed_corpus)[valid_idx])\n",
    "    train_embs = np.array(doc_embs)[train_idx]\n",
    "    valid_embs = np.array(doc_embs)[valid_idx]\n",
    "    train_label = np.array(label)[train_idx]\n",
    "    valid_label = np.array(label)[valid_idx]\n",
    "    \n",
    "    # Generate labeled mask\n",
    "    label_masks = np.zeros((train_embs.shape[0], 1), dtype=bool)\n",
    "    num_labeled_data = int(train_embs.shape[0] * config['ratio'])\n",
    "    while True:\n",
    "        if num_labeled_data > 0:\n",
    "            idx = random.randrange(0, train_embs.shape[0])\n",
    "            if label_masks[idx] == 0:\n",
    "                label_masks[idx] = 1\n",
    "                num_labeled_data -= 1\n",
    "        else:\n",
    "            break\n",
    "    print('[INFO] mask labels done.')\n",
    "\n",
    "    # Balance data if required\n",
    "    original_num_data = train_embs.shape[0]\n",
    "    if config['ratio'] != 1 and config['balance']:\n",
    "        print('[INFO] Balance required.')\n",
    "        for idx in range(original_num_data): \n",
    "            if label_masks[idx]:\n",
    "                balance = int(1/config['ratio'])\n",
    "                balance = int(math.log(balance,2))\n",
    "                if balance < 1:\n",
    "                    balance = 1\n",
    "                for b in range(0, int(balance)):\n",
    "                    train_unpreprocessed_corpus.append(train_unpreprocessed_corpus[idx])\n",
    "                    train_embs = np.concatenate((train_embs, train_embs[idx].reshape(1, train_embs.shape[1])), axis=0)\n",
    "                    train_label = np.concatenate((train_label, train_label[idx].reshape(1, train_label.shape[1])), axis=0)\n",
    "                    label_masks = np.concatenate((label_masks, label_masks[idx].reshape(1, label_masks.shape[1])), axis=0)\n",
    "    \n",
    "    training_set = IDEDataset(train_unpreprocessed_corpus, train_embs, train_label, label_masks)\n",
    "    validation_set = IDEDataset(valid_unpreprocessed_corpus, valid_embs, valid_label, np.ones((valid_embs.shape[0], 1), dtype=bool))\n",
    "    \n",
    "    return training_set, validation_set, vocabularys, id2token, device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e2adad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'experiment': 'autoencoder_testting',\n",
    "    'model': 'VAE',\n",
    "    'architecture': 'concatenate',\n",
    "    'activation': 'sigmoid',\n",
    "    'dataset': '20news',\n",
    "    'vocab_size':0,\n",
    "    'encoder': 'mpnet',\n",
    "    'target': 'tf-idf-gensim',\n",
    "    'seed': 123,\n",
    "    'epochs': 300,\n",
    "    'ae_epochs':10,\n",
    "    'lr': 1e-4,\n",
    "    'ae_lr':1e-4,\n",
    "    'optim': 'AdamW',\n",
    "    'scheduler': False,\n",
    "    'warmup': 'linear',\n",
    "    'warmup_proportion': 0.1, \n",
    "    'loss': 'listnet',\n",
    "    'batch_size': 32,\n",
    "    'weight_decay': 0,\n",
    "    'ratio': 0.1,\n",
    "    'topk': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'save': False,\n",
    "    'threshold': 0.7,\n",
    "    'balance': False,\n",
    "}\n",
    "same_seeds(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79161887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter\n",
    "if config['dataset'] == '20news':\n",
    "    config['min_df'], config['max_df'], config['min_doc_word'] = 62, 1.0, 15\n",
    "elif config['dataset'] == 'agnews':\n",
    "    config['min_df'], config['max_df'], config['min_doc_word'] = 425, 1.0, 15\n",
    "elif config['dataset'] == 'IMDB':\n",
    "    config['min_df'], config['max_df'], config['min_doc_word'] = 166, 1.0, 15\n",
    "elif config['dataset'] == 'wiki':\n",
    "    config['min_df'], config['max_df'], config['min_doc_word'] = 2872, 1.0, 15\n",
    "elif config['dataset'] == 'tweet':\n",
    "    config['min_df'], config['max_df'], config['min_doc_word'] = 5, 1.0, 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa25b34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule_with_warmup, BertTokenizer, BertForMaskedLM, RobertaTokenizer, RobertaForMaskedLM, AlbertTokenizer, AlbertForMaskedLM\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(\"./\")\n",
    "from utils.loss import Singular_MythNet\n",
    "from utils.eval import retrieval_normalized_dcg_all, retrieval_precision_all, semantic_precision_all, retrieval_precision_all_v2, semantic_precision_all_v2\n",
    "from utils.toolbox import get_free_gpu, record_settings\n",
    "from model.inference_network import ContextualInferenceNetwork\n",
    "\n",
    "class IDEDataset(Dataset):\n",
    "    def __init__(self, corpus, emb, target, mask):\n",
    "        \n",
    "        assert len(emb) == len(target)\n",
    "        self.corpus = corpus\n",
    "        self.emb = torch.FloatTensor(emb)\n",
    "        self.target = torch.FloatTensor(target)\n",
    "        self.mask = torch.BoolTensor(mask)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.corpus[idx], self.emb[idx], self.target[idx], self.mask[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emb)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = BertForMaskedLM.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "    def forward(self, documents):\n",
    "        return self.get_docvec(documents)\n",
    "\n",
    "    def get_docvec(self, documents):\n",
    "        inputs = self.tokenizer(documents, return_tensors='pt', padding=True,\n",
    "                                truncation=True, max_length=128).to(self.device)\n",
    "        embedding = self.model.bert(**inputs).last_hidden_state[:, 0, :]\n",
    "        return embedding\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim=768, output_dim=100, dropout=0.2):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim*4),\n",
    "            nn.BatchNorm1d(input_dim*4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(input_dim*4, output_dim),\n",
    "            nn.BatchNorm1d(output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, embs):\n",
    "        recons = self.decoder(embs)\n",
    "        return recons\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim=768, output_dim=2):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.logit = nn.Linear(input_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, embs):\n",
    "        logits = self.logit(embs)\n",
    "        probs = self.softmax(logits)\n",
    "        return logits, probs\n",
    "    \n",
    "class MLPDecoder(nn.Module):\n",
    "    def __init__(self, input_dim=768, output_dim=768, dropout=0.2):\n",
    "        super(MLPDecoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim*4),\n",
    "            nn.BatchNorm1d(input_dim*4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(input_dim*4, output_dim),\n",
    "            nn.BatchNorm1d(output_dim),\n",
    "        )\n",
    "    def forward(self, embs):\n",
    "        recons = self.decoder(embs)\n",
    "        return recons\n",
    "    \n",
    "class VariationalAE(nn.Module):\n",
    "    def __init__(self, config, device, vocab_size, contextual_size=768, encoded_size=768, n_components=50, hidden_sizes=(100,100), activation='relu', dropout=0.2, learn_priors=True):\n",
    "        super(VariationalAE, self).__init__()\n",
    "\n",
    "        assert activation in ['softplus', 'relu']\n",
    "\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.vocab_size = vocab_size\n",
    "        self.contextual_size = contextual_size\n",
    "        self.encoded_size = encoded_size\n",
    "        self.n_components = n_components\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.learn_priors = learn_priors\n",
    "        self.topic_word_matrix = None\n",
    "\n",
    "        # decoder architecture\n",
    "        self.batch_norm = nn.BatchNorm1d(vocab_size)\n",
    "        self.word_embedding =  nn.Parameter(torch.randn(vocab_size*4, vocab_size))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoded_size, contextual_size*4),\n",
    "            nn.BatchNorm1d(contextual_size*4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(contextual_size*4, contextual_size),\n",
    "            nn.BatchNorm1d(contextual_size),\n",
    "        )\n",
    "        \n",
    "        # topic model architecture\n",
    "        self.inf_net = ContextualInferenceNetwork(encoded_size, contextual_size, n_components, hidden_sizes, activation, label_size=0)\n",
    "        \n",
    "        topic_prior_mean = 0.0\n",
    "        self.prior_mean = torch.tensor([topic_prior_mean] * n_components).to(device)\n",
    "        if self.learn_priors:\n",
    "            self.prior_mean = nn.Parameter(self.prior_mean)\n",
    "\n",
    "        topic_prior_variance = 1. - (1. / self.n_components)\n",
    "        self.prior_variance = torch.tensor([topic_prior_variance] * n_components).to(device)\n",
    "        if self.learn_priors:\n",
    "            self.prior_variance = nn.Parameter(self.prior_variance)\n",
    "\n",
    "        self.beta = torch.Tensor(n_components, encoded_size).to(device)\n",
    "        self.beta = nn.Parameter(self.beta)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.beta)\n",
    "        \n",
    "        self.beta_batchnorm = nn.BatchNorm1d(encoded_size, affine=False)\n",
    "        \n",
    "        self.drop_theta = nn.Dropout(p=self.dropout)\n",
    "    \n",
    "    @staticmethod\n",
    "    def reparameterize(mu, logvar):\n",
    "        \"\"\"Reparameterize the theta distribution.\"\"\"\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def forward(self, emb, target, labels=None):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        posterior_mu, posterior_log_sigma = self.inf_net(target, emb, labels)\n",
    "        posterior_sigma = torch.exp(posterior_log_sigma)\n",
    "\n",
    "        # generate samples from theta\n",
    "        theta = F.softmax(self.reparameterize(posterior_mu, posterior_log_sigma), dim=1)\n",
    "        theta = self.drop_theta(theta)\n",
    "\n",
    "        # prodLDA\n",
    "        # in: batch_size x input_size x n_components\n",
    "        word_dist = F.softmax(self.beta_batchnorm(torch.matmul(theta, self.beta)), dim=1)\n",
    "        # word_dist: batch_size x input_size\n",
    "        \n",
    "        self.topic_word_matrix = self.beta\n",
    "        \n",
    "        # decode\n",
    "        recon = self.decoder(word_dist);\n",
    "        return self.prior_mean, self.prior_variance, posterior_mu, posterior_sigma, posterior_log_sigma, word_dist, recon\n",
    "    \n",
    "    def get_theta(self, target, emb, labels=None):\n",
    "        with torch.no_grad():\n",
    "            posterior_mu, posterior_log_sigma = self.inf_net(target, emb, labels)\n",
    "            theta = F.softmax(self.reparameterize(posterior_mu, posterior_log_sigma), dim=1)\n",
    "\n",
    "            return theta\n",
    "\n",
    "\n",
    "class IDEAEDecoder:\n",
    "    def __init__(self, config, train_set, valid_set, vocab = None, id2token=None, device=None, contextual_dim=768, encoded_dim=768, noise_dim=100, word_embeddings=None, dropout=0.2, momentum=0.99, num_data_loader_workers=mp.cpu_count(), loss_weights=None, eps=1e-8):\n",
    "        self.config = config\n",
    "        self.train_set = train_set\n",
    "        self.valid_set = valid_set\n",
    "        self.vocab = vocab\n",
    "        self.id2token = id2token\n",
    "        self.device = device\n",
    "        self.contextual_dim = contextual_dim\n",
    "        self.encoded_dim = encoded_dim\n",
    "        self.noise_dim = noise_dim\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.dropout = dropout\n",
    "        self.momentum = momentum\n",
    "        self.num_data_loader_workers = num_data_loader_workers\n",
    "        self.loss_weights = loss_weights\n",
    "        self.eps = eps\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.cls_loss = torch.nn.CrossEntropyLoss()\n",
    "        self.mse_loss = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "        # model\n",
    "        self.vae = VariationalAE(config, device, len(vocab), contextual_dim, encoded_dim, 50, (100, 100), 'relu', 0.2, True)\n",
    "        self.decoder = MLPDecoder(encoded_dim, len(vocab), 0.2)\n",
    "        self.generator = Generator(device)\n",
    "        self.discriminator = Discriminator(input_dim=contextual_dim, output_dim=len(vocab), dropout=dropout)\n",
    "        self.classifier = Classifier(input_dim=contextual_dim, output_dim=2)\n",
    "        \n",
    "        # optimizer\n",
    "        if config['optim'] == 'AdamW':\n",
    "            self.vae_optimizer = AdamW(self.vae.parameters(), lr=config['ae_lr'], eps=eps)\n",
    "            self.decoder_optimizer = AdamW(self.decoder.parameters(), lr=config['lr'], eps=eps)\n",
    "            self.gen_optimizer = AdamW(self.generator.parameters(), lr=config['lr'], eps=eps)\n",
    "            self.dis_optimizer = AdamW(self.discriminator.parameters(), lr=config['lr'], eps=eps)\n",
    "            self.cls_optimizer = AdamW(self.classifier.parameters(), lr=config['lr'], eps=eps)\n",
    "        else:\n",
    "            self.vae_optimizer = torch.optim.Adam(self.vae.parameters(), lr=config['ae_lr'], betas=(self.momentum, 0.99), weight_decay=config['weight_decay'])\n",
    "            self.decoder_optimizer = torch.optim.Adam(self.decoder.parameters(), lr=config['lr'], betas=(self.momentum, 0.99), weight_decay=config['weight_decay'])\n",
    "            self.gen_optimizer = torch.optim.Adam(self.generator.parameters(), lr=config['lr'], betas=(self.momentum, 0.99), weight_decay=config['weight_decay'])\n",
    "            self.dis_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=config['lr'], betas=(self.momentum, 0.99), weight_decay=config['weight_decay'])\n",
    "            self.cls_optimizer = torch.optim.Adam(self.classifier.parameters(), lr=config['lr'], betas=(self.momentum, 0.99), weight_decay=config['weight_decay'])\n",
    "        \n",
    "        # scheduler\n",
    "        if config['scheduler']:\n",
    "            num_training_steps = int(len(train_set) / config['batch_size'] * config['epochs'])\n",
    "            num_warmup_steps = int(num_training_steps * config['warmup_proportion'])\n",
    "            self.vae_optimizer = AdamW(self.vae.parameters(), lr=config['ae_lr'], eps=eps)\n",
    "            self.decoder_optimizer = AdamW(self.decoder.parameters(), lr=config['lr'], eps=eps)\n",
    "            self.gen_optimizer = AdamW(self.generator.parameters(), lr=config['lr'], eps=eps)\n",
    "            self.dis_optimizer = AdamW(self.discriminator.parameters(), lr=config['lr'], eps=eps)\n",
    "            self.cls_optimizer = AdamW(self.classifier.parameters(), lr=config['lr'], eps=eps)\n",
    "            if config['warmup'] == 'linear':\n",
    "                self.vae_scheduler = get_linear_schedule_with_warmup(self.vae_optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "                self.decoder_scheduler = get_linear_schedule_with_warmup(self.decoder_optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "                self.gen_scheduler = get_linear_schedule_with_warmup(self.gen_optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "                self.dis_scheduler = get_linear_schedule_with_warmup(self.dis_optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "                self.cls_scheduler = get_linear_schedule_with_warmup(self.cls_optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "            else:\n",
    "                self.vae_scheduler = get_constant_schedule_with_warmup(self.vae_optimizer, num_warmup_steps=num_warmup_steps)\n",
    "                self.decoder_scheduler = get_constant_schedule_with_warmup(self.decoder_optimizer, num_warmup_steps=num_warmup_steps)\n",
    "                self.gen_scheduler = get_constant_schedule_with_warmup(self.gen_optimizer, num_warmup_steps=num_warmup_steps)\n",
    "                self.dis_scheduler = get_constant_schedule_with_warmup(self.dis_optimizer, num_warmup_steps=num_warmup_steps)\n",
    "                self.cls_scheduler = get_constant_schedule_with_warmup(self.cls_optimizer, num_warmup_steps=num_warmup_steps)\n",
    "                \n",
    "    def ae_training(self, epoch, loader):\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch + 1, self.config['ae_epochs']))\n",
    "        print('AutoEncoder Training...')\n",
    "\n",
    "        ae_train_loss = 0\n",
    "        ae_train_cos = 0\n",
    "        \n",
    "        self.vae.train()\n",
    "\n",
    "        for batch, (corpus, embs, labels, masks) in enumerate(loader):\n",
    "            embs, masks = embs.to(self.device), masks.to(self.device)\n",
    "            _, _, _, _, _, encoded, decoded = self.vae(embs, embs)\n",
    "            \n",
    "            # Loss weight\n",
    "            cos = torch.nn.functional.cosine_similarity(torch.mean(embs, dim=0), torch.mean(decoded, dim=0), dim=0)\n",
    "            # w = 1 - cos\n",
    "            \n",
    "            # Encode-Decode's Loss\n",
    "            recon_loss = torch.mean(self.mse_loss(decoded, embs), dim=1)\n",
    "            mask_loss = torch.masked_select(recon_loss, torch.flatten(~masks))     \n",
    "            decoded_loss = torch.mean(mask_loss)\n",
    "            print(decoded_loss)\n",
    "                       \n",
    "            self.vae_optimizer.zero_grad()\n",
    "            decoded_loss.backward() \n",
    "            self.vae_optimizer.step()\n",
    "\n",
    "            ae_train_loss += decoded_loss.item()\n",
    "            ae_train_cos += cos\n",
    "\n",
    "        avg_ae_train_loss = ae_train_loss / len(loader)  \n",
    "        avg_ae_train_cos = ae_train_cos / len(loader)\n",
    "        \n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss AutoEncoder: {0:.3f}\".format(avg_ae_train_loss))\n",
    "        print(\"  Average training Cosine-Similarity AutoEncoder: {0:.3f}\".format(avg_ae_train_cos))\n",
    "\n",
    "        return avg_ae_train_loss, avg_ae_train_cos\n",
    "    \n",
    "    def mlp_training(self, epoch, loader):\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch + 1, self.config['epochs']))\n",
    "        print('Decoder Training...')\n",
    "\n",
    "        decode_train_loss = 0\n",
    "\n",
    "        self.vae.eval()\n",
    "        self.decoder.train()\n",
    "\n",
    "        for batch, (corpus, embs, labels, masks) in enumerate(loader):\n",
    "            embs, labels, masks = embs.to(self.device), labels.to(self.device), masks.to(self.device)\n",
    "\n",
    "            # VAE transform\n",
    "            _, _, _, _, _, encoded, _ = self.vae(embs, embs)   \n",
    "            \n",
    "            # Decode\n",
    "            recons = self.decoder(encoded)\n",
    "            \n",
    "            # Decoder's LOSS\n",
    "            mask_loss = torch.masked_select(Singular_MythNet(recons, labels), torch.flatten(masks))\n",
    "            labeled_count = mask_loss.type(torch.float32).numel()\n",
    "            if labeled_count == 0:\n",
    "                continue\n",
    "            else:\n",
    "                decoded_loss = torch.mean(mask_loss)\n",
    "            \n",
    "            self.decoder_optimizer.zero_grad()\n",
    "            decoded_loss.backward() \n",
    "            self.decoder_optimizer.step()\n",
    "\n",
    "            decode_train_loss += decoded_loss.item()\n",
    "\n",
    "        avg_decoded_train_loss = decode_train_loss / len(loader)             \n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss decoder: {0:.3f}\".format(avg_decoded_train_loss))\n",
    "\n",
    "        return avg_decoded_train_loss\n",
    "    \n",
    "    def gen_training(self, epoch, loader):\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch + 1, self.config['epochs']))\n",
    "        print('Bert GAN Training...')\n",
    "        \n",
    "        gen_train_loss = 0\n",
    "        \n",
    "        #self.vae.train()\n",
    "        self.generator.train()\n",
    "        self.discriminator.eval()\n",
    "        self.classifier.eval()\n",
    "\n",
    "        for batch, (corpus, embs, labels, masks) in enumerate(loader):\n",
    "            real_embs, labels, masks = embs.to(self.device), labels.to(self.device), masks.to(self.device)\n",
    "            cur_batch_size = embs.shape[0]\n",
    "            \n",
    "            # vae transform\n",
    "            #real_embs_t = self.vae(real_embs)\n",
    "            real_embs_t = real_embs\n",
    "            \n",
    "            # fake label from BERT\n",
    "            noise = torch.empty(cur_batch_size, dtype=torch.long).random_(len(self.train_set))\n",
    "            noise_docs = []\n",
    "            noise_labels = torch.FloatTensor([])\n",
    "            for i in range(cur_batch_size):     \n",
    "                noise_docs.append(self.train_set[i][0])\n",
    "                noise_labels = torch.cat((noise_labels, self.train_set[i][2]))\n",
    "            fake_labels = torch.reshape(noise_labels, (cur_batch_size, len(self.vocab))).to(self.device)\n",
    "            \n",
    "            fake_embs = self.generator(noise_docs).to(self.device)\n",
    "\n",
    "            mixed_embs = torch.cat((real_embs_t, fake_embs), dim=0)\n",
    "            logits, probs = self.classifier(mixed_embs)\n",
    "            recons = self.discriminator(mixed_embs)         \n",
    "\n",
    "            recons_list = torch.split(recons, cur_batch_size)\n",
    "            D_real_recons = recons_list[0]\n",
    "            D_fake_recons = recons_list[1]\n",
    "        \n",
    "            logits_list = torch.split(logits, cur_batch_size)\n",
    "            D_real_logits = logits_list[0]\n",
    "            D_fake_logits = logits_list[1]\n",
    "            \n",
    "            probs_list = torch.split(probs, cur_batch_size)\n",
    "            D_real_probs = probs_list[0]\n",
    "            D_fake_probs = probs_list[1]\n",
    "\n",
    "            # Generator's LOSS\n",
    "            g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + self.eps))\n",
    "            g_feat_emb = torch.mean(torch.pow(torch.mean(real_embs_t, dim=0) - torch.mean(fake_embs, dim=0), 2))\n",
    "            gen_loss = g_loss_d + g_feat_emb\n",
    "            \n",
    "\n",
    "            self.gen_optimizer.zero_grad()\n",
    "            gen_loss.backward()\n",
    "            self.gen_optimizer.step()\n",
    "            if self.config['scheduler']:\n",
    "                self.gen_scheduler.step()\n",
    "\n",
    "            gen_train_loss += gen_loss.item()\n",
    "\n",
    "        avg_gen_train_loss = gen_train_loss / len(loader)           \n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss generetor: {0:.3f}\".format(avg_gen_train_loss))\n",
    "\n",
    "        return avg_gen_train_loss\n",
    "        \n",
    "    def dis_training(self, epoch, loader):      \n",
    "        cls_train_loss, dis_train_loss = 0, 0\n",
    "        \n",
    "        #self.vae.train()\n",
    "        self.generator.eval()\n",
    "        self.discriminator.train()\n",
    "        self.classifier.train()\n",
    "\n",
    "        for batch, (corpus, embs, labels, masks) in enumerate(loader):\n",
    "            real_embs, labels, masks = embs.to(self.device), labels.to(self.device), masks.to(self.device)\n",
    "            cur_batch_size = embs.shape[0]\n",
    "            \n",
    "            # vae transform\n",
    "            #real_embs_t = self.vae(real_embs)\n",
    "            real_embs_t = real_embs\n",
    "            \n",
    "            # fake label from BERT\n",
    "            noise = torch.empty(cur_batch_size, dtype=torch.long).random_(len(self.train_set))\n",
    "            noise_docs = []\n",
    "            noise_labels = torch.FloatTensor([])\n",
    "            for i in range(cur_batch_size):     \n",
    "                noise_docs.append(self.train_set[i][0])\n",
    "                noise_labels = torch.cat((noise_labels, self.train_set[i][2]))\n",
    "            fake_labels = torch.reshape(noise_labels, (cur_batch_size, len(self.vocab))).to(self.device)\n",
    "            \n",
    "            fake_embs = self.generator(noise_docs).to(self.device)\n",
    "\n",
    "            mixed_embs = torch.cat((real_embs_t, fake_embs), dim=0)\n",
    "            logits, probs = self.classifier(mixed_embs)\n",
    "            recons = self.discriminator(mixed_embs)\n",
    "\n",
    "            recons_list = torch.split(recons, cur_batch_size)\n",
    "            D_real_recons = recons_list[0]\n",
    "            D_fake_recons = recons_list[1]\n",
    "        \n",
    "            logits_list = torch.split(logits, cur_batch_size)\n",
    "            D_real_logits = logits_list[0]\n",
    "            D_fake_logits = logits_list[1]\n",
    "            \n",
    "            probs_list = torch.split(probs, cur_batch_size)\n",
    "            D_real_probs = probs_list[0]\n",
    "            D_fake_probs = probs_list[1]\n",
    "            \n",
    "            # Classifier's Loss\n",
    "            D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + self.eps))\n",
    "            D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + self.eps))\n",
    "            #D_L_unsupervised1U = self.cls_loss(D_real_logits, torch.ones(cur_batch_size, dtype=torch.long).to(self.device))\n",
    "            #D_L_unsupervised2U = self.cls_loss(D_fake_logits, torch.zeros(cur_batch_size, dtype=torch.long).to(self.device)) \n",
    "            cls_loss =  D_L_unsupervised1U + D_L_unsupervised2U\n",
    "            \n",
    "            # Disciminator's LOSS\n",
    "            recon_loss = torch.masked_select(Singular_MythNet(D_real_recons, labels), torch.flatten(masks))\n",
    "            g_recon_weight =  D_fake_probs[:, 0]\n",
    "            fake_recon_loss = Singular_MythNet(D_fake_recons, fake_labels) * g_recon_weight\n",
    "            labeled_count = recon_loss.type(torch.float32).numel()\n",
    "            \n",
    "            if labeled_count == 0:\n",
    "                D_L_Supervised = torch.mean(fake_recon_loss)\n",
    "            else:\n",
    "                D_L_Supervised = torch.mean(recon_loss) + torch.mean(fake_recon_loss)                    \n",
    "            dis_loss = D_L_Supervised + cls_loss\n",
    "            \n",
    "            self.cls_optimizer.zero_grad()\n",
    "            cls_loss.backward(retain_graph=True)\n",
    "            self.cls_optimizer.step()\n",
    "            if self.config['scheduler']:\n",
    "                self.cls_scheduler.step()\n",
    "                \n",
    "            self.dis_optimizer.zero_grad()\n",
    "            dis_loss.backward()\n",
    "            self.dis_optimizer.step()\n",
    "            if self.config['scheduler']:\n",
    "                self.dis_scheduler.step()\n",
    "            \n",
    "            cls_train_loss += cls_loss.item()\n",
    "            dis_train_loss += dis_loss.item()\n",
    "        \n",
    "        avg_cls_train_loss = cls_train_loss / len(loader)\n",
    "        avg_dis_train_loss = dis_train_loss / len(loader)           \n",
    "        \n",
    "        print(\"  Average training loss classifier: {0:.3f}\".format(avg_cls_train_loss))\n",
    "        print(\"  Average training loss discriminator: {0:.3f}\".format(avg_dis_train_loss))\n",
    "\n",
    "        return avg_cls_train_loss, avg_dis_train_loss\n",
    "        \n",
    "    def ae_validation(self, loader):\n",
    "        ae_val_loss = 0\n",
    "        ae_val_cos = 0\n",
    "        \n",
    "        self.vae.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch, (corpus, embs, labels, masks) in enumerate(loader):\n",
    "                embs, masks = embs.to(self.device), masks.to(self.device)\n",
    "                prior_mean, prior_variance, posterior_mean, posterior_variance,\\\n",
    "                posterior_log_variance, encoded, decoded = self.vae(embs, embs)\n",
    "\n",
    "                # Loss weight\n",
    "                cos = torch.nn.functional.cosine_similarity(torch.mean(embs, dim=0), torch.mean(decoded, dim=0), dim=0)\n",
    "                w = 1 - cos\n",
    "                \n",
    "                # Encode-Decode's Loss\n",
    "                recon_loss = torch.mean(self.mse_loss(decoded, embs), dim=1)    \n",
    "                decoded_loss = torch.mean(recon_loss) * w\n",
    "\n",
    "                ae_val_loss += decoded_loss.item()\n",
    "                ae_val_cos += cos\n",
    "                \n",
    "            avg_ae_val_loss = ae_val_loss / len(loader)\n",
    "            avg_ae_val_cos = ae_val_cos / len(loader)\n",
    "        \n",
    "        return avg_ae_val_loss, avg_ae_val_cos\n",
    "    \n",
    "    def mlp_validation(self, loader):\n",
    "        self.vae.eval()\n",
    "        self.decoder.eval()\n",
    "        \n",
    "        results = defaultdict(list)\n",
    "        with torch.no_grad():\n",
    "            for batch, (corpus, embs, labels, masks) in enumerate(loader):\n",
    "                embs, labels = embs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                # VAE transform\n",
    "                _, _, _, _, _, encoded, _ = self.vae(embs, embs)   \n",
    "\n",
    "                # Decode\n",
    "                recons = self.decoder(encoded)\n",
    "                \n",
    "                # Precision for reconstruct\n",
    "                precision_scores = retrieval_precision_all(recons, labels, k=self.config['topk'])\n",
    "                for k, v in precision_scores.items():\n",
    "                    results['[Recon] Precision v1@{}'.format(k)].append(v)\n",
    "                \n",
    "                precision_scores = retrieval_precision_all_v2(recons, labels, k=self.config['topk'])\n",
    "                for k, v in precision_scores.items():\n",
    "                    results['[Recon] Precision v2@{}'.format(k)].append(v)\n",
    "\n",
    "                # NDCG for reconstruct\n",
    "                ndcg_scores = retrieval_normalized_dcg_all(recons, labels, k=self.config['topk'])\n",
    "                for k, v in ndcg_scores.items():\n",
    "                    results['[Recon] ndcg@{}'.format(k)].append(v)\n",
    "\n",
    "        for k in results:\n",
    "            results[k] = np.mean(results[k])\n",
    "                \n",
    "        return results\n",
    "    \n",
    "    def gan_validation(self, loader):\n",
    "        self.vae.eval()\n",
    "        self.generator.eval()\n",
    "        self.classifier.eval()\n",
    "        self.discriminator.eval()\n",
    "        \n",
    "        results = defaultdict(list)\n",
    "        with torch.no_grad():\n",
    "            for batch, (corpus, embs, labels, masks) in enumerate(loader):\n",
    "                embs, labels = embs.to(self.device), labels.to(self.device)\n",
    "                #embs_t = self.vae(embs, embs)\n",
    "                embs_t = embs\n",
    "                \n",
    "                logits, probs = self.classifier(embs_t)\n",
    "                recons = self.discriminator(embs_t)\n",
    "                \n",
    "                # Precision for reconstruct\n",
    "                precision_scores = retrieval_precision_all(recons, labels, k=self.config['topk'])\n",
    "                for k, v in precision_scores.items():\n",
    "                    results['[Recon] Precision v1@{}'.format(k)].append(v)\n",
    "                \n",
    "                precision_scores = retrieval_precision_all_v2(recons, labels, k=self.config['topk'])\n",
    "                for k, v in precision_scores.items():\n",
    "                    results['[Recon] Precision v2@{}'.format(k)].append(v)\n",
    "\n",
    "                # NDCG for reconstruct\n",
    "                ndcg_scores = retrieval_normalized_dcg_all(recons, labels, k=self.config['topk'])\n",
    "                for k, v in ndcg_scores.items():\n",
    "                    results['[Recon] ndcg@{}'.format(k)].append(v)\n",
    "\n",
    "        for k in results:\n",
    "            results[k] = np.mean(results[k])\n",
    "                \n",
    "        return results\n",
    "    \n",
    "    def ae_fit(self):\n",
    "        self.vae.to(self.device)\n",
    "\n",
    "        train_loader = DataLoader(self.train_set, batch_size=self.config['batch_size'], shuffle=True, num_workers=self.num_data_loader_workers)\n",
    "        valid_loader = DataLoader(self.valid_set, batch_size=self.config['batch_size'], shuffle=False, num_workers=self.num_data_loader_workers)\n",
    "        ae_loss = 0\n",
    "\n",
    "        for epoch in range(self.config['ae_epochs']):\n",
    "            ae_loss, ae_cos = self.ae_training(epoch, train_loader)\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                val_loss, val_cos = self.ae_validation(valid_loader)\n",
    "                withscheduler = 'with_scheduler' if self.config['scheduler'] else '_without_scheduler'\n",
    "                withbalance = 'with_balance' if self.config['balance'] else '_without_balance'\n",
    "                record = open('./ae_'+self.config['experiment']+'_'+self.config['dataset']+str(int(self.config['ratio'] * 100))+'_'+self.config['encoder']+'_loss_'+self.config['loss']+'_lr'+str(self.config['lr'])+'_optim'+self.config['optim']+withscheduler+withbalance+'_weightdecay'+str(self.config['weight_decay'])+'.txt', 'a')\n",
    "                print('---------------------------------------')\n",
    "                record.write('-------------------------------------------------\\n')\n",
    "                print(\"AutoEncoder Validation loss: {0:.3f}\".format(val_loss))\n",
    "                record.write(\"AutoEncoder training loss: {0:.3f}\\n\".format(val_loss))\n",
    "                print(\"AutoEncoder validation Cosine-Similarity: {0:.3f}\".format(val_cos))\n",
    "                record.write(\"AutoEncoder validation Cosine-Similarity: {0:.3f}\\n\".format(val_cos))\n",
    "    \n",
    "    def mlp_fit(self):\n",
    "        self.decoder.to(self.device)\n",
    "\n",
    "        train_loader = DataLoader(self.train_set, batch_size=self.config['batch_size'], shuffle=True, num_workers=self.num_data_loader_workers)\n",
    "        valid_loader = DataLoader(self.valid_set, batch_size=self.config['batch_size'], shuffle=False, num_workers=self.num_data_loader_workers)\n",
    "\n",
    "        decoded_train_loss = 0\n",
    "\n",
    "        for epoch in range(self.config['epochs']):\n",
    "            decoded_train_loss = self.mlp_training(epoch, train_loader)\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                val_res = self.mlp_validation(valid_loader)\n",
    "                withscheduler = 'with_scheduler' if self.config['scheduler'] else '_without_scheduler'\n",
    "                withbalance = 'with_balance' if self.config['balance'] else '_without_balance'\n",
    "                record = open('./ide_semi_'+self.config['experiment']+'_'+self.config['dataset']+str(int(self.config['ratio'] * 100))+'_'+self.config['encoder']+'_'+self.config['target']+'_loss_'+self.config['loss']+'_lr'+str(self.config['lr'])+'_optim'+self.config['optim']+withscheduler+withbalance+'_weightdecay'+str(self.config['weight_decay'])+'.txt', 'a')\n",
    "                print('---------------------------------------')\n",
    "                record.write('-------------------------------------------------\\n')\n",
    "                for key,val in val_res.items():\n",
    "                    print(f\"{key}:{val:.4f}\")\n",
    "                    record.write(f\"{key}:{val:.4f}\\n\")\n",
    "                print(\"Decoder training loss: {0:.3f}\".format(decoded_train_loss))\n",
    "                record.write(\"Decoder training loss: {0:.3f}\\n\".format(decoded_train_loss))\n",
    "\n",
    "    def gan_fit(self):\n",
    "        self.generator.to(self.device)\n",
    "        self.classifier.to(self.device)\n",
    "        self.discriminator.to(self.device)\n",
    "\n",
    "        train_loader = DataLoader(self.train_set, batch_size=self.config['batch_size'], shuffle=True, num_workers=self.num_data_loader_workers)\n",
    "        valid_loader = DataLoader(self.valid_set, batch_size=self.config['batch_size'], shuffle=False, num_workers=self.num_data_loader_workers)\n",
    "\n",
    "        gen_train_loss, cls_train_loss, dis_train_loss = 0, 0, 0\n",
    "\n",
    "        for epoch in range(self.config['epochs']):\n",
    "            gen_train_loss = self.gen_training(epoch, train_loader)\n",
    "            cls_train_loss, dis_train_loss = self.dis_training(epoch, train_loader)\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                val_res = self.gan_validation(valid_loader)\n",
    "                withscheduler = 'with_scheduler' if self.config['scheduler'] else '_without_scheduler'\n",
    "                withbalance = 'with_balance' if self.config['balance'] else '_without_balance'\n",
    "                record = open('./ide_gan_'+self.config['experiment']+'_'+self.config['dataset']+str(int(self.config['ratio'] * 100))+'_'+self.config['encoder']+'_'+self.config['target']+'_loss_'+self.config['loss']+'_lr'+str(self.config['lr'])+'_optim'+self.config['optim']+withscheduler+withbalance+'_weightdecay'+str(self.config['weight_decay'])+'.txt', 'a')\n",
    "                print('---------------------------------------')\n",
    "                record.write('-------------------------------------------------\\n')\n",
    "                for key,val in val_res.items():\n",
    "                    print(f\"{key}:{val:.4f}\")\n",
    "                    record.write(f\"{key}:{val:.4f}\\n\")\n",
    "                print(\"Generator training loss: {0:.3f}\".format(gen_train_loss))\n",
    "                record.write(\"Generator training loss: {0:.3f}\\n\".format(gen_train_loss))\n",
    "                print(\"Classifier training loss: {0:.3f}\".format(cls_train_loss))\n",
    "                record.write(\"Classifier training loss: {0:.3f}\\n\".format(cls_train_loss))\n",
    "                print(\"Discriminator training loss: {0:.3f}\".format(dis_train_loss))\n",
    "                record.write(\"Discriminator training loss: {0:.3f}\\n\".format(dis_train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcf106a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting preprocess documents: 20news\n",
      "min_df: 62 max_df: 1.0 vocabulary_size: None min_doc_word: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/casimir0304/miniconda3/envs/ide/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "training_set, validation_set, vocabularys, id2token, device = generate_dataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67989cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = IDEAEDecoder(config, training_set, validation_set, vocabularys, id2token, device)\n",
    "#model.ae_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a69996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.061\n",
      "  Average training loss classifier: 1.749\n",
      "  Average training loss discriminator: 13.628\n",
      "\n",
      "======== Epoch 2 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.095\n",
      "  Average training loss classifier: 1.278\n",
      "  Average training loss discriminator: 11.809\n",
      "\n",
      "======== Epoch 3 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.116\n",
      "  Average training loss classifier: 1.306\n",
      "  Average training loss discriminator: 11.340\n",
      "\n",
      "======== Epoch 4 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.116\n",
      "  Average training loss classifier: 1.066\n",
      "  Average training loss discriminator: 10.708\n",
      "\n",
      "======== Epoch 5 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.104\n",
      "  Average training loss classifier: 1.007\n",
      "  Average training loss discriminator: 10.264\n",
      "\n",
      "======== Epoch 6 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.097\n",
      "  Average training loss classifier: 0.938\n",
      "  Average training loss discriminator: 9.821\n",
      "\n",
      "======== Epoch 7 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.085\n",
      "  Average training loss classifier: 0.938\n",
      "  Average training loss discriminator: 9.876\n",
      "\n",
      "======== Epoch 8 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.087\n",
      "  Average training loss classifier: 0.931\n",
      "  Average training loss discriminator: 9.921\n",
      "\n",
      "======== Epoch 9 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.077\n",
      "  Average training loss classifier: 0.967\n",
      "  Average training loss discriminator: 10.174\n",
      "\n",
      "======== Epoch 10 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.078\n",
      "  Average training loss classifier: 0.970\n",
      "  Average training loss discriminator: 10.263\n",
      "---------------------------------------\n",
      "[Recon] Precision v1@5:0.3502\n",
      "[Recon] Precision v1@10:0.2846\n",
      "[Recon] Precision v1@15:0.2519\n",
      "[Recon] Precision v1@20:0.2304\n",
      "[Recon] Precision v1@25:0.2142\n",
      "[Recon] Precision v1@30:0.2024\n",
      "[Recon] Precision v1@35:0.1925\n",
      "[Recon] Precision v1@40:0.1843\n",
      "[Recon] Precision v1@45:0.1772\n",
      "[Recon] Precision v1@50:0.1712\n",
      "[Recon] Precision v2@5:0.0947\n",
      "[Recon] Precision v2@10:0.0977\n",
      "[Recon] Precision v2@15:0.1037\n",
      "[Recon] Precision v2@20:0.1134\n",
      "[Recon] Precision v2@25:0.1219\n",
      "[Recon] Precision v2@30:0.1295\n",
      "[Recon] Precision v2@35:0.1341\n",
      "[Recon] Precision v2@40:0.1373\n",
      "[Recon] Precision v2@45:0.1387\n",
      "[Recon] Precision v2@50:0.1392\n",
      "[Recon] ndcg@5:0.1973\n",
      "[Recon] ndcg@10:0.1884\n",
      "[Recon] ndcg@15:0.1846\n",
      "[Recon] ndcg@20:0.1836\n",
      "[Recon] ndcg@25:0.1836\n",
      "[Recon] ndcg@30:0.1851\n",
      "[Recon] ndcg@35:0.1871\n",
      "[Recon] ndcg@40:0.1896\n",
      "[Recon] ndcg@45:0.1924\n",
      "[Recon] ndcg@50:0.1955\n",
      "[Recon] ndcg@all:0.4474\n",
      "Generator training loss: 0.078\n",
      "Classifier training loss: 0.970\n",
      "Discriminator training loss: 10.263\n",
      "\n",
      "======== Epoch 11 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.066\n",
      "  Average training loss classifier: 0.988\n",
      "  Average training loss discriminator: 10.348\n",
      "\n",
      "======== Epoch 12 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.069\n",
      "  Average training loss classifier: 0.936\n",
      "  Average training loss discriminator: 10.170\n",
      "\n",
      "======== Epoch 13 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.066\n",
      "  Average training loss classifier: 0.960\n",
      "  Average training loss discriminator: 10.208\n",
      "\n",
      "======== Epoch 14 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.079\n",
      "  Average training loss classifier: 0.945\n",
      "  Average training loss discriminator: 10.168\n",
      "\n",
      "======== Epoch 15 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.075\n",
      "  Average training loss classifier: 0.998\n",
      "  Average training loss discriminator: 10.273\n",
      "\n",
      "======== Epoch 16 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.064\n",
      "  Average training loss classifier: 0.983\n",
      "  Average training loss discriminator: 10.167\n",
      "\n",
      "======== Epoch 17 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.059\n",
      "  Average training loss classifier: 1.013\n",
      "  Average training loss discriminator: 10.347\n",
      "\n",
      "======== Epoch 18 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.062\n",
      "  Average training loss classifier: 0.980\n",
      "  Average training loss discriminator: 10.279\n",
      "\n",
      "======== Epoch 19 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.055\n",
      "  Average training loss classifier: 0.989\n",
      "  Average training loss discriminator: 10.210\n",
      "\n",
      "======== Epoch 20 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.054\n",
      "  Average training loss classifier: 0.963\n",
      "  Average training loss discriminator: 10.277\n",
      "---------------------------------------\n",
      "[Recon] Precision v1@5:0.4693\n",
      "[Recon] Precision v1@10:0.3901\n",
      "[Recon] Precision v1@15:0.3426\n",
      "[Recon] Precision v1@20:0.3100\n",
      "[Recon] Precision v1@25:0.2857\n",
      "[Recon] Precision v1@30:0.2665\n",
      "[Recon] Precision v1@35:0.2507\n",
      "[Recon] Precision v1@40:0.2371\n",
      "[Recon] Precision v1@45:0.2257\n",
      "[Recon] Precision v1@50:0.2159\n",
      "[Recon] Precision v2@5:0.1074\n",
      "[Recon] Precision v2@10:0.1127\n",
      "[Recon] Precision v2@15:0.1220\n",
      "[Recon] Precision v2@20:0.1344\n",
      "[Recon] Precision v2@25:0.1472\n",
      "[Recon] Precision v2@30:0.1575\n",
      "[Recon] Precision v2@35:0.1643\n",
      "[Recon] Precision v2@40:0.1690\n",
      "[Recon] Precision v2@45:0.1709\n",
      "[Recon] Precision v2@50:0.1712\n",
      "[Recon] ndcg@5:0.2294\n",
      "[Recon] ndcg@10:0.2234\n",
      "[Recon] ndcg@15:0.2204\n",
      "[Recon] ndcg@20:0.2193\n",
      "[Recon] ndcg@25:0.2199\n",
      "[Recon] ndcg@30:0.2216\n",
      "[Recon] ndcg@35:0.2240\n",
      "[Recon] ndcg@40:0.2266\n",
      "[Recon] ndcg@45:0.2298\n",
      "[Recon] ndcg@50:0.2332\n",
      "[Recon] ndcg@all:0.4740\n",
      "Generator training loss: 0.054\n",
      "Classifier training loss: 0.963\n",
      "Discriminator training loss: 10.277\n",
      "\n",
      "======== Epoch 21 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.050\n",
      "  Average training loss classifier: 1.002\n",
      "  Average training loss discriminator: 10.255\n",
      "\n",
      "======== Epoch 22 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.054\n",
      "  Average training loss classifier: 0.990\n",
      "  Average training loss discriminator: 10.198\n",
      "\n",
      "======== Epoch 23 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.053\n",
      "  Average training loss classifier: 1.032\n",
      "  Average training loss discriminator: 10.332\n",
      "\n",
      "======== Epoch 24 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.076\n",
      "  Average training loss classifier: 1.008\n",
      "  Average training loss discriminator: 10.229\n",
      "\n",
      "======== Epoch 25 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.050\n",
      "  Average training loss classifier: 1.054\n",
      "  Average training loss discriminator: 10.462\n",
      "\n",
      "======== Epoch 26 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.047\n",
      "  Average training loss classifier: 1.034\n",
      "  Average training loss discriminator: 10.456\n",
      "\n",
      "======== Epoch 27 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.050\n",
      "  Average training loss classifier: 1.050\n",
      "  Average training loss discriminator: 10.245\n",
      "\n",
      "======== Epoch 28 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.055\n",
      "  Average training loss classifier: 1.000\n",
      "  Average training loss discriminator: 10.098\n",
      "\n",
      "======== Epoch 29 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.051\n",
      "  Average training loss classifier: 1.014\n",
      "  Average training loss discriminator: 10.077\n",
      "\n",
      "======== Epoch 30 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.049\n",
      "  Average training loss classifier: 0.994\n",
      "  Average training loss discriminator: 10.179\n",
      "---------------------------------------\n",
      "[Recon] Precision v1@5:0.5342\n",
      "[Recon] Precision v1@10:0.4440\n",
      "[Recon] Precision v1@15:0.3890\n",
      "[Recon] Precision v1@20:0.3518\n",
      "[Recon] Precision v1@25:0.3243\n",
      "[Recon] Precision v1@30:0.3009\n",
      "[Recon] Precision v1@35:0.2821\n",
      "[Recon] Precision v1@40:0.2658\n",
      "[Recon] Precision v1@45:0.2522\n",
      "[Recon] Precision v1@50:0.2403\n",
      "[Recon] Precision v2@5:0.1007\n",
      "[Recon] Precision v2@10:0.1124\n",
      "[Recon] Precision v2@15:0.1236\n",
      "[Recon] Precision v2@20:0.1396\n",
      "[Recon] Precision v2@25:0.1553\n",
      "[Recon] Precision v2@30:0.1672\n",
      "[Recon] Precision v2@35:0.1759\n",
      "[Recon] Precision v2@40:0.1814\n",
      "[Recon] Precision v2@45:0.1838\n",
      "[Recon] Precision v2@50:0.1847\n",
      "[Recon] ndcg@5:0.2283\n",
      "[Recon] ndcg@10:0.2268\n",
      "[Recon] ndcg@15:0.2256\n",
      "[Recon] ndcg@20:0.2259\n",
      "[Recon] ndcg@25:0.2279\n",
      "[Recon] ndcg@30:0.2300\n",
      "[Recon] ndcg@35:0.2330\n",
      "[Recon] ndcg@40:0.2359\n",
      "[Recon] ndcg@45:0.2393\n",
      "[Recon] ndcg@50:0.2431\n",
      "[Recon] ndcg@all:0.4807\n",
      "Generator training loss: 0.049\n",
      "Classifier training loss: 0.994\n",
      "Discriminator training loss: 10.179\n",
      "\n",
      "======== Epoch 31 / 300 ========\n",
      "Bert GAN Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss generetor: 0.051\n",
      "  Average training loss classifier: 1.043\n",
      "  Average training loss discriminator: 10.250\n",
      "\n",
      "======== Epoch 32 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.048\n",
      "  Average training loss classifier: 1.020\n",
      "  Average training loss discriminator: 10.090\n",
      "\n",
      "======== Epoch 33 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.062\n",
      "  Average training loss classifier: 1.055\n",
      "  Average training loss discriminator: 10.276\n",
      "\n",
      "======== Epoch 34 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.047\n",
      "  Average training loss classifier: 1.009\n",
      "  Average training loss discriminator: 9.938\n",
      "\n",
      "======== Epoch 35 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.071\n",
      "  Average training loss classifier: 1.013\n",
      "  Average training loss discriminator: 9.944\n",
      "\n",
      "======== Epoch 36 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.051\n",
      "  Average training loss classifier: 1.031\n",
      "  Average training loss discriminator: 10.207\n",
      "\n",
      "======== Epoch 37 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.050\n",
      "  Average training loss classifier: 1.073\n",
      "  Average training loss discriminator: 10.101\n",
      "\n",
      "======== Epoch 38 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.063\n",
      "  Average training loss classifier: 1.027\n",
      "  Average training loss discriminator: 9.979\n",
      "\n",
      "======== Epoch 39 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.068\n",
      "  Average training loss classifier: 1.052\n",
      "  Average training loss discriminator: 9.948\n",
      "\n",
      "======== Epoch 40 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.077\n",
      "  Average training loss classifier: 1.025\n",
      "  Average training loss discriminator: 9.953\n",
      "---------------------------------------\n",
      "[Recon] Precision v1@5:0.5543\n",
      "[Recon] Precision v1@10:0.4557\n",
      "[Recon] Precision v1@15:0.3984\n",
      "[Recon] Precision v1@20:0.3578\n",
      "[Recon] Precision v1@25:0.3288\n",
      "[Recon] Precision v1@30:0.3048\n",
      "[Recon] Precision v1@35:0.2853\n",
      "[Recon] Precision v1@40:0.2693\n",
      "[Recon] Precision v1@45:0.2553\n",
      "[Recon] Precision v1@50:0.2428\n",
      "[Recon] Precision v2@5:0.1011\n",
      "[Recon] Precision v2@10:0.1134\n",
      "[Recon] Precision v2@15:0.1252\n",
      "[Recon] Precision v2@20:0.1411\n",
      "[Recon] Precision v2@25:0.1568\n",
      "[Recon] Precision v2@30:0.1683\n",
      "[Recon] Precision v2@35:0.1764\n",
      "[Recon] Precision v2@40:0.1825\n",
      "[Recon] Precision v2@45:0.1853\n",
      "[Recon] Precision v2@50:0.1859\n",
      "[Recon] ndcg@5:0.2293\n",
      "[Recon] ndcg@10:0.2281\n",
      "[Recon] ndcg@15:0.2273\n",
      "[Recon] ndcg@20:0.2274\n",
      "[Recon] ndcg@25:0.2295\n",
      "[Recon] ndcg@30:0.2317\n",
      "[Recon] ndcg@35:0.2345\n",
      "[Recon] ndcg@40:0.2382\n",
      "[Recon] ndcg@45:0.2418\n",
      "[Recon] ndcg@50:0.2454\n",
      "[Recon] ndcg@all:0.4815\n",
      "Generator training loss: 0.077\n",
      "Classifier training loss: 1.025\n",
      "Discriminator training loss: 9.953\n",
      "\n",
      "======== Epoch 41 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.077\n",
      "  Average training loss classifier: 1.059\n",
      "  Average training loss discriminator: 10.098\n",
      "\n",
      "======== Epoch 42 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.083\n",
      "  Average training loss classifier: 1.040\n",
      "  Average training loss discriminator: 9.887\n",
      "\n",
      "======== Epoch 43 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.086\n",
      "  Average training loss classifier: 1.059\n",
      "  Average training loss discriminator: 9.938\n",
      "\n",
      "======== Epoch 44 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.091\n",
      "  Average training loss classifier: 1.038\n",
      "  Average training loss discriminator: 9.897\n",
      "\n",
      "======== Epoch 45 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.091\n",
      "  Average training loss classifier: 1.079\n",
      "  Average training loss discriminator: 9.991\n",
      "\n",
      "======== Epoch 46 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.095\n",
      "  Average training loss classifier: 1.043\n",
      "  Average training loss discriminator: 9.893\n",
      "\n",
      "======== Epoch 47 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.098\n",
      "  Average training loss classifier: 1.070\n",
      "  Average training loss discriminator: 9.975\n",
      "\n",
      "======== Epoch 48 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.097\n",
      "  Average training loss classifier: 1.074\n",
      "  Average training loss discriminator: 9.945\n",
      "\n",
      "======== Epoch 49 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.103\n",
      "  Average training loss classifier: 1.081\n",
      "  Average training loss discriminator: 9.948\n",
      "\n",
      "======== Epoch 50 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.108\n",
      "  Average training loss classifier: 1.075\n",
      "  Average training loss discriminator: 9.920\n",
      "---------------------------------------\n",
      "[Recon] Precision v1@5:0.5672\n",
      "[Recon] Precision v1@10:0.4637\n",
      "[Recon] Precision v1@15:0.4064\n",
      "[Recon] Precision v1@20:0.3667\n",
      "[Recon] Precision v1@25:0.3362\n",
      "[Recon] Precision v1@30:0.3114\n",
      "[Recon] Precision v1@35:0.2911\n",
      "[Recon] Precision v1@40:0.2745\n",
      "[Recon] Precision v1@45:0.2600\n",
      "[Recon] Precision v1@50:0.2475\n",
      "[Recon] Precision v2@5:0.1184\n",
      "[Recon] Precision v2@10:0.1286\n",
      "[Recon] Precision v2@15:0.1405\n",
      "[Recon] Precision v2@20:0.1559\n",
      "[Recon] Precision v2@25:0.1692\n",
      "[Recon] Precision v2@30:0.1793\n",
      "[Recon] Precision v2@35:0.1869\n",
      "[Recon] Precision v2@40:0.1917\n",
      "[Recon] Precision v2@45:0.1933\n",
      "[Recon] Precision v2@50:0.1930\n",
      "[Recon] ndcg@5:0.2529\n",
      "[Recon] ndcg@10:0.2498\n",
      "[Recon] ndcg@15:0.2487\n",
      "[Recon] ndcg@20:0.2491\n",
      "[Recon] ndcg@25:0.2502\n",
      "[Recon] ndcg@30:0.2520\n",
      "[Recon] ndcg@35:0.2547\n",
      "[Recon] ndcg@40:0.2582\n",
      "[Recon] ndcg@45:0.2616\n",
      "[Recon] ndcg@50:0.2649\n",
      "[Recon] ndcg@all:0.4949\n",
      "Generator training loss: 0.108\n",
      "Classifier training loss: 1.075\n",
      "Discriminator training loss: 9.920\n",
      "\n",
      "======== Epoch 51 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.105\n",
      "  Average training loss classifier: 1.127\n",
      "  Average training loss discriminator: 10.178\n",
      "\n",
      "======== Epoch 52 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.103\n",
      "  Average training loss classifier: 1.094\n",
      "  Average training loss discriminator: 10.001\n",
      "\n",
      "======== Epoch 53 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.101\n",
      "  Average training loss classifier: 1.122\n",
      "  Average training loss discriminator: 9.949\n",
      "\n",
      "======== Epoch 54 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.112\n",
      "  Average training loss classifier: 1.138\n",
      "  Average training loss discriminator: 10.074\n",
      "\n",
      "======== Epoch 55 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.100\n",
      "  Average training loss classifier: 1.078\n",
      "  Average training loss discriminator: 9.748\n",
      "\n",
      "======== Epoch 56 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.112\n",
      "  Average training loss classifier: 1.100\n",
      "  Average training loss discriminator: 9.866\n",
      "\n",
      "======== Epoch 57 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.110\n",
      "  Average training loss classifier: 1.193\n",
      "  Average training loss discriminator: 10.159\n",
      "\n",
      "======== Epoch 58 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.091\n",
      "  Average training loss classifier: 1.038\n",
      "  Average training loss discriminator: 9.666\n",
      "\n",
      "======== Epoch 59 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.123\n",
      "  Average training loss classifier: 1.115\n",
      "  Average training loss discriminator: 9.855\n",
      "\n",
      "======== Epoch 60 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.127\n",
      "  Average training loss classifier: 1.191\n",
      "  Average training loss discriminator: 10.116\n",
      "---------------------------------------\n",
      "[Recon] Precision v1@5:0.5738\n",
      "[Recon] Precision v1@10:0.4791\n",
      "[Recon] Precision v1@15:0.4205\n",
      "[Recon] Precision v1@20:0.3792\n",
      "[Recon] Precision v1@25:0.3479\n",
      "[Recon] Precision v1@30:0.3230\n",
      "[Recon] Precision v1@35:0.3025\n",
      "[Recon] Precision v1@40:0.2852\n",
      "[Recon] Precision v1@45:0.2699\n",
      "[Recon] Precision v1@50:0.2571\n",
      "[Recon] Precision v2@5:0.1226\n",
      "[Recon] Precision v2@10:0.1301\n",
      "[Recon] Precision v2@15:0.1406\n",
      "[Recon] Precision v2@20:0.1541\n",
      "[Recon] Precision v2@25:0.1675\n",
      "[Recon] Precision v2@30:0.1791\n",
      "[Recon] Precision v2@35:0.1869\n",
      "[Recon] Precision v2@40:0.1925\n",
      "[Recon] Precision v2@45:0.1948\n",
      "[Recon] Precision v2@50:0.1955\n",
      "[Recon] ndcg@5:0.2610\n",
      "[Recon] ndcg@10:0.2570\n",
      "[Recon] ndcg@15:0.2548\n",
      "[Recon] ndcg@20:0.2540\n",
      "[Recon] ndcg@25:0.2546\n",
      "[Recon] ndcg@30:0.2566\n",
      "[Recon] ndcg@35:0.2593\n",
      "[Recon] ndcg@40:0.2625\n",
      "[Recon] ndcg@45:0.2656\n",
      "[Recon] ndcg@50:0.2690\n",
      "[Recon] ndcg@all:0.4998\n",
      "Generator training loss: 0.127\n",
      "Classifier training loss: 1.191\n",
      "Discriminator training loss: 10.116\n",
      "\n",
      "======== Epoch 61 / 300 ========\n",
      "Bert GAN Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss generetor: 0.145\n",
      "  Average training loss classifier: 1.034\n",
      "  Average training loss discriminator: 9.598\n",
      "\n",
      "======== Epoch 62 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.119\n",
      "  Average training loss classifier: 1.221\n",
      "  Average training loss discriminator: 10.066\n",
      "\n",
      "======== Epoch 63 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.106\n",
      "  Average training loss classifier: 1.190\n",
      "  Average training loss discriminator: 10.028\n",
      "\n",
      "======== Epoch 64 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.082\n",
      "  Average training loss classifier: 1.113\n",
      "  Average training loss discriminator: 9.644\n",
      "\n",
      "======== Epoch 65 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.128\n",
      "  Average training loss classifier: 1.133\n",
      "  Average training loss discriminator: 9.784\n",
      "\n",
      "======== Epoch 66 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.092\n",
      "  Average training loss classifier: 1.147\n",
      "  Average training loss discriminator: 9.726\n",
      "\n",
      "======== Epoch 67 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.111\n",
      "  Average training loss classifier: 1.087\n",
      "  Average training loss discriminator: 9.630\n",
      "\n",
      "======== Epoch 68 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.114\n",
      "  Average training loss classifier: 1.215\n",
      "  Average training loss discriminator: 10.028\n",
      "\n",
      "======== Epoch 69 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.052\n",
      "  Average training loss classifier: 1.089\n",
      "  Average training loss discriminator: 9.520\n",
      "\n",
      "======== Epoch 70 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.082\n",
      "  Average training loss classifier: 1.108\n",
      "  Average training loss discriminator: 9.493\n",
      "---------------------------------------\n",
      "[Recon] Precision v1@5:0.6144\n",
      "[Recon] Precision v1@10:0.5137\n",
      "[Recon] Precision v1@15:0.4446\n",
      "[Recon] Precision v1@20:0.3972\n",
      "[Recon] Precision v1@25:0.3625\n",
      "[Recon] Precision v1@30:0.3346\n",
      "[Recon] Precision v1@35:0.3127\n",
      "[Recon] Precision v1@40:0.2938\n",
      "[Recon] Precision v1@45:0.2777\n",
      "[Recon] Precision v1@50:0.2636\n",
      "[Recon] Precision v2@5:0.1309\n",
      "[Recon] Precision v2@10:0.1384\n",
      "[Recon] Precision v2@15:0.1471\n",
      "[Recon] Precision v2@20:0.1603\n",
      "[Recon] Precision v2@25:0.1738\n",
      "[Recon] Precision v2@30:0.1847\n",
      "[Recon] Precision v2@35:0.1933\n",
      "[Recon] Precision v2@40:0.1985\n",
      "[Recon] Precision v2@45:0.2003\n",
      "[Recon] Precision v2@50:0.2006\n",
      "[Recon] ndcg@5:0.2730\n",
      "[Recon] ndcg@10:0.2692\n",
      "[Recon] ndcg@15:0.2651\n",
      "[Recon] ndcg@20:0.2636\n",
      "[Recon] ndcg@25:0.2643\n",
      "[Recon] ndcg@30:0.2657\n",
      "[Recon] ndcg@35:0.2685\n",
      "[Recon] ndcg@40:0.2716\n",
      "[Recon] ndcg@45:0.2750\n",
      "[Recon] ndcg@50:0.2784\n",
      "[Recon] ndcg@all:0.5054\n",
      "Generator training loss: 0.082\n",
      "Classifier training loss: 1.108\n",
      "Discriminator training loss: 9.493\n",
      "\n",
      "======== Epoch 71 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.095\n",
      "  Average training loss classifier: 1.112\n",
      "  Average training loss discriminator: 9.625\n",
      "\n",
      "======== Epoch 72 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.133\n",
      "  Average training loss classifier: 1.154\n",
      "  Average training loss discriminator: 9.661\n",
      "\n",
      "======== Epoch 73 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.063\n",
      "  Average training loss classifier: 1.073\n",
      "  Average training loss discriminator: 9.363\n",
      "\n",
      "======== Epoch 74 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.130\n",
      "  Average training loss classifier: 1.097\n",
      "  Average training loss discriminator: 9.314\n",
      "\n",
      "======== Epoch 75 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.152\n",
      "  Average training loss classifier: 1.113\n",
      "  Average training loss discriminator: 9.566\n",
      "\n",
      "======== Epoch 76 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.139\n",
      "  Average training loss classifier: 1.195\n",
      "  Average training loss discriminator: 9.728\n",
      "\n",
      "======== Epoch 77 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.147\n",
      "  Average training loss classifier: 1.066\n",
      "  Average training loss discriminator: 9.397\n",
      "\n",
      "======== Epoch 78 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.158\n",
      "  Average training loss classifier: 1.178\n",
      "  Average training loss discriminator: 9.780\n",
      "\n",
      "======== Epoch 79 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.177\n",
      "  Average training loss classifier: 1.179\n",
      "  Average training loss discriminator: 9.700\n",
      "\n",
      "======== Epoch 80 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.162\n",
      "  Average training loss classifier: 1.167\n",
      "  Average training loss discriminator: 9.627\n",
      "---------------------------------------\n",
      "[Recon] Precision v1@5:0.5607\n",
      "[Recon] Precision v1@10:0.4676\n",
      "[Recon] Precision v1@15:0.4115\n",
      "[Recon] Precision v1@20:0.3703\n",
      "[Recon] Precision v1@25:0.3397\n",
      "[Recon] Precision v1@30:0.3159\n",
      "[Recon] Precision v1@35:0.2962\n",
      "[Recon] Precision v1@40:0.2798\n",
      "[Recon] Precision v1@45:0.2656\n",
      "[Recon] Precision v1@50:0.2528\n",
      "[Recon] Precision v2@5:0.1539\n",
      "[Recon] Precision v2@10:0.1573\n",
      "[Recon] Precision v2@15:0.1637\n",
      "[Recon] Precision v2@20:0.1738\n",
      "[Recon] Precision v2@25:0.1842\n",
      "[Recon] Precision v2@30:0.1917\n",
      "[Recon] Precision v2@35:0.1973\n",
      "[Recon] Precision v2@40:0.2002\n",
      "[Recon] Precision v2@45:0.2011\n",
      "[Recon] Precision v2@50:0.2001\n",
      "[Recon] ndcg@5:0.3086\n",
      "[Recon] ndcg@10:0.2977\n",
      "[Recon] ndcg@15:0.2916\n",
      "[Recon] ndcg@20:0.2879\n",
      "[Recon] ndcg@25:0.2867\n",
      "[Recon] ndcg@30:0.2872\n",
      "[Recon] ndcg@35:0.2889\n",
      "[Recon] ndcg@40:0.2912\n",
      "[Recon] ndcg@45:0.2941\n",
      "[Recon] ndcg@50:0.2973\n",
      "[Recon] ndcg@all:0.5210\n",
      "Generator training loss: 0.162\n",
      "Classifier training loss: 1.167\n",
      "Discriminator training loss: 9.627\n",
      "\n",
      "======== Epoch 81 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.145\n",
      "  Average training loss classifier: 1.234\n",
      "  Average training loss discriminator: 9.673\n",
      "\n",
      "======== Epoch 82 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.117\n",
      "  Average training loss classifier: 1.198\n",
      "  Average training loss discriminator: 9.599\n",
      "\n",
      "======== Epoch 83 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.070\n",
      "  Average training loss classifier: 1.139\n",
      "  Average training loss discriminator: 9.454\n",
      "\n",
      "======== Epoch 84 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.115\n",
      "  Average training loss classifier: 1.152\n",
      "  Average training loss discriminator: 9.375\n",
      "\n",
      "======== Epoch 85 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.152\n",
      "  Average training loss classifier: 1.037\n",
      "  Average training loss discriminator: 8.888\n",
      "\n",
      "======== Epoch 86 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.114\n",
      "  Average training loss classifier: 1.262\n",
      "  Average training loss discriminator: 9.740\n",
      "\n",
      "======== Epoch 87 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.088\n",
      "  Average training loss classifier: 1.129\n",
      "  Average training loss discriminator: 9.281\n",
      "\n",
      "======== Epoch 88 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.114\n",
      "  Average training loss classifier: 1.122\n",
      "  Average training loss discriminator: 9.300\n",
      "\n",
      "======== Epoch 89 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.167\n",
      "  Average training loss classifier: 1.098\n",
      "  Average training loss discriminator: 9.131\n",
      "\n",
      "======== Epoch 90 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.189\n",
      "  Average training loss classifier: 1.150\n",
      "  Average training loss discriminator: 9.375\n",
      "---------------------------------------\n",
      "[Recon] Precision v1@5:0.5766\n",
      "[Recon] Precision v1@10:0.4813\n",
      "[Recon] Precision v1@15:0.4210\n",
      "[Recon] Precision v1@20:0.3786\n",
      "[Recon] Precision v1@25:0.3475\n",
      "[Recon] Precision v1@30:0.3224\n",
      "[Recon] Precision v1@35:0.3020\n",
      "[Recon] Precision v1@40:0.2848\n",
      "[Recon] Precision v1@45:0.2701\n",
      "[Recon] Precision v1@50:0.2569\n",
      "[Recon] Precision v2@5:0.1668\n",
      "[Recon] Precision v2@10:0.1657\n",
      "[Recon] Precision v2@15:0.1717\n",
      "[Recon] Precision v2@20:0.1804\n",
      "[Recon] Precision v2@25:0.1906\n",
      "[Recon] Precision v2@30:0.1977\n",
      "[Recon] Precision v2@35:0.2030\n",
      "[Recon] Precision v2@40:0.2055\n",
      "[Recon] Precision v2@45:0.2059\n",
      "[Recon] Precision v2@50:0.2044\n",
      "[Recon] ndcg@5:0.3300\n",
      "[Recon] ndcg@10:0.3159\n",
      "[Recon] ndcg@15:0.3077\n",
      "[Recon] ndcg@20:0.3032\n",
      "[Recon] ndcg@25:0.3017\n",
      "[Recon] ndcg@30:0.3015\n",
      "[Recon] ndcg@35:0.3027\n",
      "[Recon] ndcg@40:0.3050\n",
      "[Recon] ndcg@45:0.3077\n",
      "[Recon] ndcg@50:0.3106\n",
      "[Recon] ndcg@all:0.5316\n",
      "Generator training loss: 0.189\n",
      "Classifier training loss: 1.150\n",
      "Discriminator training loss: 9.375\n",
      "\n",
      "======== Epoch 91 / 300 ========\n",
      "Bert GAN Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss generetor: 0.183\n",
      "  Average training loss classifier: 1.163\n",
      "  Average training loss discriminator: 9.393\n",
      "\n",
      "======== Epoch 92 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.129\n",
      "  Average training loss classifier: 1.225\n",
      "  Average training loss discriminator: 9.628\n",
      "\n",
      "======== Epoch 93 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.136\n",
      "  Average training loss classifier: 1.180\n",
      "  Average training loss discriminator: 9.437\n",
      "\n",
      "======== Epoch 94 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.121\n",
      "  Average training loss classifier: 1.200\n",
      "  Average training loss discriminator: 9.479\n",
      "\n",
      "======== Epoch 95 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.167\n",
      "  Average training loss classifier: 1.137\n",
      "  Average training loss discriminator: 9.273\n",
      "\n",
      "======== Epoch 96 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.209\n",
      "  Average training loss classifier: 1.079\n",
      "  Average training loss discriminator: 9.014\n",
      "\n",
      "======== Epoch 97 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.162\n",
      "  Average training loss classifier: 1.265\n",
      "  Average training loss discriminator: 9.584\n",
      "\n",
      "======== Epoch 98 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.133\n",
      "  Average training loss classifier: 1.198\n",
      "  Average training loss discriminator: 9.487\n",
      "\n",
      "======== Epoch 99 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.142\n",
      "  Average training loss classifier: 1.135\n",
      "  Average training loss discriminator: 8.976\n",
      "\n",
      "======== Epoch 100 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.195\n",
      "  Average training loss classifier: 1.108\n",
      "  Average training loss discriminator: 9.090\n",
      "---------------------------------------\n",
      "[Recon] Precision v1@5:0.5959\n",
      "[Recon] Precision v1@10:0.4995\n",
      "[Recon] Precision v1@15:0.4393\n",
      "[Recon] Precision v1@20:0.3968\n",
      "[Recon] Precision v1@25:0.3639\n",
      "[Recon] Precision v1@30:0.3373\n",
      "[Recon] Precision v1@35:0.3153\n",
      "[Recon] Precision v1@40:0.2973\n",
      "[Recon] Precision v1@45:0.2815\n",
      "[Recon] Precision v1@50:0.2675\n",
      "[Recon] Precision v2@5:0.1654\n",
      "[Recon] Precision v2@10:0.1651\n",
      "[Recon] Precision v2@15:0.1703\n",
      "[Recon] Precision v2@20:0.1792\n",
      "[Recon] Precision v2@25:0.1898\n",
      "[Recon] Precision v2@30:0.1986\n",
      "[Recon] Precision v2@35:0.2050\n",
      "[Recon] Precision v2@40:0.2086\n",
      "[Recon] Precision v2@45:0.2095\n",
      "[Recon] Precision v2@50:0.2088\n",
      "[Recon] ndcg@5:0.3298\n",
      "[Recon] ndcg@10:0.3163\n",
      "[Recon] ndcg@15:0.3082\n",
      "[Recon] ndcg@20:0.3035\n",
      "[Recon] ndcg@25:0.3018\n",
      "[Recon] ndcg@30:0.3021\n",
      "[Recon] ndcg@35:0.3032\n",
      "[Recon] ndcg@40:0.3054\n",
      "[Recon] ndcg@45:0.3081\n",
      "[Recon] ndcg@50:0.3110\n",
      "[Recon] ndcg@all:0.5322\n",
      "Generator training loss: 0.195\n",
      "Classifier training loss: 1.108\n",
      "Discriminator training loss: 9.090\n",
      "\n",
      "======== Epoch 101 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.183\n",
      "  Average training loss classifier: 1.161\n",
      "  Average training loss discriminator: 9.195\n",
      "\n",
      "======== Epoch 102 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.103\n",
      "  Average training loss classifier: 1.282\n",
      "  Average training loss discriminator: 9.473\n",
      "\n",
      "======== Epoch 103 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.120\n",
      "  Average training loss classifier: 1.152\n",
      "  Average training loss discriminator: 9.137\n",
      "\n",
      "======== Epoch 104 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.118\n",
      "  Average training loss classifier: 1.206\n",
      "  Average training loss discriminator: 9.265\n",
      "\n",
      "======== Epoch 105 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.177\n",
      "  Average training loss classifier: 1.138\n",
      "  Average training loss discriminator: 9.096\n",
      "\n",
      "======== Epoch 106 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.080\n",
      "  Average training loss classifier: 1.176\n",
      "  Average training loss discriminator: 8.943\n",
      "\n",
      "======== Epoch 107 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.202\n",
      "  Average training loss classifier: 1.097\n",
      "  Average training loss discriminator: 8.819\n",
      "\n",
      "======== Epoch 108 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.165\n",
      "  Average training loss classifier: 1.119\n",
      "  Average training loss discriminator: 9.056\n",
      "\n",
      "======== Epoch 109 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.222\n",
      "  Average training loss classifier: 1.141\n",
      "  Average training loss discriminator: 8.915\n",
      "\n",
      "======== Epoch 110 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.231\n",
      "  Average training loss classifier: 1.073\n",
      "  Average training loss discriminator: 8.711\n",
      "---------------------------------------\n",
      "[Recon] Precision v1@5:0.5656\n",
      "[Recon] Precision v1@10:0.4711\n",
      "[Recon] Precision v1@15:0.4156\n",
      "[Recon] Precision v1@20:0.3766\n",
      "[Recon] Precision v1@25:0.3452\n",
      "[Recon] Precision v1@30:0.3199\n",
      "[Recon] Precision v1@35:0.2997\n",
      "[Recon] Precision v1@40:0.2833\n",
      "[Recon] Precision v1@45:0.2692\n",
      "[Recon] Precision v1@50:0.2567\n",
      "[Recon] Precision v2@5:0.1721\n",
      "[Recon] Precision v2@10:0.1714\n",
      "[Recon] Precision v2@15:0.1750\n",
      "[Recon] Precision v2@20:0.1834\n",
      "[Recon] Precision v2@25:0.1924\n",
      "[Recon] Precision v2@30:0.1985\n",
      "[Recon] Precision v2@35:0.2032\n",
      "[Recon] Precision v2@40:0.2055\n",
      "[Recon] Precision v2@45:0.2058\n",
      "[Recon] Precision v2@50:0.2051\n",
      "[Recon] ndcg@5:0.3343\n",
      "[Recon] ndcg@10:0.3199\n",
      "[Recon] ndcg@15:0.3113\n",
      "[Recon] ndcg@20:0.3066\n",
      "[Recon] ndcg@25:0.3045\n",
      "[Recon] ndcg@30:0.3039\n",
      "[Recon] ndcg@35:0.3048\n",
      "[Recon] ndcg@40:0.3069\n",
      "[Recon] ndcg@45:0.3096\n",
      "[Recon] ndcg@50:0.3129\n",
      "[Recon] ndcg@all:0.5330\n",
      "Generator training loss: 0.231\n",
      "Classifier training loss: 1.073\n",
      "Discriminator training loss: 8.711\n",
      "\n",
      "======== Epoch 111 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.205\n",
      "  Average training loss classifier: 1.215\n",
      "  Average training loss discriminator: 9.243\n",
      "\n",
      "======== Epoch 112 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.172\n",
      "  Average training loss classifier: 1.282\n",
      "  Average training loss discriminator: 9.440\n",
      "\n",
      "======== Epoch 113 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.158\n",
      "  Average training loss classifier: 1.199\n",
      "  Average training loss discriminator: 9.166\n",
      "\n",
      "======== Epoch 114 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.148\n",
      "  Average training loss classifier: 1.183\n",
      "  Average training loss discriminator: 8.951\n",
      "\n",
      "======== Epoch 115 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.139\n",
      "  Average training loss classifier: 1.212\n",
      "  Average training loss discriminator: 9.149\n",
      "\n",
      "======== Epoch 116 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.138\n",
      "  Average training loss classifier: 1.117\n",
      "  Average training loss discriminator: 8.708\n",
      "\n",
      "======== Epoch 117 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.208\n",
      "  Average training loss classifier: 1.168\n",
      "  Average training loss discriminator: 8.837\n",
      "\n",
      "======== Epoch 118 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.104\n",
      "  Average training loss classifier: 1.167\n",
      "  Average training loss discriminator: 8.998\n",
      "\n",
      "======== Epoch 119 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.112\n",
      "  Average training loss classifier: 1.196\n",
      "  Average training loss discriminator: 9.135\n",
      "\n",
      "======== Epoch 120 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.217\n",
      "  Average training loss classifier: 1.106\n",
      "  Average training loss discriminator: 8.569\n",
      "---------------------------------------\n",
      "[Recon] Precision v1@5:0.5776\n",
      "[Recon] Precision v1@10:0.4789\n",
      "[Recon] Precision v1@15:0.4189\n",
      "[Recon] Precision v1@20:0.3784\n",
      "[Recon] Precision v1@25:0.3469\n",
      "[Recon] Precision v1@30:0.3225\n",
      "[Recon] Precision v1@35:0.3020\n",
      "[Recon] Precision v1@40:0.2850\n",
      "[Recon] Precision v1@45:0.2701\n",
      "[Recon] Precision v1@50:0.2574\n",
      "[Recon] Precision v2@5:0.1906\n",
      "[Recon] Precision v2@10:0.1839\n",
      "[Recon] Precision v2@15:0.1858\n",
      "[Recon] Precision v2@20:0.1932\n",
      "[Recon] Precision v2@25:0.2008\n",
      "[Recon] Precision v2@30:0.2066\n",
      "[Recon] Precision v2@35:0.2101\n",
      "[Recon] Precision v2@40:0.2115\n",
      "[Recon] Precision v2@45:0.2103\n",
      "[Recon] Precision v2@50:0.2083\n",
      "[Recon] ndcg@5:0.3646\n",
      "[Recon] ndcg@10:0.3452\n",
      "[Recon] ndcg@15:0.3338\n",
      "[Recon] ndcg@20:0.3274\n",
      "[Recon] ndcg@25:0.3242\n",
      "[Recon] ndcg@30:0.3235\n",
      "[Recon] ndcg@35:0.3241\n",
      "[Recon] ndcg@40:0.3257\n",
      "[Recon] ndcg@45:0.3278\n",
      "[Recon] ndcg@50:0.3306\n",
      "[Recon] ndcg@all:0.5468\n",
      "Generator training loss: 0.217\n",
      "Classifier training loss: 1.106\n",
      "Discriminator training loss: 8.569\n",
      "\n",
      "======== Epoch 121 / 300 ========\n",
      "Bert GAN Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss generetor: 0.263\n",
      "  Average training loss classifier: 1.105\n",
      "  Average training loss discriminator: 8.588\n",
      "\n",
      "======== Epoch 122 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.182\n",
      "  Average training loss classifier: 1.280\n",
      "  Average training loss discriminator: 9.240\n",
      "\n",
      "======== Epoch 123 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.163\n",
      "  Average training loss classifier: 1.244\n",
      "  Average training loss discriminator: 9.276\n",
      "\n",
      "======== Epoch 124 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.216\n",
      "  Average training loss classifier: 1.140\n",
      "  Average training loss discriminator: 8.682\n",
      "\n",
      "======== Epoch 125 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.176\n",
      "  Average training loss classifier: 1.267\n",
      "  Average training loss discriminator: 9.058\n",
      "\n",
      "======== Epoch 126 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.143\n",
      "  Average training loss classifier: 1.201\n",
      "  Average training loss discriminator: 8.917\n",
      "\n",
      "======== Epoch 127 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.135\n",
      "  Average training loss classifier: 1.198\n",
      "  Average training loss discriminator: 8.948\n",
      "\n",
      "======== Epoch 128 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.164\n",
      "  Average training loss classifier: 1.131\n",
      "  Average training loss discriminator: 8.655\n",
      "\n",
      "======== Epoch 129 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.152\n",
      "  Average training loss classifier: 1.174\n",
      "  Average training loss discriminator: 8.690\n",
      "\n",
      "======== Epoch 130 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.146\n",
      "  Average training loss classifier: 1.197\n",
      "  Average training loss discriminator: 8.943\n",
      "---------------------------------------\n",
      "[Recon] Precision v1@5:0.5677\n",
      "[Recon] Precision v1@10:0.4660\n",
      "[Recon] Precision v1@15:0.4064\n",
      "[Recon] Precision v1@20:0.3659\n",
      "[Recon] Precision v1@25:0.3358\n",
      "[Recon] Precision v1@30:0.3126\n",
      "[Recon] Precision v1@35:0.2932\n",
      "[Recon] Precision v1@40:0.2765\n",
      "[Recon] Precision v1@45:0.2625\n",
      "[Recon] Precision v1@50:0.2502\n",
      "[Recon] Precision v2@5:0.1980\n",
      "[Recon] Precision v2@10:0.1907\n",
      "[Recon] Precision v2@15:0.1907\n",
      "[Recon] Precision v2@20:0.1953\n",
      "[Recon] Precision v2@25:0.2014\n",
      "[Recon] Precision v2@30:0.2056\n",
      "[Recon] Precision v2@35:0.2082\n",
      "[Recon] Precision v2@40:0.2083\n",
      "[Recon] Precision v2@45:0.2071\n",
      "[Recon] Precision v2@50:0.2046\n",
      "[Recon] ndcg@5:0.3749\n",
      "[Recon] ndcg@10:0.3533\n",
      "[Recon] ndcg@15:0.3408\n",
      "[Recon] ndcg@20:0.3332\n",
      "[Recon] ndcg@25:0.3294\n",
      "[Recon] ndcg@30:0.3286\n",
      "[Recon] ndcg@35:0.3290\n",
      "[Recon] ndcg@40:0.3303\n",
      "[Recon] ndcg@45:0.3324\n",
      "[Recon] ndcg@50:0.3349\n",
      "[Recon] ndcg@all:0.5505\n",
      "Generator training loss: 0.146\n",
      "Classifier training loss: 1.197\n",
      "Discriminator training loss: 8.943\n",
      "\n",
      "======== Epoch 131 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.095\n",
      "  Average training loss classifier: 1.246\n",
      "  Average training loss discriminator: 8.970\n",
      "\n",
      "======== Epoch 132 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.156\n",
      "  Average training loss classifier: 1.160\n",
      "  Average training loss discriminator: 8.574\n",
      "\n",
      "======== Epoch 133 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.103\n",
      "  Average training loss classifier: 1.164\n",
      "  Average training loss discriminator: 8.561\n",
      "\n",
      "======== Epoch 134 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.146\n",
      "  Average training loss classifier: 1.183\n",
      "  Average training loss discriminator: 8.680\n",
      "\n",
      "======== Epoch 135 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.146\n",
      "  Average training loss classifier: 1.108\n",
      "  Average training loss discriminator: 8.449\n",
      "\n",
      "======== Epoch 136 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.113\n",
      "  Average training loss classifier: 1.148\n",
      "  Average training loss discriminator: 8.409\n",
      "\n",
      "======== Epoch 137 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.097\n",
      "  Average training loss classifier: 1.278\n",
      "  Average training loss discriminator: 9.048\n",
      "\n",
      "======== Epoch 138 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.078\n",
      "  Average training loss classifier: 1.210\n",
      "  Average training loss discriminator: 8.653\n",
      "\n",
      "======== Epoch 139 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.097\n",
      "  Average training loss classifier: 1.116\n",
      "  Average training loss discriminator: 8.207\n",
      "\n",
      "======== Epoch 140 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.068\n",
      "  Average training loss classifier: 1.182\n",
      "  Average training loss discriminator: 8.521\n",
      "---------------------------------------\n",
      "[Recon] Precision v1@5:0.5551\n",
      "[Recon] Precision v1@10:0.4539\n",
      "[Recon] Precision v1@15:0.3958\n",
      "[Recon] Precision v1@20:0.3566\n",
      "[Recon] Precision v1@25:0.3275\n",
      "[Recon] Precision v1@30:0.3048\n",
      "[Recon] Precision v1@35:0.2860\n",
      "[Recon] Precision v1@40:0.2701\n",
      "[Recon] Precision v1@45:0.2565\n",
      "[Recon] Precision v1@50:0.2448\n",
      "[Recon] Precision v2@5:0.1976\n",
      "[Recon] Precision v2@10:0.1883\n",
      "[Recon] Precision v2@15:0.1878\n",
      "[Recon] Precision v2@20:0.1920\n",
      "[Recon] Precision v2@25:0.1970\n",
      "[Recon] Precision v2@30:0.2007\n",
      "[Recon] Precision v2@35:0.2031\n",
      "[Recon] Precision v2@40:0.2036\n",
      "[Recon] Precision v2@45:0.2022\n",
      "[Recon] Precision v2@50:0.1998\n",
      "[Recon] ndcg@5:0.3692\n",
      "[Recon] ndcg@10:0.3476\n",
      "[Recon] ndcg@15:0.3349\n",
      "[Recon] ndcg@20:0.3277\n",
      "[Recon] ndcg@25:0.3240\n",
      "[Recon] ndcg@30:0.3226\n",
      "[Recon] ndcg@35:0.3231\n",
      "[Recon] ndcg@40:0.3245\n",
      "[Recon] ndcg@45:0.3266\n",
      "[Recon] ndcg@50:0.3292\n",
      "[Recon] ndcg@all:0.5464\n",
      "Generator training loss: 0.068\n",
      "Classifier training loss: 1.182\n",
      "Discriminator training loss: 8.521\n",
      "\n",
      "======== Epoch 141 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.100\n",
      "  Average training loss classifier: 1.095\n",
      "  Average training loss discriminator: 8.214\n",
      "\n",
      "======== Epoch 142 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.132\n",
      "  Average training loss classifier: 1.177\n",
      "  Average training loss discriminator: 8.649\n",
      "\n",
      "======== Epoch 143 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.057\n",
      "  Average training loss classifier: 1.163\n",
      "  Average training loss discriminator: 8.377\n",
      "\n",
      "======== Epoch 144 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.053\n",
      "  Average training loss classifier: 1.187\n",
      "  Average training loss discriminator: 8.376\n",
      "\n",
      "======== Epoch 145 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.171\n",
      "  Average training loss classifier: 1.028\n",
      "  Average training loss discriminator: 7.905\n",
      "\n",
      "======== Epoch 146 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.094\n",
      "  Average training loss classifier: 1.086\n",
      "  Average training loss discriminator: 8.124\n",
      "\n",
      "======== Epoch 147 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.210\n",
      "  Average training loss classifier: 1.166\n",
      "  Average training loss discriminator: 8.294\n",
      "\n",
      "======== Epoch 148 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.135\n",
      "  Average training loss classifier: 1.056\n",
      "  Average training loss discriminator: 8.141\n",
      "\n",
      "======== Epoch 149 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.046\n",
      "  Average training loss classifier: 1.224\n",
      "  Average training loss discriminator: 8.671\n",
      "\n",
      "======== Epoch 150 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.146\n",
      "  Average training loss classifier: 1.213\n",
      "  Average training loss discriminator: 8.428\n",
      "---------------------------------------\n",
      "[Recon] Precision v1@5:0.5607\n",
      "[Recon] Precision v1@10:0.4608\n",
      "[Recon] Precision v1@15:0.4048\n",
      "[Recon] Precision v1@20:0.3645\n",
      "[Recon] Precision v1@25:0.3349\n",
      "[Recon] Precision v1@30:0.3120\n",
      "[Recon] Precision v1@35:0.2924\n",
      "[Recon] Precision v1@40:0.2757\n",
      "[Recon] Precision v1@45:0.2616\n",
      "[Recon] Precision v1@50:0.2485\n",
      "[Recon] Precision v2@5:0.2011\n",
      "[Recon] Precision v2@10:0.1939\n",
      "[Recon] Precision v2@15:0.1936\n",
      "[Recon] Precision v2@20:0.1973\n",
      "[Recon] Precision v2@25:0.2028\n",
      "[Recon] Precision v2@30:0.2067\n",
      "[Recon] Precision v2@35:0.2084\n",
      "[Recon] Precision v2@40:0.2088\n",
      "[Recon] Precision v2@45:0.2076\n",
      "[Recon] Precision v2@50:0.2042\n",
      "[Recon] ndcg@5:0.3769\n",
      "[Recon] ndcg@10:0.3557\n",
      "[Recon] ndcg@15:0.3439\n",
      "[Recon] ndcg@20:0.3361\n",
      "[Recon] ndcg@25:0.3327\n",
      "[Recon] ndcg@30:0.3316\n",
      "[Recon] ndcg@35:0.3317\n",
      "[Recon] ndcg@40:0.3330\n",
      "[Recon] ndcg@45:0.3351\n",
      "[Recon] ndcg@50:0.3371\n",
      "[Recon] ndcg@all:0.5515\n",
      "Generator training loss: 0.146\n",
      "Classifier training loss: 1.213\n",
      "Discriminator training loss: 8.428\n",
      "\n",
      "======== Epoch 151 / 300 ========\n",
      "Bert GAN Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss generetor: 0.188\n",
      "  Average training loss classifier: 1.050\n",
      "  Average training loss discriminator: 7.871\n",
      "\n",
      "======== Epoch 152 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.289\n",
      "  Average training loss classifier: 1.150\n",
      "  Average training loss discriminator: 8.262\n",
      "\n",
      "======== Epoch 153 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.227\n",
      "  Average training loss classifier: 1.037\n",
      "  Average training loss discriminator: 8.124\n",
      "\n",
      "======== Epoch 154 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.199\n",
      "  Average training loss classifier: 1.246\n",
      "  Average training loss discriminator: 8.748\n",
      "\n",
      "======== Epoch 155 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.224\n",
      "  Average training loss classifier: 1.244\n",
      "  Average training loss discriminator: 8.851\n",
      "\n",
      "======== Epoch 156 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.254\n",
      "  Average training loss classifier: 1.070\n",
      "  Average training loss discriminator: 8.244\n",
      "\n",
      "======== Epoch 157 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.218\n",
      "  Average training loss classifier: 1.150\n",
      "  Average training loss discriminator: 8.297\n",
      "\n",
      "======== Epoch 158 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.165\n",
      "  Average training loss classifier: 1.343\n",
      "  Average training loss discriminator: 8.957\n",
      "\n",
      "======== Epoch 159 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.142\n",
      "  Average training loss classifier: 1.163\n",
      "  Average training loss discriminator: 8.310\n",
      "\n",
      "======== Epoch 160 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.151\n",
      "  Average training loss classifier: 1.374\n",
      "  Average training loss discriminator: 8.962\n",
      "---------------------------------------\n",
      "[Recon] Precision v1@5:0.5582\n",
      "[Recon] Precision v1@10:0.4502\n",
      "[Recon] Precision v1@15:0.3918\n",
      "[Recon] Precision v1@20:0.3530\n",
      "[Recon] Precision v1@25:0.3232\n",
      "[Recon] Precision v1@30:0.2996\n",
      "[Recon] Precision v1@35:0.2813\n",
      "[Recon] Precision v1@40:0.2655\n",
      "[Recon] Precision v1@45:0.2511\n",
      "[Recon] Precision v1@50:0.2396\n",
      "[Recon] Precision v2@5:0.2092\n",
      "[Recon] Precision v2@10:0.1975\n",
      "[Recon] Precision v2@15:0.1945\n",
      "[Recon] Precision v2@20:0.1959\n",
      "[Recon] Precision v2@25:0.2001\n",
      "[Recon] Precision v2@30:0.2019\n",
      "[Recon] Precision v2@35:0.2029\n",
      "[Recon] Precision v2@40:0.2022\n",
      "[Recon] Precision v2@45:0.1996\n",
      "[Recon] Precision v2@50:0.1972\n",
      "[Recon] ndcg@5:0.3874\n",
      "[Recon] ndcg@10:0.3621\n",
      "[Recon] ndcg@15:0.3482\n",
      "[Recon] ndcg@20:0.3396\n",
      "[Recon] ndcg@25:0.3353\n",
      "[Recon] ndcg@30:0.3333\n",
      "[Recon] ndcg@35:0.3331\n",
      "[Recon] ndcg@40:0.3343\n",
      "[Recon] ndcg@45:0.3358\n",
      "[Recon] ndcg@50:0.3382\n",
      "[Recon] ndcg@all:0.5532\n",
      "Generator training loss: 0.151\n",
      "Classifier training loss: 1.374\n",
      "Discriminator training loss: 8.962\n",
      "\n",
      "======== Epoch 161 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.094\n",
      "  Average training loss classifier: 1.216\n",
      "  Average training loss discriminator: 8.629\n",
      "\n",
      "======== Epoch 162 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.092\n",
      "  Average training loss classifier: 1.200\n",
      "  Average training loss discriminator: 8.357\n",
      "\n",
      "======== Epoch 163 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.058\n",
      "  Average training loss classifier: 1.181\n",
      "  Average training loss discriminator: 8.116\n",
      "\n",
      "======== Epoch 164 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.171\n",
      "  Average training loss classifier: 1.076\n",
      "  Average training loss discriminator: 7.742\n",
      "\n",
      "======== Epoch 165 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.222\n",
      "  Average training loss classifier: 1.113\n",
      "  Average training loss discriminator: 7.981\n",
      "\n",
      "======== Epoch 166 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.255\n",
      "  Average training loss classifier: 1.100\n",
      "  Average training loss discriminator: 8.006\n",
      "\n",
      "======== Epoch 167 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.192\n",
      "  Average training loss classifier: 1.209\n",
      "  Average training loss discriminator: 8.530\n",
      "\n",
      "======== Epoch 168 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.099\n",
      "  Average training loss classifier: 1.166\n",
      "  Average training loss discriminator: 8.386\n",
      "\n",
      "======== Epoch 169 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.065\n",
      "  Average training loss classifier: 1.295\n",
      "  Average training loss discriminator: 8.699\n",
      "\n",
      "======== Epoch 170 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.081\n",
      "  Average training loss classifier: 1.246\n",
      "  Average training loss discriminator: 8.370\n",
      "---------------------------------------\n",
      "[Recon] Precision v1@5:0.5358\n",
      "[Recon] Precision v1@10:0.4337\n",
      "[Recon] Precision v1@15:0.3776\n",
      "[Recon] Precision v1@20:0.3397\n",
      "[Recon] Precision v1@25:0.3121\n",
      "[Recon] Precision v1@30:0.2898\n",
      "[Recon] Precision v1@35:0.2719\n",
      "[Recon] Precision v1@40:0.2563\n",
      "[Recon] Precision v1@45:0.2432\n",
      "[Recon] Precision v1@50:0.2315\n",
      "[Recon] Precision v2@5:0.2013\n",
      "[Recon] Precision v2@10:0.1929\n",
      "[Recon] Precision v2@15:0.1910\n",
      "[Recon] Precision v2@20:0.1933\n",
      "[Recon] Precision v2@25:0.1964\n",
      "[Recon] Precision v2@30:0.1980\n",
      "[Recon] Precision v2@35:0.1987\n",
      "[Recon] Precision v2@40:0.1977\n",
      "[Recon] Precision v2@45:0.1956\n",
      "[Recon] Precision v2@50:0.1923\n",
      "[Recon] ndcg@5:0.3741\n",
      "[Recon] ndcg@10:0.3515\n",
      "[Recon] ndcg@15:0.3387\n",
      "[Recon] ndcg@20:0.3311\n",
      "[Recon] ndcg@25:0.3275\n",
      "[Recon] ndcg@30:0.3257\n",
      "[Recon] ndcg@35:0.3258\n",
      "[Recon] ndcg@40:0.3270\n",
      "[Recon] ndcg@45:0.3289\n",
      "[Recon] ndcg@50:0.3310\n",
      "[Recon] ndcg@all:0.5468\n",
      "Generator training loss: 0.081\n",
      "Classifier training loss: 1.246\n",
      "Discriminator training loss: 8.370\n",
      "\n",
      "======== Epoch 171 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.162\n",
      "  Average training loss classifier: 1.075\n",
      "  Average training loss discriminator: 7.771\n",
      "\n",
      "======== Epoch 172 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.147\n",
      "  Average training loss classifier: 1.142\n",
      "  Average training loss discriminator: 8.030\n",
      "\n",
      "======== Epoch 173 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.173\n",
      "  Average training loss classifier: 1.208\n",
      "  Average training loss discriminator: 8.344\n",
      "\n",
      "======== Epoch 174 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.185\n",
      "  Average training loss classifier: 1.107\n",
      "  Average training loss discriminator: 7.963\n",
      "\n",
      "======== Epoch 175 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.148\n",
      "  Average training loss classifier: 1.164\n",
      "  Average training loss discriminator: 8.156\n",
      "\n",
      "======== Epoch 176 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.097\n",
      "  Average training loss classifier: 1.191\n",
      "  Average training loss discriminator: 8.214\n",
      "\n",
      "======== Epoch 177 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.236\n",
      "  Average training loss classifier: 1.129\n",
      "  Average training loss discriminator: 7.995\n",
      "\n",
      "======== Epoch 178 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.290\n",
      "  Average training loss classifier: 1.034\n",
      "  Average training loss discriminator: 7.797\n",
      "\n",
      "======== Epoch 179 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.194\n",
      "  Average training loss classifier: 1.322\n",
      "  Average training loss discriminator: 8.688\n",
      "\n",
      "======== Epoch 180 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.082\n",
      "  Average training loss classifier: 1.190\n",
      "  Average training loss discriminator: 8.298\n",
      "---------------------------------------\n",
      "[Recon] Precision v1@5:0.5437\n",
      "[Recon] Precision v1@10:0.4437\n",
      "[Recon] Precision v1@15:0.3869\n",
      "[Recon] Precision v1@20:0.3486\n",
      "[Recon] Precision v1@25:0.3196\n",
      "[Recon] Precision v1@30:0.2971\n",
      "[Recon] Precision v1@35:0.2782\n",
      "[Recon] Precision v1@40:0.2629\n",
      "[Recon] Precision v1@45:0.2491\n",
      "[Recon] Precision v1@50:0.2375\n",
      "[Recon] Precision v2@5:0.2033\n",
      "[Recon] Precision v2@10:0.1937\n",
      "[Recon] Precision v2@15:0.1900\n",
      "[Recon] Precision v2@20:0.1925\n",
      "[Recon] Precision v2@25:0.1959\n",
      "[Recon] Precision v2@30:0.1982\n",
      "[Recon] Precision v2@35:0.1992\n",
      "[Recon] Precision v2@40:0.1994\n",
      "[Recon] Precision v2@45:0.1972\n",
      "[Recon] Precision v2@50:0.1944\n",
      "[Recon] ndcg@5:0.3798\n",
      "[Recon] ndcg@10:0.3569\n",
      "[Recon] ndcg@15:0.3423\n",
      "[Recon] ndcg@20:0.3348\n",
      "[Recon] ndcg@25:0.3304\n",
      "[Recon] ndcg@30:0.3287\n",
      "[Recon] ndcg@35:0.3286\n",
      "[Recon] ndcg@40:0.3299\n",
      "[Recon] ndcg@45:0.3312\n",
      "[Recon] ndcg@50:0.3336\n",
      "[Recon] ndcg@all:0.5496\n",
      "Generator training loss: 0.082\n",
      "Classifier training loss: 1.190\n",
      "Discriminator training loss: 8.298\n",
      "\n",
      "======== Epoch 181 / 300 ========\n",
      "Bert GAN Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss generetor: 0.160\n",
      "  Average training loss classifier: 1.168\n",
      "  Average training loss discriminator: 8.108\n",
      "\n",
      "======== Epoch 182 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.157\n",
      "  Average training loss classifier: 1.262\n",
      "  Average training loss discriminator: 8.394\n",
      "\n",
      "======== Epoch 183 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.261\n",
      "  Average training loss classifier: 1.063\n",
      "  Average training loss discriminator: 7.752\n",
      "\n",
      "======== Epoch 184 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.113\n",
      "  Average training loss classifier: 1.230\n",
      "  Average training loss discriminator: 8.617\n",
      "\n",
      "======== Epoch 185 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.195\n",
      "  Average training loss classifier: 1.132\n",
      "  Average training loss discriminator: 7.942\n",
      "\n",
      "======== Epoch 186 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.078\n",
      "  Average training loss classifier: 1.154\n",
      "  Average training loss discriminator: 8.009\n",
      "\n",
      "======== Epoch 187 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.175\n",
      "  Average training loss classifier: 1.276\n",
      "  Average training loss discriminator: 8.480\n",
      "\n",
      "======== Epoch 188 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.107\n",
      "  Average training loss classifier: 1.228\n",
      "  Average training loss discriminator: 8.214\n",
      "\n",
      "======== Epoch 189 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.086\n",
      "  Average training loss classifier: 1.190\n",
      "  Average training loss discriminator: 8.010\n",
      "\n",
      "======== Epoch 190 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.177\n",
      "  Average training loss classifier: 1.184\n",
      "  Average training loss discriminator: 8.016\n",
      "---------------------------------------\n",
      "[Recon] Precision v1@5:0.5417\n",
      "[Recon] Precision v1@10:0.4389\n",
      "[Recon] Precision v1@15:0.3818\n",
      "[Recon] Precision v1@20:0.3428\n",
      "[Recon] Precision v1@25:0.3150\n",
      "[Recon] Precision v1@30:0.2929\n",
      "[Recon] Precision v1@35:0.2743\n",
      "[Recon] Precision v1@40:0.2583\n",
      "[Recon] Precision v1@45:0.2446\n",
      "[Recon] Precision v1@50:0.2335\n",
      "[Recon] Precision v2@5:0.2080\n",
      "[Recon] Precision v2@10:0.1962\n",
      "[Recon] Precision v2@15:0.1931\n",
      "[Recon] Precision v2@20:0.1935\n",
      "[Recon] Precision v2@25:0.1963\n",
      "[Recon] Precision v2@30:0.1976\n",
      "[Recon] Precision v2@35:0.1982\n",
      "[Recon] Precision v2@40:0.1971\n",
      "[Recon] Precision v2@45:0.1945\n",
      "[Recon] Precision v2@50:0.1915\n",
      "[Recon] ndcg@5:0.3848\n",
      "[Recon] ndcg@10:0.3595\n",
      "[Recon] ndcg@15:0.3454\n",
      "[Recon] ndcg@20:0.3369\n",
      "[Recon] ndcg@25:0.3329\n",
      "[Recon] ndcg@30:0.3309\n",
      "[Recon] ndcg@35:0.3307\n",
      "[Recon] ndcg@40:0.3313\n",
      "[Recon] ndcg@45:0.3326\n",
      "[Recon] ndcg@50:0.3349\n",
      "[Recon] ndcg@all:0.5502\n",
      "Generator training loss: 0.177\n",
      "Classifier training loss: 1.184\n",
      "Discriminator training loss: 8.016\n",
      "\n",
      "======== Epoch 191 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.187\n",
      "  Average training loss classifier: 1.116\n",
      "  Average training loss discriminator: 8.036\n",
      "\n",
      "======== Epoch 192 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.203\n",
      "  Average training loss classifier: 1.141\n",
      "  Average training loss discriminator: 7.919\n",
      "\n",
      "======== Epoch 193 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.140\n",
      "  Average training loss classifier: 1.204\n",
      "  Average training loss discriminator: 8.100\n",
      "\n",
      "======== Epoch 194 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.224\n",
      "  Average training loss classifier: 1.156\n",
      "  Average training loss discriminator: 8.040\n",
      "\n",
      "======== Epoch 195 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.260\n",
      "  Average training loss classifier: 0.980\n",
      "  Average training loss discriminator: 7.557\n",
      "\n",
      "======== Epoch 196 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.095\n",
      "  Average training loss classifier: 1.063\n",
      "  Average training loss discriminator: 7.574\n",
      "\n",
      "======== Epoch 197 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.195\n",
      "  Average training loss classifier: 1.427\n",
      "  Average training loss discriminator: 8.989\n",
      "\n",
      "======== Epoch 198 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.220\n",
      "  Average training loss classifier: 1.110\n",
      "  Average training loss discriminator: 7.897\n",
      "\n",
      "======== Epoch 199 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.171\n",
      "  Average training loss classifier: 1.248\n",
      "  Average training loss discriminator: 8.182\n",
      "\n",
      "======== Epoch 200 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.072\n",
      "  Average training loss classifier: 1.161\n",
      "  Average training loss discriminator: 8.085\n",
      "---------------------------------------\n",
      "[Recon] Precision v1@5:0.5457\n",
      "[Recon] Precision v1@10:0.4443\n",
      "[Recon] Precision v1@15:0.3886\n",
      "[Recon] Precision v1@20:0.3501\n",
      "[Recon] Precision v1@25:0.3210\n",
      "[Recon] Precision v1@30:0.2984\n",
      "[Recon] Precision v1@35:0.2794\n",
      "[Recon] Precision v1@40:0.2639\n",
      "[Recon] Precision v1@45:0.2510\n",
      "[Recon] Precision v1@50:0.2392\n",
      "[Recon] Precision v2@5:0.2023\n",
      "[Recon] Precision v2@10:0.1885\n",
      "[Recon] Precision v2@15:0.1863\n",
      "[Recon] Precision v2@20:0.1885\n",
      "[Recon] Precision v2@25:0.1922\n",
      "[Recon] Precision v2@30:0.1952\n",
      "[Recon] Precision v2@35:0.1962\n",
      "[Recon] Precision v2@40:0.1966\n",
      "[Recon] Precision v2@45:0.1955\n",
      "[Recon] Precision v2@50:0.1930\n",
      "[Recon] ndcg@5:0.3762\n",
      "[Recon] ndcg@10:0.3511\n",
      "[Recon] ndcg@15:0.3376\n",
      "[Recon] ndcg@20:0.3298\n",
      "[Recon] ndcg@25:0.3259\n",
      "[Recon] ndcg@30:0.3246\n",
      "[Recon] ndcg@35:0.3243\n",
      "[Recon] ndcg@40:0.3254\n",
      "[Recon] ndcg@45:0.3275\n",
      "[Recon] ndcg@50:0.3296\n",
      "[Recon] ndcg@all:0.5467\n",
      "Generator training loss: 0.072\n",
      "Classifier training loss: 1.161\n",
      "Discriminator training loss: 8.085\n",
      "\n",
      "======== Epoch 201 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.218\n",
      "  Average training loss classifier: 1.191\n",
      "  Average training loss discriminator: 8.145\n",
      "\n",
      "======== Epoch 202 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.232\n",
      "  Average training loss classifier: 1.126\n",
      "  Average training loss discriminator: 7.837\n",
      "\n",
      "======== Epoch 203 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.268\n",
      "  Average training loss classifier: 0.985\n",
      "  Average training loss discriminator: 7.584\n",
      "\n",
      "======== Epoch 204 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.239\n",
      "  Average training loss classifier: 1.205\n",
      "  Average training loss discriminator: 7.960\n",
      "\n",
      "======== Epoch 205 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.157\n",
      "  Average training loss classifier: 1.146\n",
      "  Average training loss discriminator: 7.942\n",
      "\n",
      "======== Epoch 206 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.264\n",
      "  Average training loss classifier: 1.243\n",
      "  Average training loss discriminator: 8.380\n",
      "\n",
      "======== Epoch 207 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.217\n",
      "  Average training loss classifier: 1.324\n",
      "  Average training loss discriminator: 8.209\n",
      "\n",
      "======== Epoch 208 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.100\n",
      "  Average training loss classifier: 1.218\n",
      "  Average training loss discriminator: 8.215\n",
      "\n",
      "======== Epoch 209 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.143\n",
      "  Average training loss classifier: 1.140\n",
      "  Average training loss discriminator: 7.818\n",
      "\n",
      "======== Epoch 210 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.187\n",
      "  Average training loss classifier: 1.221\n",
      "  Average training loss discriminator: 8.152\n",
      "---------------------------------------\n",
      "[Recon] Precision v1@5:0.5094\n",
      "[Recon] Precision v1@10:0.4091\n",
      "[Recon] Precision v1@15:0.3540\n",
      "[Recon] Precision v1@20:0.3165\n",
      "[Recon] Precision v1@25:0.2890\n",
      "[Recon] Precision v1@30:0.2673\n",
      "[Recon] Precision v1@35:0.2498\n",
      "[Recon] Precision v1@40:0.2353\n",
      "[Recon] Precision v1@45:0.2228\n",
      "[Recon] Precision v1@50:0.2120\n",
      "[Recon] Precision v2@5:0.2042\n",
      "[Recon] Precision v2@10:0.1936\n",
      "[Recon] Precision v2@15:0.1889\n",
      "[Recon] Precision v2@20:0.1882\n",
      "[Recon] Precision v2@25:0.1879\n",
      "[Recon] Precision v2@30:0.1874\n",
      "[Recon] Precision v2@35:0.1861\n",
      "[Recon] Precision v2@40:0.1838\n",
      "[Recon] Precision v2@45:0.1806\n",
      "[Recon] Precision v2@50:0.1764\n",
      "[Recon] ndcg@5:0.3749\n",
      "[Recon] ndcg@10:0.3503\n",
      "[Recon] ndcg@15:0.3366\n",
      "[Recon] ndcg@20:0.3281\n",
      "[Recon] ndcg@25:0.3233\n",
      "[Recon] ndcg@30:0.3210\n",
      "[Recon] ndcg@35:0.3207\n",
      "[Recon] ndcg@40:0.3213\n",
      "[Recon] ndcg@45:0.3228\n",
      "[Recon] ndcg@50:0.3246\n",
      "[Recon] ndcg@all:0.5413\n",
      "Generator training loss: 0.187\n",
      "Classifier training loss: 1.221\n",
      "Discriminator training loss: 8.152\n",
      "\n",
      "======== Epoch 211 / 300 ========\n",
      "Bert GAN Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss generetor: 0.197\n",
      "  Average training loss classifier: 1.137\n",
      "  Average training loss discriminator: 7.881\n",
      "\n",
      "======== Epoch 212 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.199\n",
      "  Average training loss classifier: 1.150\n",
      "  Average training loss discriminator: 7.938\n",
      "\n",
      "======== Epoch 213 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.172\n",
      "  Average training loss classifier: 1.248\n",
      "  Average training loss discriminator: 8.189\n",
      "\n",
      "======== Epoch 214 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.144\n",
      "  Average training loss classifier: 1.253\n",
      "  Average training loss discriminator: 8.202\n",
      "\n",
      "======== Epoch 215 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.157\n",
      "  Average training loss classifier: 1.179\n",
      "  Average training loss discriminator: 7.969\n",
      "\n",
      "======== Epoch 216 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.196\n",
      "  Average training loss classifier: 1.145\n",
      "  Average training loss discriminator: 7.786\n",
      "\n",
      "======== Epoch 217 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.126\n",
      "  Average training loss classifier: 1.145\n",
      "  Average training loss discriminator: 7.941\n",
      "\n",
      "======== Epoch 218 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.246\n",
      "  Average training loss classifier: 1.227\n",
      "  Average training loss discriminator: 8.353\n",
      "\n",
      "======== Epoch 219 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.264\n",
      "  Average training loss classifier: 1.104\n",
      "  Average training loss discriminator: 7.799\n",
      "\n",
      "======== Epoch 220 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.231\n",
      "  Average training loss classifier: 1.170\n",
      "  Average training loss discriminator: 8.062\n",
      "---------------------------------------\n",
      "[Recon] Precision v1@5:0.5203\n",
      "[Recon] Precision v1@10:0.4098\n",
      "[Recon] Precision v1@15:0.3539\n",
      "[Recon] Precision v1@20:0.3172\n",
      "[Recon] Precision v1@25:0.2898\n",
      "[Recon] Precision v1@30:0.2682\n",
      "[Recon] Precision v1@35:0.2513\n",
      "[Recon] Precision v1@40:0.2373\n",
      "[Recon] Precision v1@45:0.2250\n",
      "[Recon] Precision v1@50:0.2145\n",
      "[Recon] Precision v2@5:0.2116\n",
      "[Recon] Precision v2@10:0.1975\n",
      "[Recon] Precision v2@15:0.1917\n",
      "[Recon] Precision v2@20:0.1905\n",
      "[Recon] Precision v2@25:0.1906\n",
      "[Recon] Precision v2@30:0.1896\n",
      "[Recon] Precision v2@35:0.1884\n",
      "[Recon] Precision v2@40:0.1862\n",
      "[Recon] Precision v2@45:0.1831\n",
      "[Recon] Precision v2@50:0.1795\n",
      "[Recon] ndcg@5:0.3855\n",
      "[Recon] ndcg@10:0.3580\n",
      "[Recon] ndcg@15:0.3431\n",
      "[Recon] ndcg@20:0.3340\n",
      "[Recon] ndcg@25:0.3293\n",
      "[Recon] ndcg@30:0.3268\n",
      "[Recon] ndcg@35:0.3263\n",
      "[Recon] ndcg@40:0.3271\n",
      "[Recon] ndcg@45:0.3285\n",
      "[Recon] ndcg@50:0.3303\n",
      "[Recon] ndcg@all:0.5461\n",
      "Generator training loss: 0.231\n",
      "Classifier training loss: 1.170\n",
      "Discriminator training loss: 8.062\n",
      "\n",
      "======== Epoch 221 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.106\n",
      "  Average training loss classifier: 1.301\n",
      "  Average training loss discriminator: 8.307\n",
      "\n",
      "======== Epoch 222 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.117\n",
      "  Average training loss classifier: 1.222\n",
      "  Average training loss discriminator: 8.077\n",
      "\n",
      "======== Epoch 223 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.121\n",
      "  Average training loss classifier: 1.162\n",
      "  Average training loss discriminator: 7.855\n",
      "\n",
      "======== Epoch 224 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.111\n",
      "  Average training loss classifier: 1.120\n",
      "  Average training loss discriminator: 7.713\n",
      "\n",
      "======== Epoch 225 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.100\n",
      "  Average training loss classifier: 1.310\n",
      "  Average training loss discriminator: 8.344\n",
      "\n",
      "======== Epoch 226 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.089\n",
      "  Average training loss classifier: 1.200\n",
      "  Average training loss discriminator: 7.980\n",
      "\n",
      "======== Epoch 227 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.059\n",
      "  Average training loss classifier: 1.190\n",
      "  Average training loss discriminator: 8.085\n",
      "\n",
      "======== Epoch 228 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.087\n",
      "  Average training loss classifier: 1.136\n",
      "  Average training loss discriminator: 7.647\n",
      "\n",
      "======== Epoch 229 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.123\n",
      "  Average training loss classifier: 1.065\n",
      "  Average training loss discriminator: 7.422\n",
      "\n",
      "======== Epoch 230 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.150\n",
      "  Average training loss classifier: 1.114\n",
      "  Average training loss discriminator: 7.510\n",
      "---------------------------------------\n",
      "[Recon] Precision v1@5:0.5070\n",
      "[Recon] Precision v1@10:0.4088\n",
      "[Recon] Precision v1@15:0.3540\n",
      "[Recon] Precision v1@20:0.3183\n",
      "[Recon] Precision v1@25:0.2903\n",
      "[Recon] Precision v1@30:0.2699\n",
      "[Recon] Precision v1@35:0.2536\n",
      "[Recon] Precision v1@40:0.2399\n",
      "[Recon] Precision v1@45:0.2272\n",
      "[Recon] Precision v1@50:0.2163\n",
      "[Recon] Precision v2@5:0.2057\n",
      "[Recon] Precision v2@10:0.1924\n",
      "[Recon] Precision v2@15:0.1874\n",
      "[Recon] Precision v2@20:0.1873\n",
      "[Recon] Precision v2@25:0.1865\n",
      "[Recon] Precision v2@30:0.1870\n",
      "[Recon] Precision v2@35:0.1862\n",
      "[Recon] Precision v2@40:0.1851\n",
      "[Recon] Precision v2@45:0.1825\n",
      "[Recon] Precision v2@50:0.1788\n",
      "[Recon] ndcg@5:0.3736\n",
      "[Recon] ndcg@10:0.3489\n",
      "[Recon] ndcg@15:0.3348\n",
      "[Recon] ndcg@20:0.3271\n",
      "[Recon] ndcg@25:0.3221\n",
      "[Recon] ndcg@30:0.3203\n",
      "[Recon] ndcg@35:0.3202\n",
      "[Recon] ndcg@40:0.3211\n",
      "[Recon] ndcg@45:0.3227\n",
      "[Recon] ndcg@50:0.3246\n",
      "[Recon] ndcg@all:0.5416\n",
      "Generator training loss: 0.150\n",
      "Classifier training loss: 1.114\n",
      "Discriminator training loss: 7.510\n",
      "\n",
      "======== Epoch 231 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.057\n",
      "  Average training loss classifier: 1.236\n",
      "  Average training loss discriminator: 8.000\n",
      "\n",
      "======== Epoch 232 / 300 ========\n",
      "Bert GAN Training...\n",
      "\n",
      "  Average training loss generetor: 0.162\n",
      "  Average training loss classifier: 1.047\n",
      "  Average training loss discriminator: 7.512\n",
      "\n",
      "======== Epoch 233 / 300 ========\n",
      "Bert GAN Training...\n"
     ]
    }
   ],
   "source": [
    "model.gan_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9e468d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(1, 768)\n",
    "label = torch.randn(1, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4c1f758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_input = torch.unsqueeze(torch.mean(input, dim=0), 0)\n",
    "n_label = torch.unsqueeze(torch.mean(label, dim=0), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d536eb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f33d3b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = torch.cdist(m_input, n_label, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "88789de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis1 = torch.mean(dis, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0bb47831",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis2 = torch.mean(dis1, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bc32348c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(38.9615)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74d2a234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5169, -0.8715],\n",
       "        [ 1.5264, -0.2267]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06604578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
